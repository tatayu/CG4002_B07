{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit, cross_val_score\n",
    "\n",
    "import brevitas.nn as nn\n",
    "\n",
    "from config import *\n",
    "from classic_models import *\n",
    "from data_preprocessing import *\n",
    "from feature_extraction import *\n",
    "from helpers import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df_train = pd.read_csv('out_12_train.csv')\n",
    "# df_train = df_train.iloc[:, list(range(15,55)) + [-1]]\n",
    "df_test = pd.read_csv('out_12_test.csv')\n",
    "# df_test = df_test.iloc[:, list(range(15,55)) + [-1]]\n",
    "\n",
    "df_train['tag'] = df_train['tag'].apply(lambda x: int(x-1))\n",
    "df_test['tag'] = df_test['tag'].apply(lambda x: int(x-1))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df_np = df.to_numpy()\n",
    "\n",
    "        self.X = df_np[:,:-1]\n",
    "        self.y = df_np[:,-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get item by index\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns length of data\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 55 60 65 70 75 80 85 90 95 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureDataset(df_train)\n",
    "D_in = 72 # df.shape[1]-1\n",
    "D_out = 8 # len(dances)\n",
    "# df_train = df_train[:,:15]\n",
    "# df_test = df_test[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv1d, MaxPool1d, Module, Softmax, BatchNorm1d, Dropout, Flatten\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=64,out_channels=64, kernel_size=5)\n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5)\n",
    "        self.conv4 = torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5)\n",
    "        self.conv5 = torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5)\n",
    "        self.conv6 = torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5)\n",
    "#         self.lstm1 = torch.nn.LSTM(\n",
    "#             input_size=14,\n",
    "#             hidden_size=32,\n",
    "#             num_layers=2,\n",
    "#             batch_first=False,\n",
    "#         )\n",
    "        self.fc1 = torch.nn.Linear(48, 26)\n",
    "        self.fc2 = torch.nn.Linear(26, d_out)\n",
    "        \n",
    "#         self.dropout = torch.nn.Dropout(p=0.3) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.float().unsqueeze(dim=1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.relu(self.conv6(x))\n",
    "        x = x[:, -1]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(model, criterion, optimizer, num_epochs=25,\n",
    "        scheduler=None, log_interval=None):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Store losses and accuracies accross epochs\n",
    "    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n",
    "\n",
    "    # tscv = TimeSeriesSplit(n_splits=5, max_train_size=5000)\n",
    "    kf = KFold(n_splits=9)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.X.reshape(dataset.X.shape[0],-1), dataset.y, test_size=0.1, random_state=0\n",
    "    )\n",
    "    confusion_matrix = torch.zeros(8, 8)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        if log_interval is not None and i % log_interval == 0:\n",
    "            print('Epoch {}/{}'.format(i, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "        # for fold, (train_index, test_index) in enumerate(tscv.split(X_train, y_train)):\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "            ### Dividing data into folds\n",
    "            x_train_fold = X_train[train_index]\n",
    "            x_test_fold = X_train[test_index]\n",
    "            y_train_fold = y_train[train_index]\n",
    "            y_test_fold = y_train[test_index]\n",
    "\n",
    "            print('Train Index Length:', len(x_train_fold), end='\\t\\t')\n",
    "            print('Test Index Length:', len(x_test_fold), end='\\n\\n')\n",
    "\n",
    "            train = torch.utils.data.TensorDataset(torch.tensor(x_train_fold), torch.tensor(y_train_fold))\n",
    "            test = torch.utils.data.TensorDataset(torch.tensor(x_test_fold), torch.tensor(y_test_fold))\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size = 20, shuffle = False)\n",
    "            test_loader = torch.utils.data.DataLoader(test, batch_size = 20, shuffle = False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            model.train()\n",
    "            for batch_index, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch)\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "                for t, p in zip(y_batch.view(-1), preds.view(-1)):\n",
    "                        confusion_matrix[t.long(), p.long()] += 1\n",
    "                single_loss = criterion(y_pred, y_batch.long().view(-1))\n",
    "                single_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += single_loss.item() * x_batch.size(0)\n",
    "                running_corrects += torch.sum(preds == y_batch.data)\n",
    "            print('Fold No. {}/{}\\tEpoch {}/{}\\t'.format(fold + 1 , kf.get_n_splits(X_train), i + 1, num_epochs), end='')\n",
    "            print(f'loss: {single_loss.item():10.8f}')\n",
    "            \n",
    "            nsamples = len(train_index)\n",
    "            epoch_loss = running_loss / nsamples\n",
    "            epoch_acc = running_corrects.double() / nsamples\n",
    "\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            if log_interval is not None and i % log_interval == 0:\n",
    "                print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n",
    "                    phase, epoch_loss, 100 * epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n",
    "    print()\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(D_in, 64, D_out)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.012, momentum=0.4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, num_epochs=NUM_EPOCHS, log_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 120\n",
    "overlap = 110\n",
    "print(D_out, window_size, overlap, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'CNN_Model_moves_{D_out}_ws{window_size}_ol{overlap}_epoch{NUM_EPOCHS}'\n",
    "torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(D_in, 70, D_out)\n",
    "cnn_model.load(name)\n",
    "cnn_model.eval()\n",
    "\n",
    "for to_predict in range(D_out):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = cnn_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(df, window_size):\n",
    "    full_features = np.array([])\n",
    "    axis = ['accel1', 'accel2', 'accel3', 'gyro1', 'gyro2', 'gyro3']\n",
    "    titles = np.ravel(np.array([i+'_'+j for i in feature_list for j in axis]))\n",
    "\n",
    "    # print(\"Begin Feature Extraction\")\n",
    "    windows = set_sliding_windows(df, 110, window_size)\n",
    "    # print(windows.shape)\n",
    "    # windows = set_windows(df, window_size)\n",
    "\n",
    "    for window in windows:\n",
    "        for _,ax in enumerate(window.T):\n",
    "                full_features = np.append(full_features, add_mean(ax))\n",
    "                full_features = np.append(full_features, add_max(ax))\n",
    "                full_features = np.append(full_features, add_min(ax))\n",
    "                full_features = np.append(full_features, add_median(ax))\n",
    "                full_features = np.append(full_features, add_gradient(ax))\n",
    "                full_features = np.append(full_features, add_std(ax))\n",
    "                full_features = np.append(full_features, add_iqr(ax))\n",
    "                # full_features = np.append(full_features, add_skew(ax))\n",
    "                full_features = np.append(full_features, add_zero_crossing_count(ax))\n",
    "                # full_features = np.append(full_features, add_cwt(ax))\n",
    "                full_features = np.append(full_features, add_no_peaks(ax))\n",
    "                full_features = np.append(full_features, add_recurring_dp(ax))\n",
    "                # full_features = np.append(full_features, add_ratio_v_tsl(ax))\n",
    "                # full_features = np.append(full_features, add_sum_recurring_dp(ax))\n",
    "                full_features = np.append(full_features, add_var_coeff(ax))\n",
    "                full_features = np.append(full_features, add_kurtosis(ax)) \n",
    "\n",
    "    full_features = full_features.reshape(\n",
    "        -1,\n",
    "        len(feature_list) * 6,\n",
    "    )   \n",
    "    full_features_df = pd.DataFrame(full_features)\n",
    "    full_features_df.columns = titles\n",
    "    return full_features_df\n",
    "\n",
    "def feature_extraction(data):\n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    if 'dance' in data:\n",
    "        del data['dance']\n",
    "\n",
    "    df = data.apply(pd.to_numeric).interpolate(method='polynomial', order=2)\n",
    "    col = df.columns\n",
    "    # X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)\n",
    "    df_scaled = df.apply(lambda x: (x - min(x)) / (max(x) - min(x)))\n",
    "    # min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # df_scaled = min_max_scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(df_scaled, columns=col)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # print(df.shape)\n",
    "    features = feature_extract(df, window_size=120).reset_index(drop=True)\n",
    "    # print(features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cnn_model = CNN(D_in, 70, D_out)\n",
    "cnn_model.load(name)\n",
    "cnn_model.eval()\n",
    "\n",
    "feature_list = [\n",
    "    'mean', \n",
    "    'max', \n",
    "    'min', \n",
    "    'median', \n",
    "    'gradient', \n",
    "    'std', \n",
    "    'iqr', \n",
    "    # 'skew', \n",
    "    'zero_crossing',\n",
    "    # 'cwt', \n",
    "    'no_peaks', \n",
    "    'recurring_dp', \n",
    "    # 'ratio_v_tsl', \n",
    "    # 'sum_recurring_dp', \n",
    "    'var_coeff', \n",
    "    'kurtosis'\n",
    "]\n",
    "\n",
    "dances = ['dab', 'elbowkick', 'gun', 'hair', 'listen', 'pointhigh', 'sidepump', 'wipetable']\n",
    "# dances = ['gun', 'hair', 'sidepump']\n",
    "# dances = ['elbowkick', 'pointhigh', 'wipetable']\n",
    "persons = ['kelvin', 'guiyong', 'xiaoxue', 'john']\n",
    "beetles = ['1', '2']\n",
    "\n",
    "test_range = 4\n",
    "leap = 160\n",
    "truth, total, skipped = 0,0,0\n",
    "for i in range(1,1+test_range):\n",
    "    print(\"Phase:\", i)\n",
    "    start, end = i * leap, i * leap + leap\n",
    "    for d in dances:\n",
    "        print(d)\n",
    "        df_full = pd.DataFrame()\n",
    "        collection = [np.array([]) for x in range(16)]\n",
    "        j = 0\n",
    "        for p in persons:\n",
    "            for b in beetles:\n",
    "                move_json = 'collected_data/' + d + b + '_' + p + '.json'\n",
    "                with open(move_json) as f:\n",
    "                    x = json.load(f)\n",
    "                x = pd.DataFrame.from_dict(x)[start:end]\n",
    "                df_target = torch.from_numpy(np.array(feature_extraction(x)))\n",
    "                output = cnn_model.predict(df_target)\n",
    "                proba_dict = {}\n",
    "\n",
    "                for x in output:\n",
    "                    x = int(x)\n",
    "                    if x not in proba_dict:\n",
    "                        proba_dict[x] = 1\n",
    "                    else:\n",
    "                        proba_dict[x] += 1\n",
    "                for k in proba_dict.keys():\n",
    "                    proba_dict[k] /= len(output)\n",
    "\n",
    "                print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.X, dataset.y, stratify=dataset.y,random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=15).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "clf.predict(X_test[:5, :])\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
