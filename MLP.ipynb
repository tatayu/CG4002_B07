{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dance move: 0, Name: gun_john\n",
      "Dance move: 1, Name: hair_john\n",
      "Dance move: 2, Name: sidepump_john\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit, cross_val_score\n",
    "\n",
    "import brevitas.nn as nn\n",
    "\n",
    "from config import *\n",
    "from classic_models import *\n",
    "from data_preprocessing import *\n",
    "from feature_extraction import *\n",
    "from helpers import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_accel1</th>\n",
       "      <th>mean_accel2</th>\n",
       "      <th>mean_accel3</th>\n",
       "      <th>mean_gyro1</th>\n",
       "      <th>mean_gyro2</th>\n",
       "      <th>mean_gyro3</th>\n",
       "      <th>max_accel1</th>\n",
       "      <th>max_accel2</th>\n",
       "      <th>max_accel3</th>\n",
       "      <th>max_gyro1</th>\n",
       "      <th>...</th>\n",
       "      <th>var_coeff_gyro1</th>\n",
       "      <th>var_coeff_gyro2</th>\n",
       "      <th>var_coeff_gyro3</th>\n",
       "      <th>kurtosis_accel1</th>\n",
       "      <th>kurtosis_accel2</th>\n",
       "      <th>kurtosis_accel3</th>\n",
       "      <th>kurtosis_gyro1</th>\n",
       "      <th>kurtosis_gyro2</th>\n",
       "      <th>kurtosis_gyro3</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321713</td>\n",
       "      <td>0.587304</td>\n",
       "      <td>0.19189</td>\n",
       "      <td>0.224644</td>\n",
       "      <td>0.132784</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.412742</td>\n",
       "      <td>-1.510622</td>\n",
       "      <td>0.425019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512475</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.465719</td>\n",
       "      <td>0.091719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.178974</td>\n",
       "      <td>-1.346002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.261652</td>\n",
       "      <td>0.499783</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.215231</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.378192</td>\n",
       "      <td>0.209323</td>\n",
       "      <td>0.482450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510602</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.086363</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>-1.079949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.221092</td>\n",
       "      <td>0.355008</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.035674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.161354</td>\n",
       "      <td>4.649814</td>\n",
       "      <td>0.501705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472475</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.177258</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294353</td>\n",
       "      <td>0.846729</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.217499</td>\n",
       "      <td>0.173952</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.590965</td>\n",
       "      <td>3.163390</td>\n",
       "      <td>0.471481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363211</td>\n",
       "      <td>0.583612</td>\n",
       "      <td>0.053512</td>\n",
       "      <td>0.433110</td>\n",
       "      <td>0.152831</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.420778</td>\n",
       "      <td>-0.618947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.420980</td>\n",
       "      <td>0.906493</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.237485</td>\n",
       "      <td>0.272744</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.647879</td>\n",
       "      <td>-1.184808</td>\n",
       "      <td>0.502093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.481605</td>\n",
       "      <td>0.053512</td>\n",
       "      <td>0.400502</td>\n",
       "      <td>0.149519</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>-1.462913</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_accel1  mean_accel2  mean_accel3  mean_gyro1  mean_gyro2  mean_gyro3  \\\n",
       "0     0.321713     0.587304      0.19189    0.224644    0.132784         3.0   \n",
       "1     0.261652     0.499783      0.16476    0.215231    0.098955         3.0   \n",
       "2     0.221092     0.355008      0.16476    0.212382    0.035674         3.0   \n",
       "3     0.294353     0.846729      0.16476    0.217499    0.173952         3.0   \n",
       "4     0.420980     0.906493      0.16476    0.237485    0.272744         4.0   \n",
       "\n",
       "   max_accel1  max_accel2  max_accel3  max_gyro1  ...  var_coeff_gyro1  \\\n",
       "0        0.08    0.412742   -1.510622   0.425019  ...         0.512475   \n",
       "1        0.12    0.378192    0.209323   0.482450  ...         0.510602   \n",
       "2        0.12    0.161354    4.649814   0.501705  ...         0.472475   \n",
       "3        0.08    0.590965    3.163390   0.471481  ...         0.363211   \n",
       "4        0.00    0.647879   -1.184808   0.502093  ...         0.320100   \n",
       "\n",
       "   var_coeff_gyro2  var_coeff_gyro3  kurtosis_accel1  kurtosis_accel2  \\\n",
       "0         0.670569         0.403010         0.465719         0.091719   \n",
       "1         0.670569         0.403010         0.469900         0.086363   \n",
       "2         0.670569         0.177258         0.461538         0.115898   \n",
       "3         0.583612         0.053512         0.433110         0.152831   \n",
       "4         0.481605         0.053512         0.400502         0.149519   \n",
       "\n",
       "   kurtosis_accel3  kurtosis_gyro1  kurtosis_gyro2  kurtosis_gyro3  tag  \n",
       "0              1.0            0.46        0.178974       -1.346002  0.0  \n",
       "1              2.0            0.34        0.169140       -1.079949  0.0  \n",
       "2              2.0            0.34        0.245300        0.466311  0.0  \n",
       "3              1.0            0.30        0.420778       -0.618947  0.0  \n",
       "4              1.0            0.26        0.467100       -1.462913  0.0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "df = pd.read_csv('out_2.csv')\n",
    "# temp = df['tag']\n",
    "# del df['tag']\n",
    "\n",
    "# x = df.values #returns a numpy array\n",
    "# col = df.columns\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# df = pd.DataFrame(x_scaled, columns=col)\n",
    "\n",
    "# df['tag'] = temp\n",
    "df['tag'] = df['tag'].apply(lambda x: x-1)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df_np = df.to_numpy()\n",
    "\n",
    "        self.X = df_np[:,:-1]\n",
    "        self.y = df_np[:,-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get item by index\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns length of data\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureDataset(df_train)\n",
    "D_in = df.shape[1]-1\n",
    "D_out = len(dances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.linear1 = nn.QuantLinear(d_in, d_hidden, bias=True)#, weight_bit_width=4)\n",
    "        self.linear2 = nn.QuantLinear(d_hidden, d_hidden, bias=True)#, weight_bit_width=4)\n",
    "        self.linear3 = nn.QuantLinear(d_hidden, d_out, bias=False)#, weight_bit_width=4)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.linear1(X.float())\n",
    "        X = self.linear2(X)\n",
    "        X = self.dropout(X)\n",
    "        X = self.linear3(X)\n",
    "        X = self.dropout(X)\n",
    "        return torch.nn.functional.log_softmax(X, dim=1)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(d_in, d_hidden)\n",
    "        self.linear2 = torch.nn.Linear(d_hidden, d_hidden)\n",
    "        self.linear3 = torch.nn.Linear(d_hidden, d_out)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.linear1(X.float())\n",
    "        X = self.linear2(X)\n",
    "        X = self.dropout(X)\n",
    "        X = self.linear3(X)\n",
    "        X = self.dropout(X)\n",
    "        return torch.nn.functional.log_softmax(X, dim=1)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(model, criterion, optimizer, X, y, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Store losses and accuracies accross epochs\n",
    "    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    for i in range(1,num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(i, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "        \n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X_train, y_train)):\n",
    "            ### Dividing data into folds\n",
    "            x_train_fold = X_train[train_index]\n",
    "            x_test_fold = X_train[test_index]\n",
    "            y_train_fold = y_train[train_index]\n",
    "            y_test_fold = y_train[test_index]\n",
    "\n",
    "            print('Train Index Length:', len(train_index), end='\\t\\t')\n",
    "            print('Test Index Length:', len(test_index), end='\\n\\n')\n",
    "\n",
    "            train = torch.utils.data.TensorDataset(torch.tensor(x_train_fold), torch.tensor(y_train_fold))\n",
    "            test = torch.utils.data.TensorDataset(torch.tensor(x_test_fold), torch.tensor(y_test_fold))\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = False)\n",
    "            test_loader = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for batch_index, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch)\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "                single_loss = criterion(y_pred, y_batch.long())\n",
    "                single_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += single_loss.item() * x_batch.size(0)\n",
    "                running_corrects += torch.sum(preds == y_batch.data)\n",
    "            print('Fold No. {}/{}\\tEpoch {}/{}\\t'.format(fold + 1 , tscv.get_n_splits(), i, num_epochs), end='')\n",
    "            print(f'loss: {single_loss.item():10.8f}')\n",
    "            \n",
    "            nsamples = len(train_index)\n",
    "            epoch_loss = running_loss / nsamples\n",
    "            epoch_acc = running_corrects.double() / nsamples\n",
    "\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n",
    "                    phase, epoch_loss, 100 * epoch_acc)\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 54])\n",
      "torch.Size([50])\n",
      "torch.Size([50, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([3, 50])\n",
      "torch.Size([3])\n",
      "Total number of parameters = 5453\n"
     ]
    }
   ],
   "source": [
    "model = MLP(D_in, 50, D_out)\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 1/10\tloss: 1.11185753\n",
      "val Loss: 1.1265 Acc: 23.33%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 1/10\tloss: 0.89893734\n",
      "val Loss: 0.9403 Acc: 66.10%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 1/10\tloss: 0.90135968\n",
      "val Loss: 0.7288 Acc: 73.86%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 1/10\tloss: 0.41855168\n",
      "val Loss: 0.6053 Acc: 76.07%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 1/10\tloss: 0.30475113\n",
      "val Loss: 0.4965 Acc: 78.08%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 1/10\tloss: 0.68643111\n",
      "val Loss: 0.5012 Acc: 80.57%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 1/10\tloss: 0.01804710\n",
      "val Loss: 0.4169 Acc: 81.37%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 1/10\tloss: 0.12955350\n",
      "val Loss: 0.4291 Acc: 82.40%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 1/10\tloss: 0.07477210\n",
      "val Loss: 0.3944 Acc: 84.35%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 1/10\tloss: 0.53247976\n",
      "val Loss: 0.5532 Acc: 83.85%\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 2/10\tloss: 1.12130117\n",
      "val Loss: 0.8769 Acc: 70.00%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 2/10\tloss: 0.59037209\n",
      "val Loss: 0.5163 Acc: 79.66%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 2/10\tloss: 0.36705005\n",
      "val Loss: 0.4488 Acc: 78.41%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 2/10\tloss: 0.18823396\n",
      "val Loss: 0.3464 Acc: 83.76%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 2/10\tloss: 0.15144236\n",
      "val Loss: 0.2995 Acc: 87.67%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 2/10\tloss: 0.25709349\n",
      "val Loss: 0.3194 Acc: 86.29%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 2/10\tloss: 0.02419017\n",
      "val Loss: 0.3141 Acc: 86.27%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 2/10\tloss: 0.14456372\n",
      "val Loss: 0.2948 Acc: 86.70%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 2/10\tloss: 0.00885346\n",
      "val Loss: 0.2925 Acc: 86.26%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 2/10\tloss: 0.37709510\n",
      "val Loss: 0.4335 Acc: 85.57%\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 3/10\tloss: 0.83269930\n",
      "val Loss: 0.6043 Acc: 80.00%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 3/10\tloss: 0.52573067\n",
      "val Loss: 0.4007 Acc: 84.75%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 3/10\tloss: 0.27558818\n",
      "val Loss: 0.4097 Acc: 81.82%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 3/10\tloss: 0.13375767\n",
      "val Loss: 0.2558 Acc: 89.74%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 3/10\tloss: 0.12041774\n",
      "val Loss: 0.2314 Acc: 91.10%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 3/10\tloss: 0.17220272\n",
      "val Loss: 0.2587 Acc: 86.29%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 3/10\tloss: 0.03099932\n",
      "val Loss: 0.2822 Acc: 87.75%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 3/10\tloss: 0.09774800\n",
      "val Loss: 0.2499 Acc: 88.41%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 3/10\tloss: 0.00412094\n",
      "val Loss: 0.2546 Acc: 87.02%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 3/10\tloss: 0.28165665\n",
      "val Loss: 0.3699 Acc: 86.60%\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 4/10\tloss: 0.63865304\n",
      "val Loss: 0.4099 Acc: 83.33%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 4/10\tloss: 0.43153402\n",
      "val Loss: 0.2767 Acc: 89.83%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 4/10\tloss: 0.25523266\n",
      "val Loss: 0.3558 Acc: 82.95%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 4/10\tloss: 0.14299645\n",
      "val Loss: 0.2076 Acc: 93.16%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 4/10\tloss: 0.08778881\n",
      "val Loss: 0.1891 Acc: 92.47%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 4/10\tloss: 0.07685845\n",
      "val Loss: 0.2188 Acc: 89.14%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 4/10\tloss: 0.03785921\n",
      "val Loss: 0.2399 Acc: 92.16%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 4/10\tloss: 0.06205228\n",
      "val Loss: 0.2446 Acc: 89.70%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 4/10\tloss: 0.00162139\n",
      "val Loss: 0.2221 Acc: 90.46%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 4/10\tloss: 0.26991335\n",
      "val Loss: 0.3246 Acc: 89.00%\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 5/10\tloss: 0.66646570\n",
      "val Loss: 0.4107 Acc: 83.33%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 5/10\tloss: 0.33790019\n",
      "val Loss: 0.2143 Acc: 91.53%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 5/10\tloss: 0.39572906\n",
      "val Loss: 0.2391 Acc: 90.91%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 5/10\tloss: 0.14840834\n",
      "val Loss: 0.2139 Acc: 90.60%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 5/10\tloss: 0.11236019\n",
      "val Loss: 0.1678 Acc: 93.15%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 5/10\tloss: 0.00624251\n",
      "val Loss: 0.3293 Acc: 90.29%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 5/10\tloss: 0.03222827\n",
      "val Loss: 0.1913 Acc: 92.65%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 5/10\tloss: 0.07071731\n",
      "val Loss: 0.2303 Acc: 91.85%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 5/10\tloss: 0.00163351\n",
      "val Loss: 0.1960 Acc: 92.75%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 5/10\tloss: 0.17529161\n",
      "val Loss: 0.2939 Acc: 91.07%\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 6/10\tloss: 0.74535751\n",
      "val Loss: 0.4397 Acc: 80.00%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 6/10\tloss: 0.27045056\n",
      "val Loss: 0.1824 Acc: 93.22%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 6/10\tloss: 0.55623275\n",
      "val Loss: 0.2120 Acc: 92.05%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 6/10\tloss: 0.16162740\n",
      "val Loss: 0.1924 Acc: 93.16%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 6/10\tloss: 0.15834410\n",
      "val Loss: 0.1895 Acc: 92.47%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 6/10\tloss: 0.01382020\n",
      "val Loss: 0.3925 Acc: 88.00%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 6/10\tloss: 0.02769526\n",
      "val Loss: 0.1945 Acc: 92.16%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 6/10\tloss: 0.07939986\n",
      "val Loss: 0.2014 Acc: 92.70%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 6/10\tloss: 0.00357756\n",
      "val Loss: 0.1865 Acc: 92.75%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 6/10\tloss: 0.07453389\n",
      "val Loss: 0.2766 Acc: 92.44%\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 7/10\tloss: 0.73561156\n",
      "val Loss: 0.4103 Acc: 80.00%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 7/10\tloss: 0.19522092\n",
      "val Loss: 0.1627 Acc: 96.61%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 7/10\tloss: 0.68555504\n",
      "val Loss: 0.2289 Acc: 92.05%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 7/10\tloss: 0.20720623\n",
      "val Loss: 0.1867 Acc: 89.74%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 7/10\tloss: 0.07768493\n",
      "val Loss: 0.1933 Acc: 93.15%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 7/10\tloss: 0.00792733\n",
      "val Loss: 0.3252 Acc: 89.14%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 7/10\tloss: 0.02194838\n",
      "val Loss: 0.1777 Acc: 93.14%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No. 8/10\tEpoch 7/10\tloss: 0.07453874\n",
      "val Loss: 0.1945 Acc: 93.56%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 7/10\tloss: 0.00731523\n",
      "val Loss: 0.1996 Acc: 93.51%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 7/10\tloss: 0.03479822\n",
      "val Loss: 0.2732 Acc: 91.75%\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 8/10\tloss: 0.70751333\n",
      "val Loss: 0.3975 Acc: 83.33%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 8/10\tloss: 0.18518749\n",
      "val Loss: 0.1559 Acc: 94.92%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 8/10\tloss: 0.76385337\n",
      "val Loss: 0.2448 Acc: 89.77%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 8/10\tloss: 0.30151352\n",
      "val Loss: 0.1826 Acc: 91.45%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 8/10\tloss: 0.03596721\n",
      "val Loss: 0.1892 Acc: 92.47%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 8/10\tloss: 0.01545222\n",
      "val Loss: 0.2727 Acc: 90.86%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 8/10\tloss: 0.02157475\n",
      "val Loss: 0.1642 Acc: 92.16%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 8/10\tloss: 0.07522570\n",
      "val Loss: 0.1922 Acc: 92.70%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 8/10\tloss: 0.00312642\n",
      "val Loss: 0.2330 Acc: 91.22%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 8/10\tloss: 0.01760265\n",
      "val Loss: 0.2389 Acc: 92.10%\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 9/10\tloss: 0.69958311\n",
      "val Loss: 0.4239 Acc: 83.33%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 9/10\tloss: 0.19128245\n",
      "val Loss: 0.1621 Acc: 96.61%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 9/10\tloss: 0.93716812\n",
      "val Loss: 0.2764 Acc: 89.77%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 9/10\tloss: 0.31592444\n",
      "val Loss: 0.1810 Acc: 90.60%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 9/10\tloss: 0.02385201\n",
      "val Loss: 0.1815 Acc: 93.15%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 9/10\tloss: 0.02101136\n",
      "val Loss: 0.2418 Acc: 89.71%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 9/10\tloss: 0.02103815\n",
      "val Loss: 0.1607 Acc: 91.67%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 9/10\tloss: 0.06828102\n",
      "val Loss: 0.1877 Acc: 93.56%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 9/10\tloss: 0.00261729\n",
      "val Loss: 0.2250 Acc: 92.37%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 9/10\tloss: 0.01179766\n",
      "val Loss: 0.2272 Acc: 92.10%\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Index Length: 30\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 1/10\tEpoch 10/10\tloss: 0.65666270\n",
      "val Loss: 0.4253 Acc: 83.33%\n",
      "\n",
      "Train Index Length: 59\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 2/10\tEpoch 10/10\tloss: 0.18660791\n",
      "val Loss: 0.1573 Acc: 96.61%\n",
      "\n",
      "Train Index Length: 88\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 3/10\tEpoch 10/10\tloss: 1.03902137\n",
      "val Loss: 0.2864 Acc: 90.91%\n",
      "\n",
      "Train Index Length: 117\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 4/10\tEpoch 10/10\tloss: 0.32617599\n",
      "val Loss: 0.1786 Acc: 89.74%\n",
      "\n",
      "Train Index Length: 146\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 5/10\tEpoch 10/10\tloss: 0.01601456\n",
      "val Loss: 0.1762 Acc: 93.84%\n",
      "\n",
      "Train Index Length: 175\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 6/10\tEpoch 10/10\tloss: 0.01563366\n",
      "val Loss: 0.2056 Acc: 90.29%\n",
      "\n",
      "Train Index Length: 204\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 7/10\tEpoch 10/10\tloss: 0.02197192\n",
      "val Loss: 0.1709 Acc: 91.18%\n",
      "\n",
      "Train Index Length: 233\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 8/10\tEpoch 10/10\tloss: 0.06412911\n",
      "val Loss: 0.1922 Acc: 93.13%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 9/10\tEpoch 10/10\tloss: 0.00580012\n",
      "val Loss: 0.1839 Acc: 93.51%\n",
      "\n",
      "Train Index Length: 291\t\tTest Index Length: 29\n",
      "\n",
      "Fold No. 10/10\tEpoch 10/10\tloss: 0.00786730\n",
      "val Loss: 0.2388 Acc: 91.41%\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val Acc: 96.61%\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataset.X, dataset.y, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MLP_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8297872340425532, 2: 0.10638297872340426, 1: 0.06382978723404255}\n",
      "{1: 0.8627450980392157, 0: 0.13725490196078433}\n",
      "{2: 0.9761904761904762, 0: 0.023809523809523808}\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(D_in, 50, D_out)\n",
    "mlp_model.load('MLP_Model')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(D_out):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical = df.copy()\n",
    "df_hierarchical['tag'] = df_hierarchical['tag'].apply(lambda x: 0 if x <= 7 else 1)\n",
    "\n",
    "msk = np.random.rand(len(df_hierarchical)) < 0.8\n",
    "df_train = df_hierarchical[msk]\n",
    "df_test = df_hierarchical[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical = MLP(D_in, 50, 2)\n",
    "optimizer = torch.optim.SGD(model_hierarchical.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical, criterion, optimizer, dataset.X, dataset.y, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 2)\n",
    "mlp_model.load('MLP_Model_Hierarchical_1')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(2):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical_2 = df.copy()\n",
    "df_hierarchical_2 = df_hierarchical_2[df_hierarchical_2['tag'] >= 8]\n",
    "df_hierarchical_2 = df_hierarchical_2.apply(lambda x: x-8)\n",
    "msk = np.random.rand(len(df_hierarchical_2)) < 0.8\n",
    "df_train = df_hierarchical_2[msk]\n",
    "df_test = df_hierarchical_2[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)\n",
    "\n",
    "model_hierarchical_2 = MLP(D_in, 50, 2)\n",
    "# optimizer = torch.optim.Adam(model_hierarchical_2.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(model_hierarchical_2.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical_2, criterion, optimizer, dataset.X, dataset.y, num_epochs=20)\n",
    "\n",
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 2)\n",
    "mlp_model.load('MLP_Model_Hierarchical_2')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(2):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x) + 8\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical_3 = df.copy()\n",
    "df_hierarchical_3 = df_hierarchical_3[df_hierarchical_3['tag'] < 8]\n",
    "msk = np.random.rand(len(df_hierarchical_3)) < 0.8\n",
    "df_train = df_hierarchical_3[msk]\n",
    "df_test = df_hierarchical_3[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)\n",
    "\n",
    "model_hierarchical_3 = MLP(D_in, 50, 8)\n",
    "# optimizer = torch.optim.Adam(model_hierarchical_2.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(model_hierarchical_3.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical_3, criterion, optimizer, dataset.X, dataset.y, num_epochs=30)\n",
    "\n",
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 8)\n",
    "mlp_model.load('MLP_Model_Hierarchical_3')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(8):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
