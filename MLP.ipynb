{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dance move: 1, Name: gun_john\n",
      "Dance move: 2, Name: hair_john\n",
      "Dance move: 3, Name: sidepump_john\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit, cross_val_score\n",
    "\n",
    "import brevitas.nn as nn\n",
    "\n",
    "from config import *\n",
    "from classic_models import *\n",
    "from data_preprocessing import *\n",
    "from feature_extraction import *\n",
    "from helpers import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_accel1</th>\n",
       "      <th>mean_accel2</th>\n",
       "      <th>mean_accel3</th>\n",
       "      <th>mean_gyro1</th>\n",
       "      <th>mean_gyro2</th>\n",
       "      <th>mean_gyro3</th>\n",
       "      <th>max_accel1</th>\n",
       "      <th>max_accel2</th>\n",
       "      <th>max_accel3</th>\n",
       "      <th>max_gyro1</th>\n",
       "      <th>...</th>\n",
       "      <th>var_coeff_gyro1</th>\n",
       "      <th>var_coeff_gyro2</th>\n",
       "      <th>var_coeff_gyro3</th>\n",
       "      <th>kurtosis_accel1</th>\n",
       "      <th>kurtosis_accel2</th>\n",
       "      <th>kurtosis_accel3</th>\n",
       "      <th>kurtosis_gyro1</th>\n",
       "      <th>kurtosis_gyro2</th>\n",
       "      <th>kurtosis_gyro3</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321713</td>\n",
       "      <td>0.587304</td>\n",
       "      <td>0.19189</td>\n",
       "      <td>0.224644</td>\n",
       "      <td>0.132784</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.412742</td>\n",
       "      <td>-1.510622</td>\n",
       "      <td>0.425019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512475</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.465719</td>\n",
       "      <td>0.091719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.178974</td>\n",
       "      <td>-1.346002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.290898</td>\n",
       "      <td>0.515375</td>\n",
       "      <td>0.18489</td>\n",
       "      <td>0.219044</td>\n",
       "      <td>0.117643</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.404411</td>\n",
       "      <td>-1.121981</td>\n",
       "      <td>0.454103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506555</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.089168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.176028</td>\n",
       "      <td>-1.119453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261652</td>\n",
       "      <td>0.499783</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.215231</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.378192</td>\n",
       "      <td>0.209323</td>\n",
       "      <td>0.482450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510602</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.086363</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>-1.079949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.236798</td>\n",
       "      <td>0.463143</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.305106</td>\n",
       "      <td>3.957013</td>\n",
       "      <td>0.498670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507860</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.341137</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.089625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.176476</td>\n",
       "      <td>-0.980839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221092</td>\n",
       "      <td>0.355008</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.035674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.161354</td>\n",
       "      <td>4.649814</td>\n",
       "      <td>0.501705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472475</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.177258</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_accel1  mean_accel2  mean_accel3  mean_gyro1  mean_gyro2  mean_gyro3  \\\n",
       "0     0.321713     0.587304      0.19189    0.224644    0.132784         3.0   \n",
       "1     0.290898     0.515375      0.18489    0.219044    0.117643         3.0   \n",
       "2     0.261652     0.499783      0.16476    0.215231    0.098955         3.0   \n",
       "3     0.236798     0.463143      0.16476    0.212382    0.072249         3.0   \n",
       "4     0.221092     0.355008      0.16476    0.212382    0.035674         3.0   \n",
       "\n",
       "   max_accel1  max_accel2  max_accel3  max_gyro1  ...  var_coeff_gyro1  \\\n",
       "0        0.08    0.412742   -1.510622   0.425019  ...         0.512475   \n",
       "1        0.12    0.404411   -1.121981   0.454103  ...         0.506555   \n",
       "2        0.12    0.378192    0.209323   0.482450  ...         0.510602   \n",
       "3        0.12    0.305106    3.957013   0.498670  ...         0.507860   \n",
       "4        0.12    0.161354    4.649814   0.501705  ...         0.472475   \n",
       "\n",
       "   var_coeff_gyro2  var_coeff_gyro3  kurtosis_accel1  kurtosis_accel2  \\\n",
       "0         0.670569         0.403010         0.465719         0.091719   \n",
       "1         0.670569         0.403010         0.467391         0.089168   \n",
       "2         0.670569         0.403010         0.469900         0.086363   \n",
       "3         0.670569         0.341137         0.469900         0.089625   \n",
       "4         0.670569         0.177258         0.461538         0.115898   \n",
       "\n",
       "   kurtosis_accel3  kurtosis_gyro1  kurtosis_gyro2  kurtosis_gyro3  tag  \n",
       "0              1.0            0.46        0.178974       -1.346002  0.0  \n",
       "1              1.0            0.42        0.176028       -1.119453  0.0  \n",
       "2              2.0            0.34        0.169140       -1.079949  0.0  \n",
       "3              2.0            0.38        0.176476       -0.980839  0.0  \n",
       "4              2.0            0.34        0.245300        0.466311  0.0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "df = pd.read_csv('out_2.csv')\n",
    "# temp = df['tag']\n",
    "# del df['tag']\n",
    "\n",
    "# x = df.values #returns a numpy array\n",
    "# col = df.columns\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# df = pd.DataFrame(x_scaled, columns=col)\n",
    "\n",
    "# df['tag'] = temp\n",
    "df['tag'] = df['tag'].apply(lambda x: x-1)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df_np = df.to_numpy()\n",
    "\n",
    "        self.X = df_np[:,:-1]\n",
    "        self.y = df_np[:,-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get item by index\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns length of data\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureDataset(df_train)\n",
    "D_in = df.shape[1]-1\n",
    "D_out = len(dances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.linear1 = nn.QuantLinear(d_in, d_hidden, bias=True)#, weight_bit_width=4)\n",
    "        self.linear2 = nn.QuantLinear(d_hidden, d_hidden, bias=True)#, weight_bit_width=4)\n",
    "        self.linear3 = nn.QuantLinear(d_hidden, d_out, bias=False)#, weight_bit_width=4)\n",
    "        \n",
    "        self.relu = torch.nn.ReL(p=0.1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.linear1(X.float())\n",
    "        X = self.linear2(X)\n",
    "        X = self.linear3(X)\n",
    "        return torch.nn.functional.sigmoid(X)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.linear1 = nn.QuantLinear(d_in, d_hidden, bias=True)\n",
    "        self.linear2 = nn.QuantLinear(d_hidden, d_hidden//4, bias=True)\n",
    "        self.linear3 = nn.QuantLinear(d_hidden//4, d_out, bias=False)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.relu(self.linear1(X.float()))\n",
    "        # X = self.dropout(X)\n",
    "        X = self.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        # X = self.dropout(X)\n",
    "        return torch.nn.functional.log_softmax(X, dim=1)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(model, criterion, optimizer, X, y, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Store losses and accuracies accross epochs\n",
    "    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    for i in range(1,num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(i, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "        \n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X_train, y_train)):\n",
    "            ### Dividing data into folds\n",
    "            x_train_fold = X_train[train_index]\n",
    "            x_test_fold = X_train[test_index]\n",
    "            y_train_fold = y_train[train_index]\n",
    "            y_test_fold = y_train[test_index]\n",
    "\n",
    "            print('Train Index Length:', len(train_index), end='\\t\\t')\n",
    "            print('Test Index Length:', len(test_index), end='\\n\\n')\n",
    "\n",
    "            train = torch.utils.data.TensorDataset(torch.tensor(x_train_fold), torch.tensor(y_train_fold))\n",
    "            test = torch.utils.data.TensorDataset(torch.tensor(x_test_fold), torch.tensor(y_test_fold))\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = False)\n",
    "            test_loader = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for batch_index, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch)\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "                # print(y_pred.shape, y_batch.view(-1, 1).shape)\n",
    "                single_loss = criterion(y_pred, y_batch.long())\n",
    "                single_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += single_loss.item() * x_batch.size(0)\n",
    "                running_corrects += torch.sum(preds == y_batch.data)\n",
    "            print('Fold No. {}/{}\\tEpoch {}/{}\\t'.format(fold + 1 , tscv.get_n_splits(), i, num_epochs), end='')\n",
    "            print(f'loss: {single_loss.item():10.8f}')\n",
    "            \n",
    "            nsamples = len(train_index)\n",
    "            epoch_loss = running_loss / nsamples\n",
    "            epoch_acc = running_corrects.double() / nsamples\n",
    "\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n",
    "                    phase, epoch_loss, 100 * epoch_acc)\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 54])\n",
      "torch.Size([50])\n",
      "torch.Size([12, 50])\n",
      "torch.Size([12])\n",
      "torch.Size([3, 12])\n",
      "Total number of parameters = 3398\n"
     ]
    }
   ],
   "source": [
    "model = MLP(D_in, 50, D_out)\n",
    "# model = MultiHead4MLP(D_in, D_out)\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 1/8\tloss: 1.10257447\n",
      "val Loss: 1.1149 Acc: 22.22%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 1/8\tloss: 1.12289035\n",
      "val Loss: 1.1046 Acc: 25.00%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 1/8\tloss: 1.08551133\n",
      "val Loss: 1.0810 Acc: 34.58%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 1/8\tloss: 1.09281862\n",
      "val Loss: 1.0575 Acc: 39.86%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 1/8\tloss: 1.11058199\n",
      "val Loss: 1.0309 Acc: 55.03%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 1/8\tloss: 0.94719440\n",
      "val Loss: 0.9830 Acc: 63.00%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 1/8\tloss: 1.30484152\n",
      "val Loss: 0.9288 Acc: 61.47%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 1/8\tloss: 0.94847107\n",
      "val Loss: 0.8507 Acc: 66.03%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 1/8\tloss: 0.69571567\n",
      "val Loss: 0.7657 Acc: 69.28%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 1/8\tloss: 0.58319676\n",
      "val Loss: 0.6829 Acc: 70.06%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 1/8\tloss: 0.78786266\n",
      "val Loss: 0.6185 Acc: 76.90%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 1/8\tloss: 0.48769888\n",
      "val Loss: 0.5476 Acc: 79.79%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 1/8\tloss: 0.40778637\n",
      "val Loss: 0.4872 Acc: 82.25%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 1/8\tloss: 0.26565951\n",
      "val Loss: 0.4364 Acc: 81.92%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 1/8\tloss: 0.39698634\n",
      "val Loss: 0.4061 Acc: 83.30%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 1/8\tloss: 0.23361507\n",
      "val Loss: 0.3633 Acc: 84.71%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 1/8\tloss: 0.19533232\n",
      "val Loss: 0.3299 Acc: 86.69%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 1/8\tloss: 0.07800826\n",
      "val Loss: 0.3142 Acc: 86.71%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 1/8\tloss: 0.08972763\n",
      "val Loss: 0.2872 Acc: 87.40%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 1/8\tloss: 0.52064145\n",
      "val Loss: 0.2700 Acc: 88.96%\n",
      "\n",
      "Epoch 2/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 2/8\tloss: 0.28437972\n",
      "val Loss: 0.2478 Acc: 82.22%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 2/8\tloss: 0.03037558\n",
      "val Loss: 0.2210 Acc: 86.84%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 2/8\tloss: 0.10934296\n",
      "val Loss: 0.2424 Acc: 89.72%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 2/8\tloss: 0.23349467\n",
      "val Loss: 0.2457 Acc: 88.41%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 2/8\tloss: 0.31926924\n",
      "val Loss: 0.2401 Acc: 89.94%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 2/8\tloss: 0.38864008\n",
      "val Loss: 0.2435 Acc: 91.50%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 2/8\tloss: 0.10015503\n",
      "val Loss: 0.2397 Acc: 90.48%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 2/8\tloss: 1.09116220\n",
      "val Loss: 0.2286 Acc: 92.75%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 2/8\tloss: 0.74505097\n",
      "val Loss: 0.2936 Acc: 89.76%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 2/8\tloss: 0.13737065\n",
      "val Loss: 0.2296 Acc: 90.43%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 2/8\tloss: 0.51090980\n",
      "val Loss: 0.2324 Acc: 91.27%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 2/8\tloss: 0.09766024\n",
      "val Loss: 0.2247 Acc: 91.71%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 2/8\tloss: 0.08450903\n",
      "val Loss: 0.2155 Acc: 92.09%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 2/8\tloss: 0.04744190\n",
      "val Loss: 0.2080 Acc: 92.86%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 2/8\tloss: 0.14080876\n",
      "val Loss: 0.2200 Acc: 91.65%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 2/8\tloss: 0.10505960\n",
      "val Loss: 0.1965 Acc: 93.14%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 2/8\tloss: 0.02425089\n",
      "val Loss: 0.1883 Acc: 93.35%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 2/8\tloss: 0.00932825\n",
      "val Loss: 0.1878 Acc: 92.83%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 2/8\tloss: 0.04155562\n",
      "val Loss: 0.1759 Acc: 93.53%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 2/8\tloss: 0.62448102\n",
      "val Loss: 0.1710 Acc: 93.69%\n",
      "\n",
      "Epoch 3/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 3/8\tloss: 0.13523059\n",
      "val Loss: 0.1875 Acc: 91.11%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 3/8\tloss: 0.01044396\n",
      "val Loss: 0.1315 Acc: 97.37%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 3/8\tloss: 0.03833475\n",
      "val Loss: 0.1528 Acc: 95.33%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 3/8\tloss: 0.13735370\n",
      "val Loss: 0.1446 Acc: 93.48%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 3/8\tloss: 0.12901460\n",
      "val Loss: 0.1568 Acc: 93.49%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 3/8\tloss: 0.28984386\n",
      "val Loss: 0.1582 Acc: 93.50%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 3/8\tloss: 0.03158536\n",
      "val Loss: 0.1503 Acc: 93.94%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 3/8\tloss: 0.45498413\n",
      "val Loss: 0.1379 Acc: 94.66%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 3/8\tloss: 0.38275352\n",
      "val Loss: 0.1933 Acc: 91.81%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 3/8\tloss: 0.06604310\n",
      "val Loss: 0.1425 Acc: 93.83%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 3/8\tloss: 0.61010325\n",
      "val Loss: 0.1495 Acc: 94.08%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 3/8\tloss: 0.04848693\n",
      "val Loss: 0.1460 Acc: 94.04%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 3/8\tloss: 0.02817735\n",
      "val Loss: 0.1390 Acc: 94.48%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 3/8\tloss: 0.01392725\n",
      "val Loss: 0.1327 Acc: 94.42%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 3/8\tloss: 0.06672222\n",
      "val Loss: 0.1395 Acc: 93.95%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 3/8\tloss: 0.07663493\n",
      "val Loss: 0.1243 Acc: 95.69%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 3/8\tloss: 0.00388059\n",
      "val Loss: 0.1201 Acc: 95.56%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 3/8\tloss: 0.00282285\n",
      "val Loss: 0.1228 Acc: 95.10%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 3/8\tloss: 0.01548107\n",
      "val Loss: 0.1185 Acc: 95.69%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 3/8\tloss: 0.72576177\n",
      "val Loss: 0.1149 Acc: 95.90%\n",
      "\n",
      "Epoch 4/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 4/8\tloss: 0.04645506\n",
      "val Loss: 0.2449 Acc: 93.33%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 4/8\tloss: 0.00490793\n",
      "val Loss: 0.0804 Acc: 98.68%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 4/8\tloss: 0.02056425\n",
      "val Loss: 0.0949 Acc: 97.20%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 4/8\tloss: 0.08343707\n",
      "val Loss: 0.0888 Acc: 96.38%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 4/8\tloss: 0.07896429\n",
      "val Loss: 0.1031 Acc: 95.86%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 4/8\tloss: 0.23081413\n",
      "val Loss: 0.1019 Acc: 95.50%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 4/8\tloss: 0.01337932\n",
      "val Loss: 0.0989 Acc: 95.67%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 4/8\tloss: 0.26845402\n",
      "val Loss: 0.0875 Acc: 96.56%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No. 9/20\tEpoch 4/8\tloss: 0.23569131\n",
      "val Loss: 0.1292 Acc: 94.54%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 4/8\tloss: 0.01880061\n",
      "val Loss: 0.0904 Acc: 96.91%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 4/8\tloss: 0.71927857\n",
      "val Loss: 0.0966 Acc: 96.34%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 4/8\tloss: 0.02893251\n",
      "val Loss: 0.0933 Acc: 96.63%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 4/8\tloss: 0.01007351\n",
      "val Loss: 0.0873 Acc: 97.36%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 4/8\tloss: 0.00483936\n",
      "val Loss: 0.0809 Acc: 97.77%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 4/8\tloss: 0.06263401\n",
      "val Loss: 0.0884 Acc: 97.08%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 4/8\tloss: 0.07982176\n",
      "val Loss: 0.0781 Acc: 98.43%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 4/8\tloss: 0.00069451\n",
      "val Loss: 0.0806 Acc: 97.60%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 4/8\tloss: 0.00118284\n",
      "val Loss: 0.0820 Acc: 97.38%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 4/8\tloss: 0.00910423\n",
      "val Loss: 0.0822 Acc: 97.01%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 4/8\tloss: 0.94962770\n",
      "val Loss: 0.0820 Acc: 97.16%\n",
      "\n",
      "Epoch 5/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 5/8\tloss: 0.05992007\n",
      "val Loss: 0.4440 Acc: 91.11%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 5/8\tloss: 0.00147374\n",
      "val Loss: 0.1097 Acc: 94.74%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 5/8\tloss: 0.01305196\n",
      "val Loss: 0.0656 Acc: 98.13%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 5/8\tloss: 0.06647165\n",
      "val Loss: 0.0629 Acc: 97.83%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 5/8\tloss: 0.04756987\n",
      "val Loss: 0.0638 Acc: 98.82%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 5/8\tloss: 0.18392093\n",
      "val Loss: 0.0710 Acc: 97.50%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 5/8\tloss: 0.00350635\n",
      "val Loss: 0.0662 Acc: 97.40%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 5/8\tloss: 0.42525694\n",
      "val Loss: 0.0565 Acc: 98.85%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 5/8\tloss: 0.13865149\n",
      "val Loss: 0.1668 Acc: 93.86%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 5/8\tloss: 0.00392196\n",
      "val Loss: 0.0595 Acc: 98.46%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 5/8\tloss: 0.70628011\n",
      "val Loss: 0.0648 Acc: 97.75%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 5/8\tloss: 0.02137095\n",
      "val Loss: 0.0611 Acc: 97.93%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 5/8\tloss: 0.00484241\n",
      "val Loss: 0.0550 Acc: 98.80%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 5/8\tloss: 0.00231536\n",
      "val Loss: 0.0521 Acc: 98.88%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 5/8\tloss: 0.03658926\n",
      "val Loss: 0.0605 Acc: 98.33%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 5/8\tloss: 0.09543581\n",
      "val Loss: 0.0527 Acc: 98.63%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 5/8\tloss: 0.00020240\n",
      "val Loss: 0.0567 Acc: 98.52%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 5/8\tloss: 0.00063425\n",
      "val Loss: 0.0566 Acc: 98.08%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 5/8\tloss: 0.00340443\n",
      "val Loss: 0.0625 Acc: 97.68%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 5/8\tloss: 0.61115479\n",
      "val Loss: 0.0566 Acc: 98.26%\n",
      "\n",
      "Epoch 6/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 6/8\tloss: 0.04732813\n",
      "val Loss: 0.3995 Acc: 91.11%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 6/8\tloss: 0.00087093\n",
      "val Loss: 0.0712 Acc: 97.37%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 6/8\tloss: 0.01245069\n",
      "val Loss: 0.0443 Acc: 99.07%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 6/8\tloss: 0.04043217\n",
      "val Loss: 0.0417 Acc: 100.00%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 6/8\tloss: 0.02817710\n",
      "val Loss: 0.0400 Acc: 99.41%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 6/8\tloss: 0.17164181\n",
      "val Loss: 0.0499 Acc: 98.50%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 6/8\tloss: 0.00110672\n",
      "val Loss: 0.0526 Acc: 98.70%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 6/8\tloss: 0.06934606\n",
      "val Loss: 0.0382 Acc: 99.24%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 6/8\tloss: 0.36545220\n",
      "val Loss: 0.0552 Acc: 98.63%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 6/8\tloss: 0.00060009\n",
      "val Loss: 0.0514 Acc: 98.15%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 6/8\tloss: 0.86069602\n",
      "val Loss: 0.0557 Acc: 98.59%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 6/8\tloss: 0.01381667\n",
      "val Loss: 0.0367 Acc: 99.22%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 6/8\tloss: 0.00232890\n",
      "val Loss: 0.0345 Acc: 99.28%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 6/8\tloss: 0.00144667\n",
      "val Loss: 0.0343 Acc: 99.33%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 6/8\tloss: 0.00898050\n",
      "val Loss: 0.0451 Acc: 99.16%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 6/8\tloss: 0.07802759\n",
      "val Loss: 0.0384 Acc: 98.82%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 6/8\tloss: 0.00003982\n",
      "val Loss: 0.0404 Acc: 98.89%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 6/8\tloss: 0.00015198\n",
      "val Loss: 0.0397 Acc: 98.78%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 6/8\tloss: 0.00201171\n",
      "val Loss: 0.0500 Acc: 98.01%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 6/8\tloss: 0.47801229\n",
      "val Loss: 0.0452 Acc: 98.42%\n",
      "\n",
      "Epoch 7/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 7/8\tloss: 0.02441821\n",
      "val Loss: 0.3997 Acc: 91.11%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 7/8\tloss: 0.00053143\n",
      "val Loss: 0.0562 Acc: 97.37%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 7/8\tloss: 0.00759023\n",
      "val Loss: 0.0297 Acc: 100.00%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 7/8\tloss: 0.02367384\n",
      "val Loss: 0.0276 Acc: 100.00%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 7/8\tloss: 0.01717393\n",
      "val Loss: 0.0276 Acc: 99.41%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 7/8\tloss: 0.15613613\n",
      "val Loss: 0.0391 Acc: 99.00%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 7/8\tloss: 0.00028701\n",
      "val Loss: 0.0421 Acc: 98.27%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 7/8\tloss: 0.05599787\n",
      "val Loss: 0.0281 Acc: 99.62%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 7/8\tloss: 0.36829257\n",
      "val Loss: 0.0480 Acc: 98.63%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 7/8\tloss: 0.00029085\n",
      "val Loss: 0.0442 Acc: 98.46%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 7/8\tloss: 0.48736772\n",
      "val Loss: 0.0308 Acc: 99.15%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 7/8\tloss: 0.01447333\n",
      "val Loss: 0.0358 Acc: 98.45%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 7/8\tloss: 0.00163994\n",
      "val Loss: 0.0241 Acc: 99.76%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 7/8\tloss: 0.00070175\n",
      "val Loss: 0.0254 Acc: 99.33%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 7/8\tloss: 0.00430649\n",
      "val Loss: 0.0376 Acc: 99.16%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 7/8\tloss: 0.04759081\n",
      "val Loss: 0.0270 Acc: 99.41%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 7/8\tloss: 0.00002432\n",
      "val Loss: 0.0300 Acc: 99.08%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No. 18/20\tEpoch 7/8\tloss: 0.00007033\n",
      "val Loss: 0.0301 Acc: 98.95%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 7/8\tloss: 0.00114346\n",
      "val Loss: 0.0376 Acc: 98.51%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 7/8\tloss: 0.21885039\n",
      "val Loss: 0.0297 Acc: 99.37%\n",
      "\n",
      "Epoch 8/8\n",
      "----------\n",
      "Train Index Length: 45\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 1/20\tEpoch 8/8\tloss: 0.00266472\n",
      "val Loss: 0.1738 Acc: 91.11%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 2/20\tEpoch 8/8\tloss: 0.00034508\n",
      "val Loss: 0.0354 Acc: 98.68%\n",
      "\n",
      "Train Index Length: 107\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 3/20\tEpoch 8/8\tloss: 0.00330343\n",
      "val Loss: 0.0196 Acc: 100.00%\n",
      "\n",
      "Train Index Length: 138\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 4/20\tEpoch 8/8\tloss: 0.01642118\n",
      "val Loss: 0.0163 Acc: 100.00%\n",
      "\n",
      "Train Index Length: 169\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 5/20\tEpoch 8/8\tloss: 0.01923461\n",
      "val Loss: 0.0169 Acc: 100.00%\n",
      "\n",
      "Train Index Length: 200\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 6/20\tEpoch 8/8\tloss: 0.14527492\n",
      "val Loss: 0.0239 Acc: 99.50%\n",
      "\n",
      "Train Index Length: 231\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 7/20\tEpoch 8/8\tloss: 0.00014972\n",
      "val Loss: 0.0367 Acc: 98.27%\n",
      "\n",
      "Train Index Length: 262\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 8/20\tEpoch 8/8\tloss: 0.00243925\n",
      "val Loss: 0.0216 Acc: 99.62%\n",
      "\n",
      "Train Index Length: 293\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 9/20\tEpoch 8/8\tloss: 0.20047225\n",
      "val Loss: 0.0272 Acc: 98.98%\n",
      "\n",
      "Train Index Length: 324\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 10/20\tEpoch 8/8\tloss: 0.00014587\n",
      "val Loss: 0.0234 Acc: 99.38%\n",
      "\n",
      "Train Index Length: 355\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 11/20\tEpoch 8/8\tloss: 0.35710916\n",
      "val Loss: 0.0209 Acc: 99.44%\n",
      "\n",
      "Train Index Length: 386\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 12/20\tEpoch 8/8\tloss: 0.01806920\n",
      "val Loss: 0.0314 Acc: 98.45%\n",
      "\n",
      "Train Index Length: 417\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 13/20\tEpoch 8/8\tloss: 0.00165696\n",
      "val Loss: 0.0173 Acc: 99.76%\n",
      "\n",
      "Train Index Length: 448\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 14/20\tEpoch 8/8\tloss: 0.00082115\n",
      "val Loss: 0.0190 Acc: 99.55%\n",
      "\n",
      "Train Index Length: 479\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 15/20\tEpoch 8/8\tloss: 0.00585228\n",
      "val Loss: 0.0279 Acc: 99.37%\n",
      "\n",
      "Train Index Length: 510\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 16/20\tEpoch 8/8\tloss: 0.03377961\n",
      "val Loss: 0.0202 Acc: 99.61%\n",
      "\n",
      "Train Index Length: 541\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 17/20\tEpoch 8/8\tloss: 0.00000942\n",
      "val Loss: 0.0219 Acc: 99.26%\n",
      "\n",
      "Train Index Length: 572\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 18/20\tEpoch 8/8\tloss: 0.00003141\n",
      "val Loss: 0.0260 Acc: 99.30%\n",
      "\n",
      "Train Index Length: 603\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 19/20\tEpoch 8/8\tloss: 0.00079305\n",
      "val Loss: 0.0315 Acc: 98.84%\n",
      "\n",
      "Train Index Length: 634\t\tTest Index Length: 31\n",
      "\n",
      "Fold No. 20/20\tEpoch 8/8\tloss: 0.14254251\n",
      "val Loss: 0.0240 Acc: 99.53%\n",
      "\n",
      "Training complete in 0m 28s\n",
      "Best val Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataset.X, dataset.y, num_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MLP_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t\t torch.Size([50, 54])\n",
      "linear1.bias \t\t torch.Size([50])\n",
      "linear2.weight \t\t torch.Size([12, 50])\n",
      "linear2.bias \t\t torch.Size([12])\n",
      "linear3.weight \t\t torch.Size([3, 12])\n",
      "Optimizer's state_dict:\n",
      "state \t torch.Size([50, 54])\n",
      "state \t tensor([[-1.4927e-02, -2.0142e-02, -1.1800e-02,  ..., -3.2256e-02,\n",
      "         -1.3668e-02, -3.4310e-01],\n",
      "        [-5.5932e-13, -2.1169e-12, -7.5310e-13,  ..., -1.2526e-12,\n",
      "         -1.1990e-12,  2.5722e-12],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 5.6369e-02,  7.5672e-02,  4.5045e-02,  ...,  1.2409e-01,\n",
      "          5.0030e-02,  1.3581e+00],\n",
      "        [ 3.2404e-02,  4.1856e-02,  2.7808e-02,  ...,  7.9942e-02,\n",
      "          2.6980e-02,  8.9899e-01],\n",
      "        [-1.3851e-04, -2.2626e-04, -5.6357e-05,  ..., -1.0258e-04,\n",
      "         -1.8543e-04, -3.2590e-04]])\n",
      "state \t torch.Size([50])\n",
      "state \t tensor([-7.0501e-02, -7.0658e-12,  0.0000e+00,  2.5580e-01, -1.6533e-01,\n",
      "         2.6815e-01,  0.0000e+00,  9.1989e-03, -6.6001e-02,  1.8440e-02,\n",
      "         3.5260e-02, -8.2560e-02, -3.9199e-04, -1.5510e-01,  4.8769e-03,\n",
      "        -1.8930e-04,  2.1713e-04,  2.2473e-01, -9.6694e-03,  5.0656e-04,\n",
      "        -7.2629e-03,  5.9434e-02, -1.7194e-01, -6.1576e-04, -1.4537e-02,\n",
      "         1.5890e-03, -3.9904e-02,  2.8088e-02,  1.5780e-02,  1.8057e-02,\n",
      "         1.2993e-01, -2.5440e-03,  2.2760e-03,  8.4712e-02,  8.5607e-04,\n",
      "        -1.9859e-01, -4.1998e-03,  7.1906e-04,  1.2463e-01,  0.0000e+00,\n",
      "        -7.8940e-04, -7.3145e-02,  0.0000e+00, -1.0326e-01, -7.1321e-03,\n",
      "         0.0000e+00,  2.7355e-03,  2.6807e-01,  1.6617e-01, -6.3575e-04])\n",
      "state \t torch.Size([12, 50])\n",
      "state \t tensor([[-3.3706e-02, -3.0884e-12,  0.0000e+00, -3.7934e-02, -7.5281e-02,\n",
      "         -3.7715e-02,  0.0000e+00, -1.5313e-03, -1.1902e-01, -4.1117e-03,\n",
      "         -9.0365e-03, -4.5082e-02, -5.1004e-04, -4.7420e-02, -2.5357e-04,\n",
      "         -7.4511e-05, -2.4528e-04, -2.7336e-02,  7.2462e-04,  2.7777e-04,\n",
      "          5.7239e-05, -9.8474e-02, -1.0596e-01, -8.4129e-03,  1.8455e-03,\n",
      "          1.2734e-03, -5.9275e-02, -8.4161e-03, -1.4751e-03,  1.0316e-03,\n",
      "         -1.1850e-02,  7.4306e-04, -7.5265e-04, -4.6390e-02, -1.5823e-04,\n",
      "         -7.6757e-02, -2.4522e-03, -1.7994e-03, -1.0353e-02,  0.0000e+00,\n",
      "         -1.0240e-03, -4.6252e-02,  0.0000e+00, -5.0871e-02, -7.8699e-04,\n",
      "          0.0000e+00, -1.1793e-03, -1.6837e-02, -1.2910e-01,  1.1136e-05],\n",
      "        [-1.5149e-03,  8.4848e-14,  0.0000e+00, -1.6339e-03, -3.3279e-03,\n",
      "         -1.6807e-03,  0.0000e+00,  4.3321e-06, -5.2891e-03, -1.9427e-04,\n",
      "         -4.2202e-04, -1.9777e-03, -2.6266e-06, -2.0786e-03,  7.5190e-09,\n",
      "         -3.2393e-06, -1.6755e-08, -1.1885e-03,  3.5518e-05, -3.0809e-07,\n",
      "          2.4587e-06, -4.3957e-03, -4.6317e-03, -3.8143e-04,  8.9397e-05,\n",
      "         -4.2009e-06, -2.6364e-03, -3.6591e-04,  4.8291e-05,  5.8374e-05,\n",
      "         -5.0517e-04,  1.9227e-05,  6.1201e-07, -2.0657e-03, -4.3004e-06,\n",
      "         -3.3204e-03, -1.3173e-07, -3.2945e-08, -3.5945e-04,  0.0000e+00,\n",
      "         -7.1668e-08, -2.0114e-03,  0.0000e+00, -2.2128e-03,  3.8698e-05,\n",
      "          0.0000e+00, -1.2791e-07, -7.7181e-04, -5.7661e-03,  4.6017e-07],\n",
      "        [-1.6994e-01, -4.1381e-12,  0.0000e+00, -1.9125e-01, -3.7922e-01,\n",
      "         -1.9078e-01,  0.0000e+00, -5.8503e-03, -5.9731e-01, -2.0699e-02,\n",
      "         -4.5413e-02, -2.2839e-01, -1.8667e-03, -2.3908e-01, -2.4185e-03,\n",
      "         -3.7579e-04, -1.3694e-03, -1.3787e-01,  3.9086e-03,  1.4257e-03,\n",
      "          2.8899e-04, -4.9657e-01, -5.2983e-01, -4.2392e-02,  1.0694e-02,\n",
      "          6.5417e-03, -2.9737e-01, -4.2446e-02, -2.1346e-03,  8.8534e-03,\n",
      "         -5.9554e-02,  2.7264e-03, -2.8800e-03, -2.3453e-01, -7.6018e-04,\n",
      "         -3.8499e-01, -7.9109e-03, -5.7943e-03, -5.0031e-02,  0.0000e+00,\n",
      "         -3.4782e-03, -2.3230e-01,  0.0000e+00, -2.5678e-01, -1.3991e-03,\n",
      "          0.0000e+00, -4.7536e-03, -8.4682e-02, -6.4889e-01,  5.4219e-05],\n",
      "        [ 3.2314e-02, -2.9524e-12,  0.0000e+00,  2.0507e-02,  6.7055e-02,\n",
      "          4.2996e-02,  0.0000e+00,  5.5974e-04,  1.1006e-01, -8.0459e-05,\n",
      "         -2.3426e-03,  3.4761e-02,  3.0321e-04,  3.2370e-02,  5.3636e-04,\n",
      "          1.1338e-04,  3.7950e-04,  3.0334e-02, -1.2701e-03,  5.1450e-13,\n",
      "         -1.0079e-04,  8.9626e-02,  1.1336e-01,  8.5909e-05, -2.5093e-03,\n",
      "          1.4716e-04,  4.7560e-02,  9.3172e-05, -1.3634e-03, -1.5924e-03,\n",
      "          1.4240e-02, -4.2728e-04,  2.1067e-04,  5.6961e-02,  2.1570e-04,\n",
      "          7.4249e-02,  5.1767e-04,  3.0721e-04,  1.3785e-02,  0.0000e+00,\n",
      "          2.4919e-04,  3.5604e-02,  0.0000e+00,  3.9757e-02, -9.4007e-04,\n",
      "          0.0000e+00,  6.6920e-04,  1.0432e-02,  1.2871e-01, -1.5754e-05],\n",
      "        [-7.3093e-03, -1.6263e-12,  0.0000e+00, -8.2444e-03, -1.6399e-02,\n",
      "         -8.1168e-03,  0.0000e+00, -5.5004e-04, -2.6189e-02, -8.9385e-04,\n",
      "         -1.9773e-03, -9.6605e-03, -2.0393e-04, -1.0310e-02,  1.2069e-04,\n",
      "         -1.6199e-05, -3.2264e-05, -5.9474e-03,  1.2373e-04,  6.0379e-05,\n",
      "          1.0993e-05, -2.1381e-02, -2.3541e-02, -1.8293e-03,  2.4767e-04,\n",
      "          2.7683e-04, -1.3046e-02, -1.8310e-03, -1.0028e-03, -2.3224e-04,\n",
      "         -2.6108e-03,  3.1419e-04, -2.6816e-04, -9.9909e-03, -3.9259e-05,\n",
      "         -1.6915e-02, -1.0822e-03, -7.9493e-04, -2.5119e-03,  0.0000e+00,\n",
      "         -4.1990e-04, -1.0167e-02,  0.0000e+00, -1.1022e-02, -4.9567e-04,\n",
      "          0.0000e+00, -3.9803e-04, -3.6784e-03, -2.8283e-02,  2.7263e-06],\n",
      "        [-1.6240e-02,  5.6258e-26,  0.0000e+00, -1.8209e-02, -3.6127e-02,\n",
      "         -1.8326e-02,  0.0000e+00,  3.1564e-05, -5.6552e-02, -1.9918e-03,\n",
      "         -4.3172e-03, -2.1951e-02, -2.9046e-05, -2.2807e-02,  6.3174e-14,\n",
      "         -3.5634e-05,  2.8801e-10, -1.3074e-02,  3.9783e-04,  1.3330e-04,\n",
      "          2.7070e-05, -4.7448e-02, -4.9807e-02, -4.0233e-03,  1.2120e-03,\n",
      "          6.1013e-04, -2.8157e-02, -4.0250e-03,  5.3354e-04,  1.1954e-03,\n",
      "         -5.5452e-03,  8.2552e-05,  3.7611e-09, -2.2429e-02, -4.7297e-05,\n",
      "         -3.6267e-02, -1.5512e-09,  0.0000e+00, -4.0052e-03,  0.0000e+00,\n",
      "         -4.4516e-13, -2.1994e-02,  0.0000e+00, -2.4502e-02,  3.0764e-04,\n",
      "          0.0000e+00,  1.2724e-10, -8.1091e-03, -6.1736e-02,  7.1608e-08],\n",
      "        [ 1.1272e-04, -6.2099e-13,  0.0000e+00,  4.9436e-04,  3.6829e-04,\n",
      "          4.8038e-04,  0.0000e+00,  1.5662e-03,  1.2988e-03, -5.9059e-09,\n",
      "          3.1390e-04, -4.3870e-05,  3.3707e-04,  8.8511e-05,  8.7708e-04,\n",
      "          0.0000e+00,  2.7848e-04,  5.9739e-06,  8.5277e-05, -6.1212e-18,\n",
      "          5.8127e-05,  2.3199e-04,  2.5003e-03,  4.4412e-07,  1.0993e-03,\n",
      "          5.0874e-24,  8.0944e-04,  1.5918e-06,  1.9080e-03,  1.2587e-03,\n",
      "          2.0433e-04, -3.8530e-05,  8.1468e-04,  2.1432e-04,  3.7735e-05,\n",
      "          1.3614e-03,  1.7520e-03,  1.4759e-03,  1.8811e-03,  0.0000e+00,\n",
      "          7.1504e-04,  6.6225e-04,  0.0000e+00,  3.8115e-04,  1.6295e-03,\n",
      "          0.0000e+00,  1.2013e-03,  4.6725e-04,  1.0966e-03, -5.8520e-07],\n",
      "        [-2.7507e-05,  8.8192e-12,  0.0000e+00,  9.5338e-04,  7.4957e-04,\n",
      "          1.6521e-03,  0.0000e+00,  3.0267e-03,  2.0672e-03, -2.5844e-08,\n",
      "          9.0079e-04,  2.3497e-04,  6.3184e-04,  3.1026e-04,  1.6338e-03,\n",
      "          5.9239e-09,  1.3726e-04,  2.8992e-05,  2.6288e-04, -3.1448e-17,\n",
      "          3.0555e-04,  3.0190e-04,  4.7380e-03,  2.3217e-06,  2.7014e-03,\n",
      "         -5.5598e-16,  1.7342e-03,  8.3215e-06,  5.1508e-03,  3.4697e-03,\n",
      "          1.0230e-04, -2.6211e-04,  1.8497e-03,  4.6877e-05,  7.1770e-06,\n",
      "          2.1992e-03,  3.8823e-03,  3.7049e-03,  2.9545e-03,  0.0000e+00,\n",
      "          1.3539e-03,  1.2335e-03,  0.0000e+00,  9.6923e-04,  4.0849e-03,\n",
      "          0.0000e+00,  2.2972e-03,  1.1320e-03,  1.8294e-03, -2.6781e-06],\n",
      "        [ 6.9893e-03, -2.3318e-12,  0.0000e+00, -1.1215e-03,  1.4775e-02,\n",
      "          1.2900e-02,  0.0000e+00, -3.1921e-03,  3.1819e-02, -4.7746e-07,\n",
      "          8.0039e-03,  2.3548e-03, -8.1886e-04,  1.8809e-04, -1.8935e-03,\n",
      "         -2.8230e-08, -1.3560e-03,  1.4687e-03,  4.5496e-03, -5.2490e-06,\n",
      "          5.3607e-04,  1.6656e-02,  3.4026e-02, -1.2030e-06,  7.2726e-03,\n",
      "         -1.2862e-05,  1.4972e-02,  5.7897e-06,  6.2266e-03,  5.8562e-03,\n",
      "          4.3186e-03, -2.4915e-04, -1.2596e-03,  1.2643e-02, -2.4070e-04,\n",
      "          1.7102e-02, -3.3855e-03, -1.8756e-03,  1.0996e-02,  0.0000e+00,\n",
      "         -1.8798e-03,  1.0064e-02,  0.0000e+00,  4.4172e-03,  1.9177e-03,\n",
      "          0.0000e+00, -2.7101e-03,  1.0907e-02,  3.5295e-02, -2.6361e-06],\n",
      "        [ 3.6168e-01, -2.0460e-11,  0.0000e+00,  4.0796e-01,  8.0878e-01,\n",
      "          4.0514e-01,  0.0000e+00,  4.4621e-05,  1.2691e+00,  4.4335e-02,\n",
      "          9.6417e-02,  4.8920e-01,  2.1512e-03,  5.1096e-01,  6.7323e-04,\n",
      "          8.0335e-04,  2.3711e-03,  2.9471e-01, -9.4932e-03, -3.0055e-03,\n",
      "         -8.5504e-04,  1.0601e+00,  1.1141e+00,  9.0704e-02, -3.0788e-02,\n",
      "         -1.3757e-02,  6.3035e-01,  9.0738e-02, -1.0297e-02, -2.7948e-02,\n",
      "          1.2679e-01, -7.0045e-03, -7.9751e-04,  4.9987e-01,  1.5333e-03,\n",
      "          8.1222e-01,  2.2382e-04,  1.6504e-04,  9.3708e-02,  0.0000e+00,\n",
      "         -8.1755e-04,  4.9362e-01,  0.0000e+00,  5.4775e-01, -9.3585e-03,\n",
      "          0.0000e+00,  2.9950e-03,  1.8020e-01,  1.3817e+00, -1.1299e-04],\n",
      "        [ 1.1430e-01, -1.1860e-11,  0.0000e+00,  1.2731e-01,  2.5400e-01,\n",
      "          1.2661e-01,  0.0000e+00, -9.3738e-04,  3.9816e-01,  1.3944e-02,\n",
      "          2.9498e-02,  1.5374e-01,  1.7086e-04,  1.6045e-01, -4.4696e-04,\n",
      "          2.5267e-04,  5.6311e-04,  9.2671e-02, -3.0262e-03, -9.4208e-04,\n",
      "         -5.2497e-04,  3.3369e-01,  3.4860e-01,  2.8528e-02, -1.1048e-02,\n",
      "         -4.3186e-03,  1.9738e-01,  2.8532e-02, -6.5907e-03, -1.1187e-02,\n",
      "          3.9688e-02, -1.2763e-03, -9.0000e-04,  1.5775e-01,  4.5378e-04,\n",
      "          2.5502e-01, -1.3216e-03, -1.9261e-03,  2.8412e-02,  0.0000e+00,\n",
      "         -1.9545e-04,  1.5431e-01,  0.0000e+00,  1.7155e-01, -5.0920e-03,\n",
      "          0.0000e+00, -4.4853e-04,  5.5578e-02,  4.1815e-01, -3.2539e-05],\n",
      "        [ 2.8615e-04,  1.3979e-12,  0.0000e+00,  6.8882e-04,  3.9817e-04,\n",
      "          5.6977e-04,  0.0000e+00,  1.6508e-03,  1.2425e-03,  2.4164e-16,\n",
      "          2.8441e-04,  1.4526e-04,  2.6869e-04,  5.7971e-05,  1.5742e-03,\n",
      "          1.8792e-15,  6.0540e-04,  3.0250e-07, -2.2227e-06,  0.0000e+00,\n",
      "         -2.2144e-07,  4.8568e-04,  2.3048e-03,  2.4364e-08,  8.5360e-04,\n",
      "          2.3766e-11,  6.4752e-04,  4.0579e-13,  4.9350e-04,  2.6544e-04,\n",
      "          4.1355e-04,  3.7046e-04,  7.4928e-04,  6.6056e-04,  8.3452e-05,\n",
      "          1.5275e-03,  1.0271e-03,  7.3557e-04,  2.2788e-03,  0.0000e+00,\n",
      "          6.1941e-04,  6.9412e-04,  0.0000e+00,  5.0363e-04,  1.1070e-03,\n",
      "          0.0000e+00,  1.3179e-03,  4.9206e-04,  1.1321e-03, -5.3380e-10]])\n",
      "state \t torch.Size([12])\n",
      "state \t tensor([-0.0295, -0.0013, -0.1470,  0.0324, -0.0066, -0.0138,  0.0013,  0.0032,\n",
      "         0.0100,  0.3055,  0.0944,  0.0011])\n",
      "state \t torch.Size([3, 12])\n",
      "state \t tensor([[-4.0617e-01, -6.1064e-02, -1.0590e+00, -1.7940e-02, -2.3523e-01,\n",
      "          5.4573e-04,  1.9831e-05, -5.3053e-03, -7.7035e-03, -1.6504e-01,\n",
      "         -1.0143e+00, -2.7862e-05],\n",
      "        [ 2.0692e-03,  5.2466e-09,  4.8071e-03, -1.9897e-04,  7.0914e-04,\n",
      "          1.0943e-13,  4.1405e-04,  4.0139e-03,  6.0767e-03,  2.2906e-03,\n",
      "          2.2321e-03,  1.2038e-06],\n",
      "        [ 4.0410e-01,  6.1064e-02,  1.0542e+00,  1.8139e-02,  2.3452e-01,\n",
      "         -5.4573e-04, -4.3388e-04,  1.2915e-03,  1.6268e-03,  1.6006e-01,\n",
      "          1.0120e+00,  2.6662e-05]])\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(D_in, 50, D_out)\n",
    "mlp_model.load('MLP_Model')\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in mlp_model.state_dict():\n",
    "    print(param_tensor, \"\\t\\t\", mlp_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    if var_name == 'state':\n",
    "        for i in range(len(mlp_model.state_dict())):\n",
    "            print(var_name, \"\\t\", optimizer.state_dict()[var_name][i]['momentum_buffer'].shape)\n",
    "            print(var_name, \"\\t\", optimizer.state_dict()[var_name][i]['momentum_buffer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8795180722891566, 2: 0.060240963855421686, 1: 0.060240963855421686}\n",
      "{1: 0.9879518072289156, 0: 0.012048192771084338}\n",
      "{2: 0.9625, 0: 0.0375}\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(D_in, 50, D_out)\n",
    "mlp_model.load('MLP_Model')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(D_out):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical = df.copy()\n",
    "df_hierarchical['tag'] = df_hierarchical['tag'].apply(lambda x: 0 if x <= 7 else 1)\n",
    "\n",
    "msk = np.random.rand(len(df_hierarchical)) < 0.8\n",
    "df_train = df_hierarchical[msk]\n",
    "df_test = df_hierarchical[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical = MLP(D_in, 50, 2)\n",
    "optimizer = torch.optim.SGD(model_hierarchical.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical, criterion, optimizer, dataset.X, dataset.y, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 2)\n",
    "mlp_model.load('MLP_Model_Hierarchical_1')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(2):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical_2 = df.copy()\n",
    "df_hierarchical_2 = df_hierarchical_2[df_hierarchical_2['tag'] >= 8]\n",
    "df_hierarchical_2 = df_hierarchical_2.apply(lambda x: x-8)\n",
    "msk = np.random.rand(len(df_hierarchical_2)) < 0.8\n",
    "df_train = df_hierarchical_2[msk]\n",
    "df_test = df_hierarchical_2[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)\n",
    "\n",
    "model_hierarchical_2 = MLP(D_in, 50, 2)\n",
    "# optimizer = torch.optim.Adam(model_hierarchical_2.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(model_hierarchical_2.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical_2, criterion, optimizer, dataset.X, dataset.y, num_epochs=20)\n",
    "\n",
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 2)\n",
    "mlp_model.load('MLP_Model_Hierarchical_2')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(2):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x) + 8\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical_3 = df.copy()\n",
    "df_hierarchical_3 = df_hierarchical_3[df_hierarchical_3['tag'] < 8]\n",
    "msk = np.random.rand(len(df_hierarchical_3)) < 0.8\n",
    "df_train = df_hierarchical_3[msk]\n",
    "df_test = df_hierarchical_3[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)\n",
    "\n",
    "model_hierarchical_3 = MLP(D_in, 50, 8)\n",
    "# optimizer = torch.optim.Adam(model_hierarchical_2.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(model_hierarchical_3.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical_3, criterion, optimizer, dataset.X, dataset.y, num_epochs=30)\n",
    "\n",
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 8)\n",
    "mlp_model.load('MLP_Model_Hierarchical_3')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(8):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
