{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit, cross_val_score\n",
    "\n",
    "import brevitas.nn as nn\n",
    "\n",
    "from config import *\n",
    "from classic_models import *\n",
    "from data_preprocessing import *\n",
    "from feature_extraction import *\n",
    "from helpers import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_accel1</th>\n",
       "      <th>mean_accel2</th>\n",
       "      <th>mean_accel3</th>\n",
       "      <th>mean_gyro1</th>\n",
       "      <th>mean_gyro2</th>\n",
       "      <th>mean_gyro3</th>\n",
       "      <th>max_accel1</th>\n",
       "      <th>max_accel2</th>\n",
       "      <th>max_accel3</th>\n",
       "      <th>max_gyro1</th>\n",
       "      <th>...</th>\n",
       "      <th>var_coeff_gyro1</th>\n",
       "      <th>var_coeff_gyro2</th>\n",
       "      <th>var_coeff_gyro3</th>\n",
       "      <th>kurtosis_accel1</th>\n",
       "      <th>kurtosis_accel2</th>\n",
       "      <th>kurtosis_accel3</th>\n",
       "      <th>kurtosis_gyro1</th>\n",
       "      <th>kurtosis_gyro2</th>\n",
       "      <th>kurtosis_gyro3</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321713</td>\n",
       "      <td>0.587304</td>\n",
       "      <td>0.19189</td>\n",
       "      <td>0.224644</td>\n",
       "      <td>0.132784</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.412742</td>\n",
       "      <td>-1.510622</td>\n",
       "      <td>0.425019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512475</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.465719</td>\n",
       "      <td>0.091719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.178974</td>\n",
       "      <td>-1.346002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.290898</td>\n",
       "      <td>0.515375</td>\n",
       "      <td>0.18489</td>\n",
       "      <td>0.219044</td>\n",
       "      <td>0.117643</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.404411</td>\n",
       "      <td>-1.121981</td>\n",
       "      <td>0.454103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506555</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.089168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.176028</td>\n",
       "      <td>-1.119453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261652</td>\n",
       "      <td>0.499783</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.215231</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.378192</td>\n",
       "      <td>0.209323</td>\n",
       "      <td>0.482450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510602</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.403010</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.086363</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>-1.079949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.236798</td>\n",
       "      <td>0.463143</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.305106</td>\n",
       "      <td>3.957013</td>\n",
       "      <td>0.498670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507860</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.341137</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.089625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.176476</td>\n",
       "      <td>-0.980839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221092</td>\n",
       "      <td>0.355008</td>\n",
       "      <td>0.16476</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.035674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.161354</td>\n",
       "      <td>4.649814</td>\n",
       "      <td>0.501705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472475</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.177258</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_accel1  mean_accel2  mean_accel3  mean_gyro1  mean_gyro2  mean_gyro3  \\\n",
       "0     0.321713     0.587304      0.19189    0.224644    0.132784         3.0   \n",
       "1     0.290898     0.515375      0.18489    0.219044    0.117643         3.0   \n",
       "2     0.261652     0.499783      0.16476    0.215231    0.098955         3.0   \n",
       "3     0.236798     0.463143      0.16476    0.212382    0.072249         3.0   \n",
       "4     0.221092     0.355008      0.16476    0.212382    0.035674         3.0   \n",
       "\n",
       "   max_accel1  max_accel2  max_accel3  max_gyro1  ...  var_coeff_gyro1  \\\n",
       "0        0.08    0.412742   -1.510622   0.425019  ...         0.512475   \n",
       "1        0.12    0.404411   -1.121981   0.454103  ...         0.506555   \n",
       "2        0.12    0.378192    0.209323   0.482450  ...         0.510602   \n",
       "3        0.12    0.305106    3.957013   0.498670  ...         0.507860   \n",
       "4        0.12    0.161354    4.649814   0.501705  ...         0.472475   \n",
       "\n",
       "   var_coeff_gyro2  var_coeff_gyro3  kurtosis_accel1  kurtosis_accel2  \\\n",
       "0         0.670569         0.403010         0.465719         0.091719   \n",
       "1         0.670569         0.403010         0.467391         0.089168   \n",
       "2         0.670569         0.403010         0.469900         0.086363   \n",
       "3         0.670569         0.341137         0.469900         0.089625   \n",
       "4         0.670569         0.177258         0.461538         0.115898   \n",
       "\n",
       "   kurtosis_accel3  kurtosis_gyro1  kurtosis_gyro2  kurtosis_gyro3  tag  \n",
       "0              1.0            0.46        0.178974       -1.346002  0.0  \n",
       "1              1.0            0.42        0.176028       -1.119453  0.0  \n",
       "2              2.0            0.34        0.169140       -1.079949  0.0  \n",
       "3              2.0            0.38        0.176476       -0.980839  0.0  \n",
       "4              2.0            0.34        0.245300        0.466311  0.0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "df = pd.read_csv('out_2.csv')\n",
    "# temp = df['tag']\n",
    "# del df['tag']\n",
    "\n",
    "# x = df.values #returns a numpy array\n",
    "# col = df.columns\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# df = pd.DataFrame(x_scaled, columns=col)\n",
    "\n",
    "# df['tag'] = temp\n",
    "df['tag'] = df['tag'].apply(lambda x: x-1)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "\n",
    "df_train.to_csv('df_train.csv')\n",
    "df_test.to_csv('df_test.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df_np = df.to_numpy()\n",
    "\n",
    "        self.X = df_np[:,:-1]\n",
    "        self.y = df_np[:,-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get item by index\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns length of data\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureDataset(df_train)\n",
    "D_in = df.shape[1]-1\n",
    "D_out = len(dances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.linear1 = nn.QuantLinear(d_in, d_hidden, bias=True)#, weight_bit_width=4)\n",
    "        self.linear2 = nn.QuantLinear(d_hidden, d_hidden, bias=True)#, weight_bit_width=4)\n",
    "        self.linear3 = nn.QuantLinear(d_hidden, d_out, bias=False)#, weight_bit_width=4)\n",
    "        \n",
    "        self.relu = torch.nn.ReL(p=0.1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.linear1(X.float())\n",
    "        X = self.linear2(X)\n",
    "        X = self.linear3(X)\n",
    "        return torch.nn.functional.sigmoid(X)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.linear1 = nn.QuantLinear(d_in, d_hidden, bias=True)\n",
    "        self.linear2 = nn.QuantLinear(d_hidden, d_hidden//4, bias=True)\n",
    "        self.linear3 = nn.QuantLinear(d_hidden//4, d_out, bias=False)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.relu(self.linear1(X.float()))\n",
    "        # X = self.dropout(X)\n",
    "        X = self.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        # X = self.dropout(X)\n",
    "        return torch.nn.functional.log_softmax(X, dim=1)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self(X.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(model, criterion, optimizer, X, y, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Store losses and accuracies accross epochs\n",
    "    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    for i in range(1,num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(i, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "        \n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X_train, y_train)):\n",
    "            ### Dividing data into folds\n",
    "            x_train_fold = X_train[train_index]\n",
    "            x_test_fold = X_train[test_index]\n",
    "            y_train_fold = y_train[train_index]\n",
    "            y_test_fold = y_train[test_index]\n",
    "\n",
    "            print('Train Index Length:', len(train_index), end='\\t\\t')\n",
    "            print('Test Index Length:', len(test_index), end='\\n\\n')\n",
    "\n",
    "            train = torch.utils.data.TensorDataset(torch.tensor(x_train_fold), torch.tensor(y_train_fold))\n",
    "            test = torch.utils.data.TensorDataset(torch.tensor(x_test_fold), torch.tensor(y_test_fold))\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = False)\n",
    "            test_loader = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for batch_index, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch)\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "                # print(y_pred.shape, y_batch.view(-1, 1).shape)\n",
    "                single_loss = criterion(y_pred, y_batch.long())\n",
    "                single_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += single_loss.item() * x_batch.size(0)\n",
    "                running_corrects += torch.sum(preds == y_batch.data)\n",
    "            print('Fold No. {}/{}\\tEpoch {}/{}\\t'.format(fold + 1 , tscv.get_n_splits(), i, num_epochs), end='')\n",
    "            print(f'loss: {single_loss.item():10.8f}')\n",
    "            \n",
    "            nsamples = len(train_index)\n",
    "            epoch_loss = running_loss / nsamples\n",
    "            epoch_acc = running_corrects.double() / nsamples\n",
    "\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n",
    "                    phase, epoch_loss, 100 * epoch_acc)\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 54])\n",
      "torch.Size([50])\n",
      "torch.Size([12, 50])\n",
      "torch.Size([12])\n",
      "torch.Size([3, 12])\n",
      "Total number of parameters = 3398\n"
     ]
    }
   ],
   "source": [
    "model = MLP(D_in, 50, D_out)\n",
    "# model = MultiHead4MLP(D_in, D_out)\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 1/8\tloss: 1.09689128\n",
      "val Loss: 1.1284 Acc: 23.91%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 1/8\tloss: 1.06161225\n",
      "val Loss: 1.0998 Acc: 31.58%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 1/8\tloss: 1.05936587\n",
      "val Loss: 1.0879 Acc: 35.85%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 1/8\tloss: 1.08360898\n",
      "val Loss: 1.0681 Acc: 50.74%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 1/8\tloss: 1.00826502\n",
      "val Loss: 1.0498 Acc: 64.46%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 1/8\tloss: 1.05543435\n",
      "val Loss: 1.0226 Acc: 66.84%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 1/8\tloss: 1.04912412\n",
      "val Loss: 0.9808 Acc: 69.47%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 1/8\tloss: 1.14934146\n",
      "val Loss: 0.9239 Acc: 69.14%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 1/8\tloss: 0.73877543\n",
      "val Loss: 0.8324 Acc: 70.63%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 1/8\tloss: 0.69731754\n",
      "val Loss: 0.7204 Acc: 75.32%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 1/8\tloss: 0.84854144\n",
      "val Loss: 0.6193 Acc: 79.19%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 1/8\tloss: 0.59988743\n",
      "val Loss: 0.5613 Acc: 82.71%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 1/8\tloss: 0.37666550\n",
      "val Loss: 0.4990 Acc: 83.74%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 1/8\tloss: 0.13215996\n",
      "val Loss: 0.4551 Acc: 84.63%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 1/8\tloss: 0.82127839\n",
      "val Loss: 0.4311 Acc: 84.98%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 1/8\tloss: 0.27086034\n",
      "val Loss: 0.3962 Acc: 86.09%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 1/8\tloss: 0.47005820\n",
      "val Loss: 0.3741 Acc: 86.69%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 1/8\tloss: 0.30940735\n",
      "val Loss: 0.3481 Acc: 87.77%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 1/8\tloss: 0.08680052\n",
      "val Loss: 0.3247 Acc: 88.05%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 1/8\tloss: 0.20725586\n",
      "val Loss: 0.3051 Acc: 88.80%\n",
      "\n",
      "Epoch 2/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 2/8\tloss: 0.15359393\n",
      "val Loss: 0.2449 Acc: 91.30%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 2/8\tloss: 0.09876021\n",
      "val Loss: 0.2144 Acc: 93.42%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 2/8\tloss: 0.29858455\n",
      "val Loss: 0.2104 Acc: 92.45%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 2/8\tloss: 0.03350665\n",
      "val Loss: 0.2016 Acc: 92.65%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 2/8\tloss: 0.05603452\n",
      "val Loss: 0.1905 Acc: 92.77%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 2/8\tloss: 0.07033830\n",
      "val Loss: 0.2159 Acc: 91.84%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 2/8\tloss: 0.19292422\n",
      "val Loss: 0.2150 Acc: 91.15%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 2/8\tloss: 0.23141004\n",
      "val Loss: 0.2266 Acc: 91.02%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 2/8\tloss: 0.43949714\n",
      "val Loss: 0.2613 Acc: 88.81%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 2/8\tloss: 0.23190255\n",
      "val Loss: 0.2370 Acc: 89.87%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 2/8\tloss: 0.45199201\n",
      "val Loss: 0.2498 Acc: 89.02%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 2/8\tloss: 0.17075594\n",
      "val Loss: 0.2559 Acc: 90.43%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 2/8\tloss: 0.10702018\n",
      "val Loss: 0.2390 Acc: 90.89%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 2/8\tloss: 0.01682213\n",
      "val Loss: 0.2302 Acc: 90.83%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 2/8\tloss: 0.69122118\n",
      "val Loss: 0.2311 Acc: 91.20%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 2/8\tloss: 0.25714195\n",
      "val Loss: 0.2190 Acc: 91.13%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 2/8\tloss: 0.41951028\n",
      "val Loss: 0.2158 Acc: 91.25%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 2/8\tloss: 0.18807010\n",
      "val Loss: 0.2056 Acc: 91.91%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 2/8\tloss: 0.04194556\n",
      "val Loss: 0.1993 Acc: 92.83%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 2/8\tloss: 0.12710498\n",
      "val Loss: 0.1963 Acc: 92.86%\n",
      "\n",
      "Epoch 3/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 3/8\tloss: 0.09045782\n",
      "val Loss: 0.1703 Acc: 95.65%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 3/8\tloss: 0.06112029\n",
      "val Loss: 0.1397 Acc: 96.05%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 3/8\tloss: 0.26772991\n",
      "val Loss: 0.1335 Acc: 95.28%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 3/8\tloss: 0.00988069\n",
      "val Loss: 0.1278 Acc: 93.38%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 3/8\tloss: 0.03780720\n",
      "val Loss: 0.1257 Acc: 93.98%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 3/8\tloss: 0.03570443\n",
      "val Loss: 0.1422 Acc: 92.86%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 3/8\tloss: 0.05222104\n",
      "val Loss: 0.1325 Acc: 93.36%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 3/8\tloss: 0.13767661\n",
      "val Loss: 0.1424 Acc: 93.75%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 3/8\tloss: 0.23145594\n",
      "val Loss: 0.1771 Acc: 91.61%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 3/8\tloss: 0.17965049\n",
      "val Loss: 0.1582 Acc: 93.35%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 3/8\tloss: 0.23139201\n",
      "val Loss: 0.1670 Acc: 92.20%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 3/8\tloss: 0.08964595\n",
      "val Loss: 0.1582 Acc: 93.35%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 3/8\tloss: 0.03139329\n",
      "val Loss: 0.1467 Acc: 93.84%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 3/8\tloss: 0.00750219\n",
      "val Loss: 0.1489 Acc: 94.50%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 3/8\tloss: 0.59680015\n",
      "val Loss: 0.1556 Acc: 94.42%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 3/8\tloss: 0.16323556\n",
      "val Loss: 0.1557 Acc: 93.55%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 3/8\tloss: 0.33274779\n",
      "val Loss: 0.1511 Acc: 94.68%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 3/8\tloss: 0.07409514\n",
      "val Loss: 0.1420 Acc: 94.60%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 3/8\tloss: 0.02371472\n",
      "val Loss: 0.1431 Acc: 94.71%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 3/8\tloss: 0.17507116\n",
      "val Loss: 0.1504 Acc: 94.81%\n",
      "\n",
      "Epoch 4/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 4/8\tloss: 0.06377826\n",
      "val Loss: 0.1318 Acc: 95.65%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 4/8\tloss: 0.04825707\n",
      "val Loss: 0.1008 Acc: 97.37%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 4/8\tloss: 0.23671167\n",
      "val Loss: 0.0924 Acc: 96.23%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 4/8\tloss: 0.00431757\n",
      "val Loss: 0.0877 Acc: 96.32%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 4/8\tloss: 0.02300452\n",
      "val Loss: 0.0905 Acc: 95.18%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 4/8\tloss: 0.01845827\n",
      "val Loss: 0.1007 Acc: 95.92%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 4/8\tloss: 0.01905044\n",
      "val Loss: 0.0870 Acc: 95.58%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 4/8\tloss: 0.06355920\n",
      "val Loss: 0.0979 Acc: 96.09%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No. 9/20\tEpoch 4/8\tloss: 0.12059321\n",
      "val Loss: 0.1275 Acc: 94.76%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 4/8\tloss: 0.14036196\n",
      "val Loss: 0.1122 Acc: 95.89%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 4/8\tloss: 0.13380247\n",
      "val Loss: 0.1176 Acc: 95.38%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 4/8\tloss: 0.06392702\n",
      "val Loss: 0.1099 Acc: 94.95%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 4/8\tloss: 0.00953544\n",
      "val Loss: 0.0997 Acc: 96.31%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 4/8\tloss: 0.00427880\n",
      "val Loss: 0.1070 Acc: 96.10%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 4/8\tloss: 0.46337616\n",
      "val Loss: 0.1093 Acc: 95.92%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 4/8\tloss: 0.05412479\n",
      "val Loss: 0.1170 Acc: 95.77%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 4/8\tloss: 0.21415812\n",
      "val Loss: 0.1127 Acc: 96.01%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 4/8\tloss: 0.06217757\n",
      "val Loss: 0.1055 Acc: 96.76%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 4/8\tloss: 0.01710746\n",
      "val Loss: 0.1100 Acc: 96.42%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 4/8\tloss: 0.29943594\n",
      "val Loss: 0.1184 Acc: 96.27%\n",
      "\n",
      "Epoch 5/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 5/8\tloss: 0.05091155\n",
      "val Loss: 0.0964 Acc: 97.83%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 5/8\tloss: 0.03321637\n",
      "val Loss: 0.0727 Acc: 98.68%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 5/8\tloss: 0.20356196\n",
      "val Loss: 0.0689 Acc: 97.17%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 5/8\tloss: 0.00200362\n",
      "val Loss: 0.0693 Acc: 97.06%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 5/8\tloss: 0.01550834\n",
      "val Loss: 0.0707 Acc: 96.39%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 5/8\tloss: 0.01393265\n",
      "val Loss: 0.0744 Acc: 97.45%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 5/8\tloss: 0.00766193\n",
      "val Loss: 0.0608 Acc: 98.23%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 5/8\tloss: 0.04532269\n",
      "val Loss: 0.0699 Acc: 98.05%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 5/8\tloss: 0.03199599\n",
      "val Loss: 0.0910 Acc: 97.55%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 5/8\tloss: 0.06847503\n",
      "val Loss: 0.0800 Acc: 97.78%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 5/8\tloss: 0.10254858\n",
      "val Loss: 0.0824 Acc: 97.40%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 5/8\tloss: 0.04527741\n",
      "val Loss: 0.0788 Acc: 97.87%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 5/8\tloss: 0.00373981\n",
      "val Loss: 0.0742 Acc: 97.29%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 5/8\tloss: 0.00219282\n",
      "val Loss: 0.0837 Acc: 96.79%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 5/8\tloss: 0.39019665\n",
      "val Loss: 0.0818 Acc: 97.21%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 5/8\tloss: 0.01142111\n",
      "val Loss: 0.0959 Acc: 97.18%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 5/8\tloss: 0.13400568\n",
      "val Loss: 0.0932 Acc: 96.77%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 5/8\tloss: 0.07448862\n",
      "val Loss: 0.0877 Acc: 97.48%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 5/8\tloss: 0.01696989\n",
      "val Loss: 0.0949 Acc: 96.42%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 5/8\tloss: 0.47110060\n",
      "val Loss: 0.1018 Acc: 96.59%\n",
      "\n",
      "Epoch 6/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 6/8\tloss: 0.04038854\n",
      "val Loss: 0.0914 Acc: 97.83%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 6/8\tloss: 0.02682132\n",
      "val Loss: 0.0590 Acc: 98.68%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 6/8\tloss: 0.17818765\n",
      "val Loss: 0.0562 Acc: 97.17%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 6/8\tloss: 0.00099432\n",
      "val Loss: 0.0551 Acc: 97.06%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 6/8\tloss: 0.00718154\n",
      "val Loss: 0.0545 Acc: 98.19%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 6/8\tloss: 0.00973048\n",
      "val Loss: 0.0564 Acc: 98.47%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 6/8\tloss: 0.00335292\n",
      "val Loss: 0.0458 Acc: 98.23%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 6/8\tloss: 0.04224098\n",
      "val Loss: 0.0555 Acc: 98.05%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 6/8\tloss: 0.01727298\n",
      "val Loss: 0.0695 Acc: 97.90%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 6/8\tloss: 0.01976547\n",
      "val Loss: 0.0641 Acc: 98.73%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 6/8\tloss: 0.11548158\n",
      "val Loss: 0.0631 Acc: 98.55%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 6/8\tloss: 0.02621885\n",
      "val Loss: 0.0593 Acc: 98.14%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 6/8\tloss: 0.00267108\n",
      "val Loss: 0.0554 Acc: 98.28%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 6/8\tloss: 0.00082261\n",
      "val Loss: 0.0640 Acc: 97.48%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 6/8\tloss: 0.34489167\n",
      "val Loss: 0.0666 Acc: 98.07%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 6/8\tloss: 0.00575098\n",
      "val Loss: 0.0812 Acc: 97.58%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 6/8\tloss: 0.08686379\n",
      "val Loss: 0.0701 Acc: 97.53%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 6/8\tloss: 0.10170496\n",
      "val Loss: 0.0693 Acc: 97.84%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 6/8\tloss: 0.01494168\n",
      "val Loss: 0.0911 Acc: 96.76%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 6/8\tloss: 0.49400035\n",
      "val Loss: 0.0915 Acc: 96.59%\n",
      "\n",
      "Epoch 7/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 7/8\tloss: 0.02592161\n",
      "val Loss: 0.0902 Acc: 97.83%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 7/8\tloss: 0.01825063\n",
      "val Loss: 0.0501 Acc: 98.68%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 7/8\tloss: 0.12307444\n",
      "val Loss: 0.0447 Acc: 98.11%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 7/8\tloss: 0.00050801\n",
      "val Loss: 0.0451 Acc: 98.53%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 7/8\tloss: 0.00365074\n",
      "val Loss: 0.0428 Acc: 98.19%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 7/8\tloss: 0.00565202\n",
      "val Loss: 0.0447 Acc: 98.98%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 7/8\tloss: 0.00232999\n",
      "val Loss: 0.0365 Acc: 98.67%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 7/8\tloss: 0.01571104\n",
      "val Loss: 0.0437 Acc: 98.83%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 7/8\tloss: 0.01112185\n",
      "val Loss: 0.0534 Acc: 98.95%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 7/8\tloss: 0.01215371\n",
      "val Loss: 0.0509 Acc: 98.73%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 7/8\tloss: 0.11182062\n",
      "val Loss: 0.0501 Acc: 98.55%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 7/8\tloss: 0.01975960\n",
      "val Loss: 0.0465 Acc: 98.67%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 7/8\tloss: 0.00226061\n",
      "val Loss: 0.0451 Acc: 98.52%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 7/8\tloss: 0.00044259\n",
      "val Loss: 0.0501 Acc: 98.39%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 7/8\tloss: 0.28062654\n",
      "val Loss: 0.0554 Acc: 98.28%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 7/8\tloss: 0.00346703\n",
      "val Loss: 0.0680 Acc: 97.98%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 7/8\tloss: 0.07391255\n",
      "val Loss: 0.0569 Acc: 98.10%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No. 18/20\tEpoch 7/8\tloss: 0.13728744\n",
      "val Loss: 0.0622 Acc: 97.48%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 7/8\tloss: 0.01192030\n",
      "val Loss: 0.0915 Acc: 96.76%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 7/8\tloss: 0.54906684\n",
      "val Loss: 0.0844 Acc: 96.75%\n",
      "\n",
      "Epoch 8/8\n",
      "----------\n",
      "Train Index Length: 46\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 1/20\tEpoch 8/8\tloss: 0.02222370\n",
      "val Loss: 0.0865 Acc: 95.65%\n",
      "\n",
      "Train Index Length: 76\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 2/20\tEpoch 8/8\tloss: 0.01373388\n",
      "val Loss: 0.0328 Acc: 98.68%\n",
      "\n",
      "Train Index Length: 106\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 3/20\tEpoch 8/8\tloss: 0.08732905\n",
      "val Loss: 0.0320 Acc: 99.06%\n",
      "\n",
      "Train Index Length: 136\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 4/20\tEpoch 8/8\tloss: 0.00034225\n",
      "val Loss: 0.0341 Acc: 98.53%\n",
      "\n",
      "Train Index Length: 166\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 5/20\tEpoch 8/8\tloss: 0.00340057\n",
      "val Loss: 0.0337 Acc: 98.80%\n",
      "\n",
      "Train Index Length: 196\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 6/20\tEpoch 8/8\tloss: 0.00317827\n",
      "val Loss: 0.0369 Acc: 98.98%\n",
      "\n",
      "Train Index Length: 226\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 7/20\tEpoch 8/8\tloss: 0.00156011\n",
      "val Loss: 0.0281 Acc: 99.12%\n",
      "\n",
      "Train Index Length: 256\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 8/20\tEpoch 8/8\tloss: 0.01186940\n",
      "val Loss: 0.0328 Acc: 99.22%\n",
      "\n",
      "Train Index Length: 286\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 9/20\tEpoch 8/8\tloss: 0.00930333\n",
      "val Loss: 0.0431 Acc: 98.95%\n",
      "\n",
      "Train Index Length: 316\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 10/20\tEpoch 8/8\tloss: 0.00744493\n",
      "val Loss: 0.0449 Acc: 98.73%\n",
      "\n",
      "Train Index Length: 346\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 11/20\tEpoch 8/8\tloss: 0.11790536\n",
      "val Loss: 0.0412 Acc: 98.84%\n",
      "\n",
      "Train Index Length: 376\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 12/20\tEpoch 8/8\tloss: 0.01557308\n",
      "val Loss: 0.0386 Acc: 99.20%\n",
      "\n",
      "Train Index Length: 406\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 13/20\tEpoch 8/8\tloss: 0.00188311\n",
      "val Loss: 0.0363 Acc: 99.01%\n",
      "\n",
      "Train Index Length: 436\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 14/20\tEpoch 8/8\tloss: 0.00031257\n",
      "val Loss: 0.0391 Acc: 99.08%\n",
      "\n",
      "Train Index Length: 466\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 15/20\tEpoch 8/8\tloss: 0.21059828\n",
      "val Loss: 0.0464 Acc: 98.50%\n",
      "\n",
      "Train Index Length: 496\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 16/20\tEpoch 8/8\tloss: 0.00423672\n",
      "val Loss: 0.0571 Acc: 98.19%\n",
      "\n",
      "Train Index Length: 526\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 17/20\tEpoch 8/8\tloss: 0.05520670\n",
      "val Loss: 0.0439 Acc: 98.67%\n",
      "\n",
      "Train Index Length: 556\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 18/20\tEpoch 8/8\tloss: 0.17603171\n",
      "val Loss: 0.0508 Acc: 98.20%\n",
      "\n",
      "Train Index Length: 586\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 19/20\tEpoch 8/8\tloss: 0.00882470\n",
      "val Loss: 0.0870 Acc: 97.27%\n",
      "\n",
      "Train Index Length: 616\t\tTest Index Length: 30\n",
      "\n",
      "Fold No. 20/20\tEpoch 8/8\tloss: 0.50064206\n",
      "val Loss: 0.0753 Acc: 97.73%\n",
      "\n",
      "Training complete in 0m 29s\n",
      "Best val Acc: 99.22%\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataset.X, dataset.y, num_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MLP_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t\t torch.Size([50, 54])\n",
      "linear1.bias \t\t torch.Size([50])\n",
      "linear2.weight \t\t torch.Size([12, 50])\n",
      "linear2.bias \t\t torch.Size([12])\n",
      "linear3.weight \t\t torch.Size([3, 12])\n",
      "Optimizer's state_dict:\n",
      "state \t torch.Size([50, 54])\n",
      "{-5.89698292e-02, -9.99343395e-02, -5.03485911e-02, -5.69856539e-02,\n",
      " -7.16278236e-03, -1.58877718e+00, 8.11489183e-04, -4.37823460e-02,\n",
      " -2.54934955e+00, -1.62832171e-01, -1.87072501e-01, -6.81562573e-02,\n",
      " -1.77684218e-01, -3.19299772e-02, -5.23579359e-01, 1.15196905e-04,\n",
      " -5.70112206e-02, -3.88996303e-01, -1.42749920e-01, -1.58009648e-01,\n",
      " -1.17020376e-01, -1.43296957e-01, -7.37844640e-03, -1.20320511e+00,\n",
      " -1.08741503e-02, -1.58869419e-02, 3.08965836e-02, -1.29745901e-01,\n",
      " -1.51776612e-01, -1.11113183e-01, -1.26800343e-01, -1.10895410e-02,\n",
      " -4.92134959e-01, -1.45323932e-01, -2.52179150e-02, 2.18075201e-01,\n",
      " -1.39375851e-01, -1.64502308e-01, -1.29130647e-01, -1.35496438e-01,\n",
      " -8.68450757e-03, -6.09673917e-01, -2.44947672e-01, -1.82667635e-02,\n",
      " -2.50552535e-01, -1.54282972e-01, -2.16525853e-01, -1.34129390e-01,\n",
      " -1.41964942e-01, -2.34375782e-02, -6.78606272e-01, -1.13538809e-01,\n",
      " -4.65493686e-02, 3.80197242e-02}\n",
      "{-1.2824205e-20, -2.1493059e-20, -4.2992361e-21, -1.4850724e-20,\n",
      " -6.4383223e-21, -6.4644572e-20, 0.0000000e+00, -1.0818151e-20,\n",
      " 3.6875776e-20, -1.0290168e-20, -1.8877249e-20, -3.4162907e-21,\n",
      " -1.0329161e-20, -4.7427903e-21, -4.3096381e-20, 0.0000000e+00,\n",
      " -9.9316700e-21, 2.1940728e-20, -1.0858058e-20, -1.5250965e-20,\n",
      " 0.0000000e+00, -1.0927238e-20, -2.4364724e-21, -4.3096381e-20,\n",
      " 0.0000000e+00, -4.8352637e-21, -1.4922731e-19, -9.4308194e-21,\n",
      " -2.0963376e-20, -6.9278105e-21, -9.1096213e-21, -2.0840949e-21,\n",
      " -6.4644572e-20, -6.8954212e-21, -4.7618845e-21, -3.8945119e-19,\n",
      " -9.3498486e-21, -1.6603914e-20, -3.0256019e-21, -9.5933726e-21,\n",
      " -2.9110206e-21, -4.3096381e-20, -7.3263847e-21, -6.7089037e-21,\n",
      " -2.3435846e-20, -8.8138590e-21, -2.1548191e-20, -2.4142621e-21,\n",
      " -7.6211413e-21, -5.9660466e-21, 0.0000000e+00, -2.5857828e-21,\n",
      " -1.4585837e-20, 1.5952071e-20}\n",
      "{ 5.3504203e-04, 9.0369512e-04, 4.2135332e-04, 5.1766087e-04,\n",
      " 8.6737688e-05, 1.2813172e-02, -2.2424741e-13, 4.1543838e-04,\n",
      " 1.9779691e-02, 1.3561768e-03, 1.6240258e-03, 5.8388541e-04,\n",
      " 1.4570876e-03, 2.7966508e-04, 5.1252693e-03, 6.0192698e-15,\n",
      " 5.2845571e-04, 2.9649294e-03, 1.2283162e-03, 1.3975138e-03,\n",
      " 1.0007218e-03, 1.2300792e-03, 8.0170641e-05, 1.0250539e-02,\n",
      " 1.0250539e-04, 1.6725992e-04, 2.1072451e-04, 1.0831546e-03,\n",
      " 1.3053922e-03, 8.8809466e-04, 1.0619686e-03, 1.0879373e-04,\n",
      " 5.1252693e-03, 1.1788120e-03, 2.5739503e-04, -1.3630977e-03,\n",
      " 1.1577492e-03, 1.3954072e-03, 1.0268091e-03, 1.1365110e-03,\n",
      " 9.0472495e-05, 5.1252693e-03, 1.9988550e-03, 2.0025746e-04,\n",
      " 2.2973528e-03, 1.2919450e-03, 1.7955585e-03, 1.0627648e-03,\n",
      " 1.2127518e-03, 2.1092633e-04, 5.1252693e-03, 9.7380113e-04,\n",
      " 4.1838249e-04, 1.6512749e-04}\n",
      "{ 2.03521568e-02, 3.40676941e-02, 1.94330290e-02, 1.99689325e-02,\n",
      " 6.20790757e-04, 6.45262599e-01, -2.77762592e-04, 1.38205085e-02,\n",
      " 1.09589064e+00, 6.60667866e-02, 7.17329457e-02, 2.83641908e-02,\n",
      " 7.29085058e-02, 1.14016086e-02, 2.05880240e-01, 5.68230280e-06,\n",
      " 1.79813765e-02, 1.68669015e-01, 5.60451597e-02, 6.00412712e-02,\n",
      " 4.65532765e-02, 5.64595573e-02, 2.12420430e-03, 4.83557224e-01,\n",
      " 4.30308934e-03, 4.72671958e-03, -1.32151525e-02, 5.17502874e-02,\n",
      " 5.98793551e-02, 4.58875820e-02, 5.04103377e-02, 3.80119635e-03,\n",
      " 1.51692420e-01, 5.92330173e-02, 8.20040796e-03, -8.92584398e-02,\n",
      " 5.59777282e-02, 6.45273775e-02, 5.26836365e-02, 5.40799201e-02,\n",
      " 2.97498144e-03, 2.31422722e-01, 1.00402042e-01, 5.67167299e-03,\n",
      " 1.12775162e-01, 6.03509545e-02, 8.42911005e-02, 5.58394156e-02,\n",
      " 5.45019284e-02, 7.61793600e-03, 2.87596852e-01, 4.57386151e-02,\n",
      " 1.56460479e-02, -1.06924027e-03}\n",
      "{ 2.8892502e-02, 5.0323077e-02, 2.3632092e-02, 2.7192874e-02,\n",
      " 4.7404910e-03, 7.3223591e-01, -4.3709058e-04, 2.3245033e-02,\n",
      " 1.1457894e+00, 7.5289808e-02, 8.9046620e-02, 3.1111157e-02,\n",
      " 8.1686333e-02, 1.5473492e-02, 2.5058013e-01, -6.8668123e-05,\n",
      " 2.8763101e-02, 1.7758645e-01, 6.7095570e-02, 7.5174317e-02,\n",
      " 5.4517072e-02, 6.7330711e-02, 3.7916941e-03, 5.6687421e-01,\n",
      " 5.0729956e-03, 8.0511235e-03, -9.8176608e-03, 6.0405530e-02,\n",
      " 7.1019992e-02, 5.0907027e-02, 5.9098236e-02, 5.5050598e-03,\n",
      " 2.5416586e-01, 6.7199215e-02, 1.2772599e-02, -1.0213131e-01,\n",
      " 6.4772420e-02, 7.6660372e-02, 5.9664112e-02, 6.3127503e-02,\n",
      " 4.1649812e-03, 2.8723091e-01, 1.1218671e-01, 9.0420451e-03,\n",
      " 1.0642700e-01, 7.2873518e-02, 1.0243029e-01, 6.1898489e-02,\n",
      " 6.7475200e-02, 1.1710271e-02, 3.0905300e-01, 5.2599721e-02,\n",
      " 2.2783948e-02, -2.1058364e-02}\n",
      "{ 4.4004873e-06, 1.3354783e-03, 4.6084038e-04, -6.2898715e-04,\n",
      " 4.8683115e-04, -1.4104938e-02, 2.2646794e-04, 3.8302559e-04,\n",
      " -3.6061760e-03, -1.0706143e-03, -2.0596436e-03, 3.2752616e-04,\n",
      " -9.7255333e-04, -7.6324184e-04, -1.2636896e-02, -7.7854289e-05,\n",
      " -2.1220462e-03, 9.8258676e-04, -1.0501780e-03, -1.2376863e-03,\n",
      " -4.6858046e-04, -1.0890604e-03, -3.9234917e-04, -9.8956912e-04,\n",
      " -3.3740856e-04, -9.0530247e-04, 7.5237155e-03, -8.2493183e-04,\n",
      " -2.5425295e-03, 5.2905915e-04, -8.6637284e-04, -7.9428096e-04,\n",
      " -5.2879001e-03, 9.0041902e-04, -2.2199280e-03, 2.8171334e-03,\n",
      " -8.3143823e-04, -2.9355809e-03, 4.9503718e-04, -9.5613097e-04,\n",
      " -8.3529373e-04, -5.3302856e-04, -8.0494276e-05, -2.0736093e-03,\n",
      " -6.1762203e-03, -1.1196929e-03, -1.6313414e-03, 4.7460687e-04,\n",
      " -1.5138197e-03, -5.6018360e-04, 6.2168320e-03, 3.5747452e-04,\n",
      " -1.1798774e-03, 5.7047652e-04}\n",
      "{-1.3606960e-03, -2.6820365e-03, -5.6247157e-04, -1.0091095e-03,\n",
      " -8.2614366e-04, -1.0222861e-02, -2.7587177e-07, -1.7806777e-03,\n",
      " 6.2599489e-03, -6.7502656e-04, -2.2786283e-03, 8.2069128e-05,\n",
      " -5.0066307e-04, -6.9512601e-04, -6.5898718e-03, 1.7403703e-05,\n",
      " -2.2465885e-03, -3.6619860e-04, -1.4084984e-03, -2.1969711e-03,\n",
      " -1.0009338e-03, -1.3520401e-03, -3.2243237e-04, -9.9424887e-03,\n",
      " -1.4432930e-04, -6.3759385e-04, 8.8795333e-04, -8.9573703e-04,\n",
      " -1.0751528e-03, -1.3529569e-04, -9.5218047e-04, -3.1593716e-04,\n",
      " -2.2325812e-02, -7.2823232e-04, -9.3398115e-04, 3.8700090e-03,\n",
      " -8.1684964e-04, -1.3716595e-03, -7.2160061e-04, -8.8921865e-04,\n",
      " -1.3445989e-04, -7.8654261e-03, -1.0047541e-03, -5.4037845e-04,\n",
      " 2.1723749e-03, -1.7237026e-03, -2.2995260e-03, -2.3932211e-04,\n",
      " -2.0545546e-03, -7.4882369e-04, 1.3904693e-03, -8.3261571e-04,\n",
      " -1.1568081e-03, 3.1679021e-03}\n",
      "{ 3.1861939e-02, 5.6686968e-02, 1.9178623e-02, 2.8796826e-02,\n",
      " 1.1499329e-02, 4.8229903e-01, -1.9611817e-04, 3.0260224e-02,\n",
      " 5.1462382e-01, 4.9760319e-02, 7.4241102e-02, 1.8316323e-02,\n",
      " 5.1022485e-02, 1.5800385e-02, 2.0462422e-01, -3.1369749e-05,\n",
      " 3.8152147e-02, 8.3153650e-02, 5.1757388e-02, 6.5159798e-02,\n",
      " 4.0179629e-02, 5.1227763e-02, 5.7362476e-03, 3.9615342e-01,\n",
      " 4.1238302e-03, 1.1635618e-02, -2.5919825e-04, 4.2858064e-02,\n",
      " 5.2218609e-02, 2.9806873e-02, 4.2686034e-02, 6.2931147e-03,\n",
      " 3.5914344e-01, 4.3995485e-02, 1.6373124e-02, -6.6389412e-02,\n",
      " 4.4587649e-02, 5.7908453e-02, 3.8501598e-02, 4.4747986e-02,\n",
      " 4.5146020e-03, 2.4695997e-01, 7.0858799e-02, 1.2031427e-02,\n",
      " 3.6472280e-02, 5.6666639e-02, 8.0056265e-02, 3.5409268e-02,\n",
      " 5.6692056e-02, 1.4568901e-02, 1.5139982e-01, 3.7356522e-02,\n",
      " 2.6421197e-02, -5.2608203e-02}\n",
      "{-1.64534021e-02, -2.86002047e-02, -1.30799245e-02, -1.55844372e-02,\n",
      " -2.94280937e-03, -3.99772882e-01, 2.73465295e-04, -1.31959841e-02,\n",
      " -6.17927015e-01, -4.12785001e-02, -4.93233874e-02, -1.68257952e-02,\n",
      " -4.46883067e-02, -8.72765947e-03, -1.35316685e-01, 4.57196547e-05,\n",
      " -1.63280629e-02, -9.48845968e-02, -3.68105844e-02, -4.14793938e-02,\n",
      " -2.96987519e-02, -3.69194038e-02, -2.18457845e-03, -3.08735281e-01,\n",
      " -2.69012339e-03, -4.64940164e-03, 8.04926455e-03, -3.32504958e-02,\n",
      " -3.94234397e-02, -2.79213898e-02, -3.25171240e-02, -3.08790570e-03,\n",
      " -1.40390098e-01, -3.67178619e-02, -7.15161674e-03, 5.20894937e-02,\n",
      " -3.56635638e-02, -4.26211283e-02, -3.25170271e-02, -3.47903147e-02,\n",
      " -2.46988563e-03, -1.59256518e-01, -6.10178486e-02, -5.35653392e-03,\n",
      " -5.49773090e-02, -4.00895961e-02, -5.68727106e-02, -3.37967277e-02,\n",
      " -3.71016748e-02, -6.72234362e-03, -1.70238853e-01, -2.82684062e-02,\n",
      " -1.31535726e-02, 1.63795538e-02}\n",
      "{-6.16590865e-03, -9.90815461e-03, -3.30603775e-03, -6.15107920e-03,\n",
      " -1.78010610e-03, -7.91183934e-02, -4.13475558e-04, -5.03286254e-03,\n",
      " -9.55039039e-02, -1.06771346e-02, -1.57468729e-02, -5.80575783e-03,\n",
      " -1.02188308e-02, -2.58797687e-03, -7.48674199e-02, -6.85402192e-05,\n",
      " -6.02917327e-03, -1.31665859e-02, -1.13815693e-02, -1.47224329e-02,\n",
      " -9.33249854e-03, -1.11771300e-02, -1.40293501e-03, -9.49745104e-02,\n",
      " -1.58122554e-03, -2.76305154e-03, -3.19971964e-02, -8.56127404e-03,\n",
      " -1.23291695e-02, -5.28234802e-03, -8.53064097e-03, -1.54687534e-03,\n",
      " -8.78302082e-02, -7.73937255e-03, -4.00222512e-03, -1.80631913e-02,\n",
      " -8.70971475e-03, -1.15413386e-02, -5.58381109e-03, -9.10702348e-03,\n",
      " -1.40256714e-03, -4.30011339e-02, -1.46310059e-02, -3.47664743e-03,\n",
      " -3.81137356e-02, -1.01247272e-02, -1.26775391e-02, -5.77125186e-03,\n",
      " -1.06927985e-02, -2.09701667e-03, -1.36219319e-02, -9.90121998e-03,\n",
      " -4.16293694e-03, -3.31408717e-02}\n",
      "{-5.4969722e-03, -1.1048922e-02, -2.6871152e-03, -4.0975600e-03,\n",
      " -3.2923124e-03, -3.1350810e-02, 1.9717417e-04, -6.3741864e-03,\n",
      " 2.1355137e-02, -2.9870460e-03, -8.4435064e-03, -8.1458857e-04,\n",
      " -2.1961166e-03, -2.1787251e-03, -2.3962105e-02, 6.2713066e-06,\n",
      " -6.9274651e-03, -4.1304762e-03, -5.3034215e-03, -8.2654748e-03,\n",
      " -3.6889035e-03, -5.0967750e-03, -1.1240025e-03, -3.7967280e-02,\n",
      " -3.7709557e-04, -2.1229843e-03, -6.7102090e-03, -3.3862197e-03,\n",
      " -3.5325773e-03, -1.2079407e-03, -3.5396256e-03, -8.4283482e-04,\n",
      " -7.4321009e-02, -3.1999978e-03, -2.4630895e-03, 1.2196636e-02,\n",
      " -3.1316169e-03, -4.2374800e-03, -2.8122640e-03, -3.5112677e-03,\n",
      " -2.9081901e-04, -2.7739681e-02, -4.1002813e-03, -1.3455979e-03,\n",
      " 1.0700358e-02, -6.1757206e-03, -8.6475154e-03, -1.4816578e-03,\n",
      " -7.0815356e-03, -2.6112350e-03, 1.2635086e-03, -3.7506481e-03,\n",
      " -4.0631965e-03, 9.7257998e-03}\n",
      "{ 5.08765504e-02, 8.92388523e-02, 3.64978276e-02, 4.72841114e-02,\n",
      " 1.28284153e-02, 1.04685998e+00, -5.54690836e-04, 4.38890830e-02,\n",
      " 1.46893501e+00, 1.08233094e-01, 1.38914853e-01, 4.31142263e-02,\n",
      " 1.15278654e-01, 2.62371041e-02, 3.87515634e-01, -8.85138797e-05,\n",
      " 5.47507629e-02, 2.27907822e-01, 1.01511568e-01, 1.18892729e-01,\n",
      " 8.10023844e-02, 1.01364546e-01, 7.78659806e-03, 8.26744199e-01,\n",
      " 7.76544400e-03, 1.61674693e-02, -1.06259454e-02, 8.89025703e-02,\n",
      " 1.06254049e-01, 7.05037490e-02, 8.74792635e-02, 9.79073159e-03,\n",
      " 4.97299880e-01, 9.60546359e-02, 2.38970555e-02, -1.38654992e-01,\n",
      " 9.44144055e-02, 1.15734935e-01, 8.46540630e-02, 9.29645300e-02,\n",
      " 7.44887348e-03, 4.52900112e-01, 1.58401281e-01, 1.76643115e-02,\n",
      " 1.28727496e-01, 1.10390209e-01, 1.55964240e-01, 8.47908854e-02,\n",
      " 1.05093315e-01, 2.17376240e-02, 4.07555282e-01, 7.68413618e-02,\n",
      " 4.11898158e-02, -5.84563389e-02}\n",
      "{-1.00417882e-02, -1.74859669e-02, -4.16849460e-03, -9.41264536e-03,\n",
      " -4.94537735e-03, -6.26860186e-02, -4.14731949e-05, -9.73692443e-03,\n",
      " 3.02419942e-02, -7.94014614e-03, -1.72882266e-02, -2.17954488e-03,\n",
      " -7.10284058e-03, -4.53306176e-03, -4.38504368e-02, -1.83181710e-05,\n",
      " -1.22925797e-02, 2.84038996e-03, -1.04128728e-02, -1.54105844e-02,\n",
      " -7.19504012e-03, -1.01402598e-02, -2.10598879e-03, -6.12571836e-02,\n",
      " -8.82381049e-04, -4.17615287e-03, -3.38154566e-03, -7.53632700e-03,\n",
      " -1.02418903e-02, -3.14201764e-03, -7.76066165e-03, -1.88579678e-03,\n",
      " -1.15346812e-01, -6.21109735e-03, -5.15527604e-03, 6.87401742e-04,\n",
      " -7.47663109e-03, -1.19617190e-02, -4.63973032e-03, -8.00544675e-03,\n",
      " -1.66713772e-03, -5.86463623e-02, -8.35505314e-03, -4.61675879e-03,\n",
      " 6.95527578e-03, -1.08681070e-02, -1.66249648e-02, -2.68028677e-03,\n",
      " -1.18996175e-02, -4.85467259e-03, -1.65965920e-03, -5.87212527e-03,\n",
      " -9.11509991e-03, 1.99439377e-02}\n",
      "{-4.4750233e-04, -7.0336508e-04, -1.6360656e-04, -5.0549954e-04,\n",
      " -1.8702779e-04, -1.2916602e-03, 1.6845941e-09, -2.9013609e-04,\n",
      " 1.1540641e-03, -2.9296934e-04, -6.0591468e-04, -6.3287276e-05,\n",
      " -2.3956881e-04, -1.7441218e-04, -1.2884681e-03, 1.9231179e-06,\n",
      " -3.8429181e-04, 7.9881318e-04, -3.1736805e-04, -5.0210644e-04,\n",
      " -1.7068408e-04, -3.0123082e-04, -8.6686727e-05, -1.3376217e-03,\n",
      " 1.8666120e-06, -1.8092056e-04, 5.3796440e-04, -2.8979353e-04,\n",
      " -5.4948602e-04, -1.5655333e-04, -2.8120817e-04, -6.4722437e-05,\n",
      " -2.7660590e-03, -1.5653920e-04, -1.4923931e-04, -2.9141076e-03,\n",
      " -3.0890989e-04, -5.6799629e-04, -5.6340414e-05, -3.3984409e-04,\n",
      " -1.2260987e-04, -2.1480755e-03, -2.2418911e-04, -2.6585886e-04,\n",
      " 3.3526486e-04, -3.0145625e-04, -6.3124846e-04, -6.2408530e-05,\n",
      " -3.0875360e-04, -1.8852172e-04, -7.6290767e-04, -2.0137340e-05,\n",
      " -4.1656569e-04, 9.5284067e-04}\n",
      "{ 7.16602951e-02, 1.22419834e-01, 5.43100946e-02, 6.94723725e-02,\n",
      " 1.40902260e-02, 1.56991756e+00, 3.59432306e-04, 5.59512302e-02,\n",
      " 2.39613152e+00, 1.69829234e-01, 2.05819607e-01, 7.31201097e-02,\n",
      " 1.82405695e-01, 3.59082185e-02, 6.22383535e-01, -1.08907181e-04,\n",
      " 6.69383630e-02, 3.53848010e-01, 1.54025063e-01, 1.76572070e-01,\n",
      " 1.24806613e-01, 1.54319882e-01, 1.02524413e-02, 1.28139818e+00,\n",
      " 1.24801435e-02, 2.11671088e-02, 3.54446098e-02, 1.35701612e-01,\n",
      " 1.62248150e-01, 1.12151228e-01, 1.33118123e-01, 1.28943976e-02,\n",
      " 6.59675956e-01, 1.48391515e-01, 2.99729295e-02, -1.58434808e-01,\n",
      " 1.44949928e-01, 1.74124643e-01, 1.27718076e-01, 1.42444596e-01,\n",
      " 1.13121727e-02, 6.62056744e-01, 2.47176707e-01, 2.48016678e-02,\n",
      " 2.72331268e-01, 1.60652623e-01, 2.26580352e-01, 1.31120697e-01,\n",
      " 1.49966136e-01, 2.78777909e-02, 6.38285935e-01, 1.20801561e-01,\n",
      " 5.65830432e-02, 1.08553506e-02}\n",
      "{ 5.35486911e-13, 4.79021120e-14, 1.72864110e-14, 6.73266672e-13,\n",
      " 2.48334421e-13, -4.69014627e-12, -6.02010642e-14, 2.31116667e-14,\n",
      " -6.29186356e-11, -9.23601473e-14, -9.88554941e-14, -2.23239924e-13,\n",
      " -1.30780207e-13, 1.69275875e-13, -3.09758599e-12, 2.82964379e-16,\n",
      " 3.85646593e-13, -5.23177335e-12, -1.39422653e-13, 1.07519896e-13,\n",
      " -3.13296210e-13, -1.60500790e-13, 5.69836265e-14, -1.66594516e-12,\n",
      " 2.82964379e-16, 1.58429205e-13, 1.55149819e-12, 5.65469438e-14,\n",
      " 2.82452555e-13, 6.69747190e-14, -8.36288504e-15, 2.67264103e-14,\n",
      " -3.17097108e-12, -1.28303347e-13, 2.41792263e-14, 7.41939669e-12,\n",
      " 3.88317308e-14, 1.81912970e-13, -1.07888850e-13, 4.63210199e-14,\n",
      " 1.21989604e-13, 1.27779601e-12, -7.90748164e-13, 2.32550334e-13,\n",
      " -3.69864252e-12, -1.68409557e-13, 2.44836811e-13, -4.48485108e-13,\n",
      " -1.90793995e-13, 2.86533194e-13, -7.33854167e-14, -4.25757574e-13,\n",
      " 7.21329192e-13, -1.77437210e-12}\n",
      "{0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      " 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      " 0., 0., 0., 0., 0., 0.}\n",
      "{-4.8890137e-03, -9.1119148e-03, -2.4738568e-03, -3.9903303e-03,\n",
      " -2.6537357e-03, -3.2785051e-02, -1.2412228e-04, -5.3170216e-03,\n",
      " 1.9049974e-02, -3.1347130e-03, -7.5897877e-03, -1.0136167e-03,\n",
      " -2.7532580e-03, -2.0694314e-03, -2.0984732e-02, -3.8548174e-06,\n",
      " -6.3983370e-03, 1.7154543e-03, -5.1846369e-03, -7.8602675e-03,\n",
      " -3.9967322e-03, -4.9914201e-03, -1.0273629e-03, -3.0768448e-02,\n",
      " -3.8667256e-04, -1.9533758e-03, -1.0285345e-03, -3.3743673e-03,\n",
      " -3.3504143e-03, -1.3320064e-03, -3.5655834e-03, -7.3663931e-04,\n",
      " -7.0502378e-02, -3.5401410e-03, -2.1605839e-03, 9.9861976e-03,\n",
      " -3.2692819e-03, -4.5817718e-03, -3.0534253e-03, -3.4978748e-03,\n",
      " -3.4023955e-04, -3.1443223e-02, -4.7250018e-03, -1.3549612e-03,\n",
      " 1.0327138e-02, -5.6629982e-03, -7.7048726e-03, -1.4029410e-03,\n",
      " -6.4526950e-03, -2.3398919e-03, 2.6264798e-04, -3.7969120e-03,\n",
      " -3.8408602e-03, 1.2190697e-02}\n",
      "{ 5.41081876e-02, 9.57203582e-02, 3.46165374e-02, 4.94648069e-02,\n",
      " 1.75657235e-02, 9.16726649e-01, -4.77961788e-04, 4.95954379e-02,\n",
      " 1.10515916e+00, 9.46321338e-02, 1.33120596e-01, 3.58403325e-02,\n",
      " 9.86192152e-02, 2.71836836e-02, 3.62031400e-01, -8.44218157e-05,\n",
      " 6.23701662e-02, 1.75572380e-01, 9.42819715e-02, 1.15430132e-01,\n",
      " 7.39046708e-02, 9.36567783e-02, 9.15938057e-03, 7.40169048e-01,\n",
      " 7.31125195e-03, 1.87156573e-02, -6.51953556e-03, 7.99575076e-02,\n",
      " 9.65637043e-02, 5.88855594e-02, 7.92348534e-02, 1.05049778e-02,\n",
      " 5.75922370e-01, 8.37429091e-02, 2.67266110e-02, -1.26463577e-01,\n",
      " 8.38663876e-02, 1.06418222e-01, 7.35932142e-02, 8.35010856e-02,\n",
      " 7.70260487e-03, 4.40476060e-01, 1.35990500e-01, 1.97265856e-02,\n",
      " 8.43277574e-02, 1.03140265e-01, 1.46148443e-01, 7.02990964e-02,\n",
      " 1.01044200e-01, 2.42269486e-02, 3.17672580e-01, 6.90121353e-02,\n",
      " 4.46397886e-02, -8.37250203e-02}\n",
      "{ 8.78567994e-03, 1.73532329e-02, -2.56566145e-03, 6.19496778e-03,\n",
      " 1.05950022e-02, -2.36570299e-01, 2.32264720e-04, 1.42504405e-02,\n",
      " -7.22675025e-01, -2.48093978e-02, -6.80831075e-03, -1.35894250e-02,\n",
      " -3.11524794e-02, 3.01784184e-03, -2.81572044e-02, 3.06921938e-05,\n",
      " 1.87568516e-02, -1.03762880e-01, -1.10368878e-02, -2.03429535e-03,\n",
      " -1.15202926e-02, -1.21021979e-02, 3.37659195e-03, -1.50401339e-01,\n",
      " -4.81820665e-04, 6.34591328e-03, 1.24206506e-02, -1.55759305e-02,\n",
      " -1.63755827e-02, -2.23634467e-02, -1.40860416e-02, 2.04687193e-03,\n",
      " 1.98608577e-01, -2.24034619e-02, 7.42613152e-03, 2.22496986e-02,\n",
      " -1.87891535e-02, -1.52561888e-02, -2.03042589e-02, -1.64855141e-02,\n",
      " 8.82272609e-04, -9.90571082e-03, -4.11494002e-02, 5.29407896e-03,\n",
      " -8.59382376e-02, -1.09717250e-02, -1.53763406e-02, -2.79212482e-02,\n",
      " -3.92927229e-03, 6.09495025e-03, -1.79418489e-01, -1.28498189e-02,\n",
      " 8.68897140e-03, -5.06698042e-02}\n",
      "{ 3.25003341e-02, 6.00904003e-02, 1.06907487e-02, 2.70075072e-02,\n",
      " 2.03142799e-02, 7.35362917e-02, 1.76707166e-04, 3.81988026e-02,\n",
      " -4.65919077e-01, 6.66072778e-03, 4.49026302e-02, -2.47201696e-03,\n",
      " 1.65220350e-04, 1.46203320e-02, 1.19730972e-01, 2.43615614e-05,\n",
      " 4.90656346e-02, -5.86467534e-02, 2.44185794e-02, 4.44958173e-02,\n",
      " 1.57589521e-02, 2.27702428e-02, 8.02694261e-03, 1.11695394e-01,\n",
      " 2.51120795e-03, 1.56843700e-02, 1.69419888e-02, 1.25238225e-02,\n",
      " 1.80913135e-02, -4.85564768e-03, 1.41659323e-02, 6.93489239e-03,\n",
      " 4.93049204e-01, 5.68181090e-03, 2.05703937e-02, -2.45882608e-02,\n",
      " 1.00093931e-02, 2.33809575e-02, 4.27718274e-03, 1.27816107e-02,\n",
      " 4.12078481e-03, 1.63061351e-01, 3.57647240e-03, 1.45446360e-02,\n",
      " -7.15870336e-02, 2.80732848e-02, 3.89045738e-02, -6.88191503e-03,\n",
      " 3.65356654e-02, 1.72784626e-02, -1.01130001e-01, 1.26465559e-02,\n",
      " 2.82411594e-02, -9.24634039e-02}\n",
      "{ 1.58908209e-04, 2.78394407e-04, 6.36586919e-05, 1.52350476e-04,\n",
      " 8.01051647e-05, 9.25231900e-04, 1.05368492e-10, 1.55456393e-04,\n",
      " -5.27809025e-04, 9.88308966e-05, 2.46442010e-04, 2.41568359e-05,\n",
      " 9.12022369e-05, 5.57704079e-05, 6.39429200e-04, 1.01815264e-13,\n",
      " 1.73897963e-04, 1.67257254e-04, 1.48887659e-04, 2.09178193e-04,\n",
      " 1.06710911e-04, 1.49675252e-04, 2.39472738e-05, 9.25218977e-04,\n",
      " 1.23359177e-05, 4.96026987e-05, -1.15183402e-05, 1.14237911e-04,\n",
      " 1.64687415e-04, 4.36552764e-05, 1.15446725e-04, 3.17376616e-05,\n",
      " 1.49678148e-03, 7.22102050e-05, 8.57399064e-05, -9.96519375e-05,\n",
      " 1.13729606e-04, 1.64007317e-04, 6.50685979e-05, 1.22016390e-04,\n",
      " 2.58879663e-05, 9.25217639e-04, 1.05311701e-04, 6.98840377e-05,\n",
      " -2.74995458e-04, 1.51688917e-04, 2.55112274e-04, 3.22885026e-05,\n",
      " 1.66630765e-04, 8.48915079e-05, 5.29427435e-09, 6.51353403e-05,\n",
      " 1.72811517e-04, -5.04361815e-04}\n",
      "{0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      " 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      " 0., 0., 0., 0., 0., 0.}\n",
      "{ 1.35732908e-02, 2.35141385e-02, 1.15013281e-02, 1.29064815e-02,\n",
      " 1.80318928e-03, 3.62302691e-01, -1.86486723e-04, 1.05659859e-02,\n",
      " 5.82547843e-01, 3.74946892e-02, 4.32767980e-02, 1.57157425e-02,\n",
      " 4.08529602e-02, 7.30754342e-03, 1.24289118e-01, -3.25455840e-05,\n",
      " 1.29150739e-02, 8.96734297e-02, 3.28830630e-02, 3.64011303e-02,\n",
      " 2.67992411e-02, 3.30466516e-02, 1.70316419e-03, 2.79546171e-01,\n",
      " 2.46886304e-03, 3.65528138e-03, -4.79730964e-03, 2.98502501e-02,\n",
      " 3.51849496e-02, 2.55407728e-02, 2.91464105e-02, 2.58011185e-03,\n",
      " 1.13345489e-01, 3.34426500e-02, 5.85630583e-03, -4.68811691e-02,\n",
      " 3.21462788e-02, 3.78288254e-02, 2.95698792e-02, 3.12560685e-02,\n",
      " 2.04716995e-03, 1.38284311e-01, 5.58862798e-02, 4.26298566e-03,\n",
      " 5.53845502e-02, 3.55490781e-02, 5.00210449e-02, 3.10695283e-02,\n",
      " 3.26338932e-02, 5.34513965e-03, 1.57369137e-01, 2.58990005e-02,\n",
      " 1.06110824e-02, -6.46160450e-03}\n",
      "{ 3.6277510e-02, 6.1612844e-02, 2.8593898e-02, 3.5095572e-02,\n",
      " 5.8504795e-03, 8.9950335e-01, -6.6605967e-04, 2.8283628e-02,\n",
      " 1.3870380e+00, 9.2725240e-02, 1.1089341e-01, 3.7669599e-02,\n",
      " 1.0024191e-01, 1.9716863e-02, 3.1122115e-01, 1.3225824e-05,\n",
      " 3.7217148e-02, 2.1352014e-01, 8.2527123e-02, 9.2861190e-02,\n",
      " 6.6790223e-02, 8.2745239e-02, 4.9944362e-03, 6.8862343e-01,\n",
      " 6.5747485e-03, 1.0734187e-02, -1.6280731e-02, 7.4430794e-02,\n",
      " 8.9363135e-02, 6.1668944e-02, 7.2868444e-02, 7.3609673e-03,\n",
      " 3.1579131e-01, 8.1191473e-02, 1.7313158e-02, -1.1327085e-01,\n",
      " 7.9670645e-02, 9.6285023e-02, 7.1864247e-02, 7.7849366e-02,\n",
      " 5.9185363e-03, 3.5565341e-01, 1.3688341e-01, 1.3043067e-02,\n",
      " 1.3916248e-01, 8.9369513e-02, 1.2616484e-01, 7.4330941e-02,\n",
      " 8.3240665e-02, 1.5068132e-02, 3.7011662e-01, 6.3806079e-02,\n",
      " 2.9773129e-02, -2.6284035e-02}\n",
      "{-3.26730646e-02, -5.97899780e-02, -1.43870516e-02, -2.78420355e-02,\n",
      " -1.74604878e-02, -2.18485758e-01, -2.19272828e-04, -3.55294682e-02,\n",
      " 1.20456286e-01, -2.19800025e-02, -5.47126271e-02, -5.25933970e-03,\n",
      " -1.85371526e-02, -1.49219604e-02, -1.45011306e-01, -3.98035900e-06,\n",
      " -4.48011719e-02, 9.44714062e-03, -3.42618302e-02, -5.20120785e-02,\n",
      " -2.47706808e-02, -3.30180489e-02, -7.14945700e-03, -2.10562885e-01,\n",
      " -2.90977047e-03, -1.40148606e-02, -9.28928517e-03, -2.32342537e-02,\n",
      " -2.86386590e-02, -7.93882832e-03, -2.42748652e-02, -6.29709288e-03,\n",
      " -4.50126022e-01, -2.03630347e-02, -1.79942045e-02, 4.55589555e-02,\n",
      " -2.22752094e-02, -3.47450003e-02, -1.73081048e-02, -2.40542665e-02,\n",
      " -3.82992160e-03, -1.97839186e-01, -2.83141509e-02, -1.26618799e-02,\n",
      " 3.93985473e-02, -3.82019952e-02, -5.35336956e-02, -8.40814319e-03,\n",
      " -4.35077734e-02, -1.63046718e-02, 8.78589042e-03, -2.20560990e-02,\n",
      " -2.75441837e-02, 8.18649679e-02}\n",
      "{-1.3777442e-10, -3.7482784e-10, -7.0160072e-11, -8.5634035e-11,\n",
      " -7.6842581e-11, -1.6005519e-09, -1.6006496e-11, -2.2320523e-10,\n",
      " -3.1423886e-10, -1.7556680e-10, -2.6604791e-10, -5.3796884e-11,\n",
      " -1.8285032e-10, -5.7317980e-11, -1.2003896e-09, 0.0000000e+00,\n",
      " -1.3063561e-10, 4.4560719e-10, -2.0161474e-10, -3.4049216e-10,\n",
      " -9.1773998e-11, -1.9580831e-10, -4.0527390e-11, -1.6006496e-09,\n",
      " 3.9070178e-15, -8.0436706e-11, -1.1989446e-09, -1.7096093e-10,\n",
      " -3.4088607e-10, -4.4256248e-11, -1.6875498e-10, -4.0605921e-11,\n",
      " -2.4010722e-09, -2.0807858e-10, -9.5045735e-11, -3.0549074e-09,\n",
      " -1.8473507e-10, -2.2750027e-10, 3.4119466e-14, -1.9117699e-10,\n",
      " -3.9500309e-11, -1.2004873e-09, -2.8010000e-10, -8.5564188e-11,\n",
      " -4.0520147e-09, -2.2118254e-10, -3.7343831e-10, -1.7332347e-10,\n",
      " -1.9974962e-10, -5.1506861e-11, -4.0016240e-10, -4.7966742e-11,\n",
      " -9.3188728e-11, -5.8934035e-10}\n",
      "{-6.52479678e-02, -1.12315729e-01, -5.53543940e-02, -6.26002401e-02,\n",
      " -8.27320851e-03, -1.74464631e+00, 8.76852660e-04, -4.96886447e-02,\n",
      " -2.81428981e+00, -1.81262523e-01, -2.08272070e-01, -7.59512335e-02,\n",
      " -1.97651088e-01, -3.50994654e-02, -5.94590664e-01, -9.68361928e-06,\n",
      " -6.10977560e-02, -4.33153093e-01, -1.58044070e-01, -1.74507096e-01,\n",
      " -1.28654554e-01, -1.58909783e-01, -8.08211789e-03, -1.34308827e+00,\n",
      " -1.18125491e-02, -1.74110979e-02, 2.86837444e-02, -1.43946320e-01,\n",
      " -1.69948936e-01, -1.23577580e-01, -1.40488967e-01, -1.22799175e-02,\n",
      " -5.28028369e-01, -1.61112159e-01, -2.76527368e-02, 2.18644202e-01,\n",
      " -1.55165657e-01, -1.82767943e-01, -1.42138556e-01, -1.50858402e-01,\n",
      " -1.00752898e-02, -6.64478421e-01, -2.69295663e-01, -2.07512528e-02,\n",
      " -2.73170412e-01, -1.70685813e-01, -2.41170377e-01, -1.50008455e-01,\n",
      " -1.56185567e-01, -2.56255940e-02, -7.63062596e-01, -1.23966612e-01,\n",
      " -5.13924174e-02, 3.06027941e-02}\n",
      "{ 1.3799833e-12, 2.2372230e-12, 4.2128060e-13, 1.5050225e-12,\n",
      " 6.6614921e-13, 4.9137534e-12, 4.5108705e-20, 1.1859903e-12,\n",
      " -4.1391790e-12, 9.4241075e-13, 1.8996822e-12, 3.1033268e-13,\n",
      " 8.7370725e-13, 4.7944065e-13, 4.9137543e-12, 0.0000000e+00,\n",
      " 1.2499081e-12, -2.1852407e-12, 1.2300776e-12, 1.7894019e-12,\n",
      " 6.2906115e-13, 1.1982893e-12, 3.2036892e-13, 2.4568782e-12,\n",
      " 9.8275080e-14, 6.3988409e-13, -2.4150554e-12, 8.3903132e-13,\n",
      " 1.1797115e-12, 4.5136781e-13, 8.1810416e-13, 1.9711834e-13,\n",
      " 9.8275103e-12, 4.9137555e-13, 5.7720799e-13, -1.7829051e-12,\n",
      " 1.0567936e-12, 1.6575506e-12, 5.7214959e-13, 9.6339874e-13,\n",
      " 3.2737558e-13, 4.9137543e-12, 7.3706341e-13, 7.6109610e-13,\n",
      " -2.3661101e-12, 1.2438865e-12, 2.0090520e-12, 2.7937732e-13,\n",
      " 1.4400258e-12, 6.3542368e-13, 2.4568761e-12, 4.9137539e-13,\n",
      " 1.2550647e-12, -4.1291211e-12}\n",
      "{ 1.4679911e-12, 1.6765044e-12, 1.2756237e-12, 1.4717405e-12,\n",
      " 1.0421264e-13, 2.8660029e-11, 5.7320056e-13, 5.0864358e-13,\n",
      " -4.6559835e-12, 4.6064992e-12, 4.9571718e-12, 3.8941897e-12,\n",
      " 4.6158966e-12, 1.9518096e-13, 3.5825034e-11, 2.8660028e-13,\n",
      " 3.0358691e-13, 2.2453419e-11, 3.8521708e-12, 4.3955516e-12,\n",
      " 3.4504467e-12, 3.8381299e-12, 2.1161121e-13, 2.1495021e-11,\n",
      " 1.0031009e-12, 3.9359520e-13, 5.6402530e-12, 2.7370625e-12,\n",
      " 3.0664434e-12, 2.1539897e-12, 2.8645070e-12, 2.7395252e-13,\n",
      " 1.4330015e-11, 4.2990043e-12, 7.1714536e-13, -5.0345032e-12,\n",
      " 3.2286697e-12, 3.4598149e-12, 2.8709103e-12, 3.2389757e-12,\n",
      " 1.4942550e-13, -6.2498565e-23, 5.7320056e-12, 3.3160242e-13,\n",
      " 9.2542716e-13, 3.3526962e-12, 4.4331982e-12, 3.0553123e-12,\n",
      " 3.2470183e-12, 2.9498821e-13, 7.1650073e-12, 4.0124037e-12,\n",
      " 6.3041575e-13, 2.8874417e-11}\n",
      "{-2.23809220e-02, -3.72065045e-02, -2.45767552e-02, -2.29316074e-02,\n",
      " 2.53828149e-03, -8.62456262e-01, 5.95214369e-04, -1.25497915e-02,\n",
      " -1.59253275e+00, -8.97753537e-02, -9.07533169e-02, -3.92565429e-02,\n",
      " -1.00270629e-01, -1.29980352e-02, -2.59047210e-01, 8.85823756e-05,\n",
      " -1.47079900e-02, -2.37517089e-01, -7.20485002e-02, -7.37162083e-02,\n",
      " -6.00169189e-02, -7.30266422e-02, -1.40845776e-03, -6.44973397e-01,\n",
      " -5.10698603e-03, -3.58741358e-03, 2.06328947e-02, -6.90387040e-02,\n",
      " -8.04819316e-02, -6.46272898e-02, -6.67065233e-02, -3.90289910e-03,\n",
      " -1.02039844e-01, -7.98130184e-02, -7.08348583e-03, 1.04029454e-01,\n",
      " -7.55492598e-02, -8.52278173e-02, -7.08089322e-02, -7.24186152e-02,\n",
      " -3.78968427e-03, -2.84017205e-01, -1.35543734e-01, -5.75701520e-03,\n",
      " -1.60598904e-01, -7.72649720e-02, -1.09572738e-01, -7.84242973e-02,\n",
      " -6.70197159e-02, -7.29724579e-03, -4.22715098e-01, -5.84867746e-02,\n",
      " -1.68594029e-02, -1.18015185e-02}\n",
      "{-2.02058833e-02, -3.71634662e-02, -9.66229476e-03, -1.69009082e-02,\n",
      " -1.08503960e-02, -1.33761853e-01, -4.25454840e-04, -2.16843896e-02,\n",
      " 1.14290968e-01, -1.33701172e-02, -3.24184969e-02, -4.00284026e-03,\n",
      " -1.15530295e-02, -8.77487566e-03, -8.71597603e-02, -1.44721795e-07,\n",
      " -2.66744494e-02, 5.87476511e-03, -2.13145576e-02, -3.23205963e-02,\n",
      " -1.59536600e-02, -2.05205623e-02, -4.30317083e-03, -1.28507361e-01,\n",
      " -1.62805384e-03, -8.26106314e-03, -7.12586008e-03, -1.40573969e-02,\n",
      " -1.53084043e-02, -5.31041808e-03, -1.47764692e-02, -3.34551977e-03,\n",
      " -2.84311920e-01, -1.38604427e-02, -9.70400311e-03, 3.58239971e-02,\n",
      " -1.34997461e-02, -1.98124442e-02, -1.17139481e-02, -1.45216240e-02,\n",
      " -1.74646173e-03, -1.25671536e-01, -1.87305715e-02, -6.40866626e-03,\n",
      " 2.97281127e-02, -2.33779922e-02, -3.22391316e-02, -5.57130482e-03,\n",
      " -2.65593566e-02, -9.76405945e-03, 3.58699728e-03, -1.49597535e-02,\n",
      " -1.62195377e-02, 4.94672209e-02}\n",
      "{ 1.1209142e-03, 3.7115377e-03, 9.4467402e-04, 3.4584984e-04,\n",
      " 1.2387191e-03, -2.0092061e-02, -5.8267987e-04, 9.2027453e-04,\n",
      " -7.9722665e-03, -1.6357896e-03, -1.6129164e-03, -1.6132327e-04,\n",
      " -2.1944626e-03, -5.6277949e-04, -8.8773808e-03, -1.4443355e-04,\n",
      " -1.4841389e-03, 2.9495684e-03, -1.3093333e-03, -1.0276806e-03,\n",
      " -1.5586787e-03, -1.3186759e-03, -6.5437853e-05, -1.1377023e-03,\n",
      " -4.9355754e-04, -2.7933958e-04, 1.0812760e-02, -9.6522336e-04,\n",
      " -2.2684494e-03, 3.2012284e-04, -1.0921585e-03, -6.2305352e-04,\n",
      " -8.2962643e-03, -9.5279457e-04, -1.8495253e-03, -2.1430980e-03,\n",
      " -1.1769482e-03, -2.7062518e-03, -8.6339610e-04, -9.6072961e-04,\n",
      " -3.9704691e-04, -8.1878379e-03, -2.4729460e-03, -1.0981408e-03,\n",
      " -1.5358964e-02, -1.3083427e-03, -5.6294270e-04, -5.1286089e-04,\n",
      " -1.5782870e-03, 3.0903117e-05, 2.8824497e-03, -1.0979850e-03,\n",
      " -1.7233368e-05, 1.2586871e-04}\n",
      "{ 7.52858818e-02, 1.31454572e-01, 5.70299551e-02, 7.05875084e-02,\n",
      " 1.61368251e-02, 1.69168293e+00, -9.39066522e-04, 6.27592802e-02,\n",
      " 2.50593019e+00, 1.74971983e-01, 2.16120824e-01, 7.09061250e-02,\n",
      " 1.87969968e-01, 3.93842533e-02, 6.06155038e-01, -1.42851335e-04,\n",
      " 7.80053362e-02, 3.85863662e-01, 1.59958363e-01, 1.83661848e-01,\n",
      " 1.28545552e-01, 1.60100326e-01, 1.08428765e-02, 1.32296681e+00,\n",
      " 1.21159442e-02, 2.27511022e-02, -2.09104773e-02, 1.42126605e-01,\n",
      " 1.69142753e-01, 1.16090335e-01, 1.39430866e-01, 1.43924644e-02,\n",
      " 6.98234797e-01, 1.55395076e-01, 3.43072750e-02, -2.21052840e-01,\n",
      " 1.51698843e-01, 1.83374286e-01, 1.37169078e-01, 1.48695499e-01,\n",
      " 1.11834258e-02, 6.99959517e-01, 2.57714987e-01, 2.54344959e-02,\n",
      " 2.27194265e-01, 1.73631310e-01, 2.45289981e-01, 1.39937297e-01,\n",
      " 1.63122311e-01, 3.14095914e-02, 6.88125789e-01, 1.22589141e-01,\n",
      " 6.04947731e-02, -7.33536631e-02}\n",
      "{-2.6895497e-03, -6.0311044e-03, -1.0842961e-03, -1.8655492e-03,\n",
      " -1.6489453e-03, -1.6530488e-02, -1.6613959e-07, -3.9074272e-03,\n",
      " 3.7638715e-03, -2.4093515e-03, -5.4529747e-03, -6.3897367e-04,\n",
      " -2.0501441e-03, -1.2074615e-03, -2.0254651e-02, 7.7914938e-06,\n",
      " -3.2359981e-03, -8.8454020e-04, -3.1748570e-03, -4.6107639e-03,\n",
      " -1.8577087e-03, -3.1590504e-03, -6.0559280e-04, -2.6965002e-02,\n",
      " -1.3621291e-04, -1.1974896e-03, -4.3607051e-03, -2.3725505e-03,\n",
      " -3.6598591e-03, -1.1296168e-03, -2.3483946e-03, -6.1102776e-04,\n",
      " -3.4013115e-02, -2.0280157e-03, -1.6222380e-03, -3.0268820e-03,\n",
      " -2.4917403e-03, -3.5518820e-03, -1.4841050e-03, -2.6077782e-03,\n",
      " -5.0641887e-04, -1.1377164e-02, -2.1004598e-03, -1.3086409e-03,\n",
      " 3.0824752e-03, -3.7074508e-03, -5.5389171e-03, -1.7828897e-03,\n",
      " -3.8469257e-03, -1.2268883e-03, -6.1292183e-03, -1.4103031e-03,\n",
      " -2.0343377e-03, 3.7925844e-03}\n",
      "{-4.46952060e-02, -7.62224495e-02, -4.11147997e-02, -4.37799990e-02,\n",
      " -2.58016400e-03, -1.34616184e+00, 8.46833573e-04, -3.14451829e-02,\n",
      " -2.29339051e+00, -1.39832079e-01, -1.53545916e-01, -5.94730861e-02,\n",
      " -1.53841645e-01, -2.45974716e-02, -4.37009245e-01, 1.21843863e-04,\n",
      " -3.82549986e-02, -3.45450282e-01, -1.18358560e-01, -1.27363041e-01,\n",
      " -9.71442014e-02, -1.19326316e-01, -4.74552624e-03, -1.02477860e+00,\n",
      " -8.66305735e-03, -1.05527183e-02, 2.56578941e-02, -1.09880641e-01,\n",
      " -1.29224181e-01, -9.75079387e-02, -1.06837243e-01, -8.21980275e-03,\n",
      " -3.15310627e-01, -1.24218315e-01, -1.74983479e-02, 1.66940764e-01,\n",
      " -1.19057409e-01, -1.38180628e-01, -1.09932669e-01, -1.15156397e-01,\n",
      " -7.13718915e-03, -4.87495661e-01, -2.09103674e-01, -1.35120312e-02,\n",
      " -2.24310786e-01, -1.27475038e-01, -1.80382997e-01, -1.18021220e-01,\n",
      " -1.14525542e-01, -1.66988261e-02, -6.15241349e-01, -9.38779563e-02,\n",
      " -3.48436125e-02, 1.04258303e-02}\n",
      "{ 2.75103142e-04, 4.29885753e-04, 1.03189384e-04, 3.10002593e-04,\n",
      " 1.13015092e-04, 8.54112091e-04, -1.23286903e-09, 1.76270492e-04,\n",
      " -6.79876423e-04, 1.94462191e-04, 3.84817715e-04, 5.52596684e-05,\n",
      " 1.62037351e-04, 1.05760599e-04, 8.54104524e-04, 5.59969085e-11,\n",
      " 2.32791615e-04, -4.87872574e-04, 2.05840712e-04, 3.22908047e-04,\n",
      " 1.16484822e-04, 1.95399756e-04, 5.37860797e-05, 8.51762597e-04,\n",
      " -9.76251044e-08, 1.11902074e-04, -2.10297920e-04, 1.85184166e-04,\n",
      " 3.42258281e-04, 1.03046455e-04, 1.80221963e-04, 3.98386983e-05,\n",
      " 1.72055874e-03, 1.08239066e-04, 9.21160245e-05, 1.73463312e-03,\n",
      " 1.97325557e-04, 3.54121119e-04, 4.38428397e-05, 2.16071639e-04,\n",
      " 7.43021665e-05, 1.35243149e-03, 1.58948431e-04, 1.61293530e-04,\n",
      " -1.70171290e-04, 1.93976855e-04, 3.92969174e-04, 4.90067432e-05,\n",
      " 1.99034504e-04, 1.14186616e-04, 4.85436118e-04, 3.39795406e-05,\n",
      " 2.52154190e-04, -6.12686155e-04}\n",
      "{ 1.8435620e-02, 3.1288773e-02, 1.4539189e-02, 1.7846501e-02,\n",
      " 2.9586004e-03, 4.5787689e-01, -3.3862938e-04, 1.4348400e-02,\n",
      " 7.0733857e-01, 4.7153234e-02, 5.6392569e-02, 1.9140210e-02,\n",
      " 5.0969902e-02, 1.0025428e-02, 1.5820263e-01, -5.6064314e-06,\n",
      " 1.8915528e-02, 1.0896311e-01, 4.1988507e-02, 4.7240105e-02,\n",
      " 3.3995286e-02, 4.2093758e-02, 2.5330861e-03, 3.5074925e-01,\n",
      " 3.3493317e-03, 5.4384363e-03, -8.1764758e-03, 3.7861329e-02,\n",
      " 4.5461025e-02, 3.1378273e-02, 3.7064638e-02, 3.7436453e-03,\n",
      " 1.6049302e-01, 4.1282378e-02, 8.8021867e-03, -5.7671204e-02,\n",
      " 4.0514283e-02, 4.8965685e-02, 3.6549680e-02, 3.9588921e-02,\n",
      " 3.0079649e-03, 1.8098195e-01, 6.9638245e-02, 6.6289669e-03,\n",
      " 7.0854321e-02, 4.5465749e-02, 6.4185455e-02, 3.7820764e-02,\n",
      " 4.2334765e-02, 7.6586325e-03, 1.8834589e-01, 3.2446355e-02,\n",
      " 1.5118929e-02, -1.2828915e-02}\n",
      "{-6.13762951e-03, -1.09598385e-02, -2.04140530e-03, -5.62564190e-03,\n",
      " -2.98205949e-03, -4.89710681e-02, 2.09306105e-04, -7.04623433e-03,\n",
      " 1.27576850e-02, -5.67212841e-03, -1.30102141e-02, -7.54243927e-04,\n",
      " -4.84500406e-03, -3.49598704e-03, -4.51705121e-02, -7.18042502e-05,\n",
      " -9.63634811e-03, 3.73274740e-03, -7.42803700e-03, -1.07920645e-02,\n",
      " -4.51249303e-03, -7.33669335e-03, -1.69557927e-03, -4.81898375e-02,\n",
      " -6.80922298e-04, -3.48944310e-03, 2.87000486e-03, -5.52789727e-03,\n",
      " -9.37942509e-03, -1.55841315e-03, -5.60370181e-03, -1.99250667e-03,\n",
      " -8.10925290e-02, -2.91750324e-03, -5.45630651e-03, 1.83242373e-05,\n",
      " -5.57718426e-03, -1.01043815e-02, -2.42105965e-03, -6.01379899e-03,\n",
      " -1.82982709e-03, -3.19037586e-02, -4.64937277e-03, -4.79315082e-03,\n",
      " -2.64591596e-04, -8.36687814e-03, -1.25482045e-02, -2.04046723e-03,\n",
      " -9.28553473e-03, -3.45167774e-03, -4.93004336e-04, -2.71462323e-03,\n",
      " -6.23165490e-03, 1.43172704e-02}\n",
      "{-2.6524438e-02, -4.7574442e-02, -1.1083956e-02, -2.3638992e-02,\n",
      " -1.3739701e-02, -1.6725309e-01, -2.1842388e-06, -2.7598776e-02,\n",
      " 9.3409002e-02, -1.8138269e-02, -4.4395551e-02, -4.0211724e-03,\n",
      " -1.5144568e-02, -1.2139999e-02, -1.1563781e-01, 8.0870919e-08,\n",
      " -3.5289340e-02, 1.2488831e-02, -2.6875526e-02, -4.0931880e-02,\n",
      " -1.8773844e-02, -2.5866052e-02, -5.8098510e-03, -1.6178472e-01,\n",
      " -2.1634910e-03, -1.1557741e-02, -3.2048118e-03, -1.8967347e-02,\n",
      " -2.5855653e-02, -6.6534793e-03, -1.9606251e-02, -5.2867797e-03,\n",
      " -3.3829719e-01, -1.5170173e-02, -1.4771206e-02, 8.1323376e-03,\n",
      " -1.8417126e-02, -2.9971492e-02, -1.2336899e-02, -1.9925410e-02,\n",
      " -3.9740480e-03, -1.5428393e-01, -2.1379849e-02, -1.1772475e-02,\n",
      " 2.7366426e-02, -2.9738579e-02, -4.3384485e-02, -6.4390125e-03,\n",
      " -3.3608254e-02, -1.3152480e-02, 1.9586785e-03, -1.5455711e-02,\n",
      " -2.3071591e-02, 6.6231392e-02}\n",
      "{-9.5246296e-06, -2.0779607e-05, -7.0728011e-06, -8.2960178e-06,\n",
      " -3.3985543e-06, -1.2490143e-04, -4.7087274e-06, -1.4296120e-05,\n",
      " -1.8317428e-04, -2.3144668e-05, -2.6340542e-05, -1.3257811e-05,\n",
      " -2.4800565e-05, -3.6772699e-06, -1.6098621e-04, -1.7127033e-17,\n",
      " -6.4056926e-06, -2.2818047e-05, -2.1721014e-05, -2.7350792e-05,\n",
      " -1.2550395e-05, -2.1140513e-05, -2.9513299e-06, -1.6004169e-04,\n",
      " -7.8930043e-08, -5.4673897e-06, -5.8103535e-05, -1.6829677e-05,\n",
      " -1.9846115e-05, -1.4799884e-05, -1.6779644e-05, -1.1427819e-06,\n",
      " -1.1669607e-04, -2.5541201e-05, -2.7512510e-06, -2.9820296e-05,\n",
      " -1.7448239e-05, -2.4132794e-05, -1.4608405e-05, -1.7268349e-05,\n",
      " -1.6292117e-06, -4.4819817e-08, -3.1181295e-05, -3.7594436e-06,\n",
      " -3.1727535e-04, -1.8020506e-05, -2.0726635e-05, -1.3579623e-05,\n",
      " -1.8340792e-05, -1.9866131e-06, -3.9321832e-05, -2.1010490e-05,\n",
      " -4.4385461e-06, -9.7265929e-06}\n",
      "{ 3.9069156e-10, 7.5009482e-10, 1.5663744e-10, 3.4396286e-10,\n",
      " 1.9334714e-10, 8.0335233e-10, 0.0000000e+00, 3.9738643e-10,\n",
      " -1.1635671e-09, 2.8353836e-10, 6.6182237e-10, 4.2699511e-11,\n",
      " 2.4894944e-10, 1.7374649e-10, 1.6059489e-09, 7.1615147e-19,\n",
      " 4.9205884e-10, -4.8317972e-10, 4.2038092e-10, 5.9233157e-10,\n",
      " 2.5309085e-10, 4.1814754e-10, 9.3835606e-11, 3.2115199e-09,\n",
      " 7.1615147e-19, 1.7923507e-10, -6.9182382e-10, 4.1182682e-10,\n",
      " 5.4646004e-10, 3.0340147e-10, 4.1990703e-10, 5.4264714e-11,\n",
      " 2.4089235e-09, 3.5328229e-10, 1.0580441e-10, -3.4352639e-11,\n",
      " 4.4481036e-10, 5.4174309e-10, 1.6227979e-10, 4.6471321e-10,\n",
      " 7.5135398e-11, 1.6059489e-09, 3.8543907e-10, 1.3563374e-10,\n",
      " 3.6685264e-09, 3.1749811e-10, 6.3507016e-10, 7.6541808e-11,\n",
      " 2.8199457e-10, 1.9173541e-10, 1.7903788e-17, 3.2190771e-11,\n",
      " 4.8490650e-10, -1.1606234e-09}\n",
      "{-6.50862232e-02, -1.12573996e-01, -5.34929298e-02, -6.21446110e-02,\n",
      " -9.89756361e-03, -1.65993047e+00, 9.76905460e-04, -5.08593246e-02,\n",
      " -2.63267851e+00, -1.72072053e-01, -2.01396435e-01, -7.13512748e-02,\n",
      " -1.86960399e-01, -3.47406007e-02, -5.68750799e-01, 1.44911217e-04,\n",
      " -6.27018213e-02, -4.00802493e-01, -1.51785105e-01, -1.69323832e-01,\n",
      " -1.23143956e-01, -1.52418196e-01, -8.39115772e-03, -1.28205836e+00,\n",
      " -1.13227144e-02, -1.79516394e-02, 2.56941523e-02, -1.37607515e-01,\n",
      " -1.62885070e-01, -1.16915450e-01, -1.34436533e-01, -1.22404490e-02,\n",
      " -5.45794725e-01, -1.52847812e-01, -2.79564187e-02, 2.11222753e-01,\n",
      " -1.47913024e-01, -1.75473899e-01, -1.35057613e-01, -1.44082963e-01,\n",
      " -9.95358266e-03, -6.46828175e-01, -2.55263001e-01, -2.10511908e-02,\n",
      " -2.48428106e-01, -1.64155751e-01, -2.32102185e-01, -1.41167000e-01,\n",
      " -1.51176333e-01, -2.60071997e-02, -7.14720666e-01, -1.18141785e-01,\n",
      " -5.16652465e-02, 4.39658090e-02}\n",
      "{ 2.54527647e-02, 4.29012701e-02, 1.99571196e-02, 2.46688407e-02,\n",
      " 4.19018464e-03, 6.07914209e-01, -5.38115346e-05, 1.94854233e-02,\n",
      " 9.42535877e-01, 6.39306977e-02, 7.65710846e-02, 2.74905898e-02,\n",
      " 6.86875880e-02, 1.31663112e-02, 2.34312296e-01, 1.22433903e-07,\n",
      " 2.48400234e-02, 1.42437056e-01, 5.77040762e-02, 6.56051114e-02,\n",
      " 4.72439416e-02, 5.77466376e-02, 3.63753154e-03, 4.84825313e-01,\n",
      " 4.92999703e-03, 7.65472883e-03, 1.07260188e-02, 5.11517823e-02,\n",
      " 6.14563152e-02, 4.21992280e-02, 5.01312613e-02, 5.00915246e-03,\n",
      " 2.39316404e-01, 5.58605567e-02, 1.17867636e-02, -6.45095259e-02,\n",
      " 5.45600690e-02, 6.55554608e-02, 4.87248376e-02, 5.35185188e-02,\n",
      " 4.08420665e-03, 2.44718790e-01, 9.48915258e-02, 9.05429665e-03,\n",
      " 1.08818635e-01, 6.08857758e-02, 8.46640617e-02, 5.02707548e-02,\n",
      " 5.71059659e-02, 9.97266546e-03, 2.43531778e-01, 4.61004563e-02,\n",
      " 1.98390502e-02, 6.70833839e-03}\n",
      "{-1.92390159e-02, -3.51330414e-02, -8.13340023e-03, -1.65033601e-02,\n",
      " -1.02660451e-02, -1.29561156e-01, -1.85945237e-06, -2.10810639e-02,\n",
      " 7.08417743e-02, -1.30093656e-02, -3.28633636e-02, -2.75555812e-03,\n",
      " -1.08119696e-02, -8.98889266e-03, -8.69408250e-02, 8.21592394e-09,\n",
      " -2.69070994e-02, 5.30927721e-03, -2.01411191e-02, -3.06233540e-02,\n",
      " -1.43336793e-02, -1.94057319e-02, -4.27430822e-03, -1.24924272e-01,\n",
      " -1.78484537e-03, -8.45536869e-03, -4.95337835e-03, -1.38168726e-02,\n",
      " -1.79521963e-02, -4.45640786e-03, -1.43968612e-02, -3.96957574e-03,\n",
      " -2.63269663e-01, -1.14235841e-02, -1.12842089e-02, 2.31783055e-02,\n",
      " -1.32287517e-02, -2.12067626e-02, -9.74379480e-03, -1.43165765e-02,\n",
      " -2.53142137e-03, -1.14947960e-01, -1.61581319e-02, -8.10031034e-03,\n",
      " 2.09096149e-02, -2.26707626e-02, -3.19434628e-02, -4.81674587e-03,\n",
      " -2.58724447e-02, -9.74609889e-03, 5.97480312e-03, -1.24412999e-02,\n",
      " -1.65320393e-02, 4.87254262e-02}\n",
      "{ 1.3268837e-02, 2.4112612e-02, 5.6206724e-03, 1.1492081e-02,\n",
      " 7.0073893e-03, 8.8939317e-02, 1.1967760e-06, 1.4364841e-02,\n",
      " -4.7950272e-02, 9.1260793e-03, 2.2703787e-02, 2.0596685e-03,\n",
      " 7.6159667e-03, 6.1715194e-03, 5.9636064e-02, 7.8563189e-06,\n",
      " 1.8356180e-02, -4.2405715e-03, 1.3911836e-02, 2.1150537e-02,\n",
      " 9.8683620e-03, 1.3407559e-02, 2.9554553e-03, 8.5239820e-02,\n",
      " 1.2141884e-03, 5.8494112e-03, 3.9050626e-03, 9.6142124e-03,\n",
      " 1.2580717e-02, 3.2187663e-03, 1.0000450e-02, 2.7220717e-03,\n",
      " 1.7889242e-01, 7.8968313e-03, 7.6982477e-03, -1.4146424e-02,\n",
      " 9.2355218e-03, 1.4803393e-02, 6.6861194e-03, 9.9910358e-03,\n",
      " 1.8037376e-03, 7.9518363e-02, 1.1203282e-02, 5.6546042e-03,\n",
      " -1.4094615e-02, 1.5575422e-02, 2.2051431e-02, 3.4047265e-03,\n",
      " 1.7707791e-02, 6.6941716e-03, -3.2167353e-03, 8.5863927e-03,\n",
      " 1.1433056e-02, -3.3806428e-02}\n",
      "{-2.5655515e-02, -4.6570230e-02, -1.0771277e-02, -2.2244973e-02,\n",
      " -1.3575583e-02, -1.7042534e-01, -2.3384962e-06, -2.7661920e-02,\n",
      " 9.2794888e-02, -1.7496372e-02, -4.3546785e-02, -3.7965064e-03,\n",
      " -1.4672259e-02, -1.1885723e-02, -1.1472371e-01, 5.9021531e-08,\n",
      " -3.5156891e-02, 8.9547858e-03, -2.6521407e-02, -4.0305149e-02,\n",
      " -1.8741053e-02, -2.5592718e-02, -5.6341803e-03, -1.6460028e-01,\n",
      " -2.3349456e-03, -1.1190683e-02, -5.8375821e-03, -1.8416373e-02,\n",
      " -2.4182959e-02, -6.2272581e-03, -1.9131225e-02, -5.2084988e-03,\n",
      " -3.4169373e-01, -1.5176688e-02, -1.4727528e-02, 2.7537189e-02,\n",
      " -1.7698480e-02, -2.8688226e-02, -1.2680901e-02, -1.9097293e-02,\n",
      " -3.5529551e-03, -1.5004620e-01, -2.1098532e-02, -1.1068591e-02,\n",
      " 2.6190685e-02, -2.9642420e-02, -4.2330664e-02, -6.3699242e-03,\n",
      " -3.3659860e-02, -1.2890239e-02, 5.3277747e-03, -1.5880892e-02,\n",
      " -2.2242809e-02, 6.4777724e-02}\n",
      "{-2.20774243e-09, -3.87192589e-09, -1.22003008e-09, -1.97390104e-09,\n",
      " -5.74573555e-10, -2.72442424e-08, 3.13310012e-19, -2.52975729e-09,\n",
      " -1.27426958e-09, -3.56967811e-09, -5.95780714e-09, -1.28708066e-09,\n",
      " -3.81072640e-09, -1.44232826e-09, -1.00650324e-08, 0.00000000e+00,\n",
      " -3.63163877e-09, 1.27123805e-08, -5.09468467e-09, -6.50191856e-09,\n",
      " -3.58292751e-09, -5.14776488e-09, -8.92365681e-10, -1.64346297e-08,\n",
      " -4.92847187e-11, -1.57247892e-09, 1.21180319e-08, -3.98829902e-09,\n",
      " -5.23909671e-09, -2.50439669e-09, -4.14412638e-09, -7.32527816e-10,\n",
      " -9.30608834e-09, -2.45318388e-09, -1.64548952e-09, 7.80138798e-09,\n",
      " -4.29228386e-09, -7.25258564e-09, -1.86904714e-09, -3.85387988e-09,\n",
      " -1.50287138e-09, -9.30536270e-09, -1.81274074e-09, -3.14541859e-09,\n",
      " 7.90621968e-09, -3.72540354e-09, -6.19860252e-09, -1.77590054e-09,\n",
      " -3.37447004e-09, -1.11806631e-09, -1.90363370e-08, -1.66234138e-09,\n",
      " -2.71402367e-09, 3.11534021e-09}\n",
      "{-1.9117920e-02, -3.4821603e-02, -7.4827778e-03, -1.6700331e-02,\n",
      " -1.0004111e-02, -1.3476139e-01, 2.7920934e-04, -2.1451846e-02,\n",
      " 6.2466089e-02, -1.4034271e-02, -3.4877647e-02, -2.3444630e-03,\n",
      " -1.1672676e-02, -9.5620202e-03, -1.0164297e-01, 1.2541150e-05,\n",
      " -2.7956247e-02, 8.2018841e-03, -2.0792130e-02, -3.1267092e-02,\n",
      " -1.4019544e-02, -2.0179637e-02, -4.5696674e-03, -1.3082512e-01,\n",
      " -1.8391781e-03, -9.1754235e-03, -1.0103617e-03, -1.4666428e-02,\n",
      " -2.1268498e-02, -4.4073928e-03, -1.5134195e-02, -4.6704728e-03,\n",
      " -2.5932553e-01, -1.0394618e-02, -1.3082388e-02, 1.6851762e-02,\n",
      " -1.4266438e-02, -2.4159538e-02, -8.9073116e-03, -1.5436330e-02,\n",
      " -3.4953256e-03, -1.0940704e-01, -1.5164163e-02, -1.0192154e-02,\n",
      " 1.7013103e-02, -2.3471011e-02, -3.3785224e-02, -4.9562207e-03,\n",
      " -2.6643042e-02, -1.0060136e-02, 4.4340035e-03, -1.0938390e-02,\n",
      " -1.7412964e-02, 4.9128544e-02}\n",
      "{ 2.4029309e-18, 4.0484353e-18, 9.0005926e-19, 2.5774516e-18,\n",
      " 1.1372809e-18, 9.6290368e-19, 0.0000000e+00, 2.0414429e-18,\n",
      " -6.7188829e-18, 1.5123029e-18, 3.1969122e-18, 4.0590906e-19,\n",
      " 1.4617912e-18, 8.1540187e-19, 4.3025158e-18, 1.3358447e-19,\n",
      " 2.3025966e-18, -2.5883619e-18, 2.0724537e-18, 3.1250648e-18,\n",
      " 1.1859305e-18, 2.0753664e-18, 5.4939418e-19, 8.6050317e-18,\n",
      " 1.3358447e-19, 1.1325780e-18, -5.1899227e-18, 1.9301126e-18,\n",
      " 2.6932829e-18, 1.1175690e-18, 1.8851719e-18, 3.8416297e-19,\n",
      " 1.9586772e-17, 1.1571702e-18, 8.5235012e-19, -2.3412156e-18,\n",
      " 2.3244660e-18, 2.9084826e-18, 1.0397737e-18, 2.4535021e-18,\n",
      " 4.4522180e-19, 1.0530839e-17, 1.9611212e-18, 8.2430626e-19,\n",
      " 4.3275989e-18, 1.9592679e-18, 3.6398358e-18, 3.4912249e-19,\n",
      " 1.8079877e-18, 1.1730287e-18, 4.3025158e-18, 2.1061677e-19,\n",
      " 2.5939143e-18, -6.5711617e-18}\n",
      "state \t torch.Size([50])\n",
      "-0.30330232\n",
      "-2.154819e-20\n",
      "0.0025626346\n",
      "0.11954683\n",
      "0.1420719\n",
      "-0.002867165\n",
      "-0.0028202573\n",
      "0.10715435\n",
      "-0.07813169\n",
      "-0.02143421\n",
      "-0.010006989\n",
      "0.21330902\n",
      "-0.02054907\n",
      "-0.00066646974\n",
      "0.3196901\n",
      "-8.045951e-14\n",
      "0.0\n",
      "-0.009808494\n",
      "0.19648145\n",
      "-0.027005881\n",
      "0.045564897\n",
      "0.00030840866\n",
      "0.0\n",
      "0.06974094\n",
      "0.17554091\n",
      "-0.06716405\n",
      "-4.001624e-10\n",
      "-0.33561188\n",
      "2.4568771e-12\n",
      "7.165007e-12\n",
      "-0.15524998\n",
      "-0.04072957\n",
      "-0.0034018243\n",
      "0.3375062\n",
      "-0.0062994515\n",
      "-0.2527269\n",
      "0.00042824753\n",
      "0.089270644\n",
      "-0.015567585\n",
      "-0.05353444\n",
      "-4.026625e-05\n",
      "8.0297446e-10\n",
      "-0.32199386\n",
      "0.1207651\n",
      "-0.03994685\n",
      "0.02758812\n",
      "-0.052796464\n",
      "-8.982033e-09\n",
      "-0.042087052\n",
      "4.3025154e-18\n",
      "state \t torch.Size([12, 50])\n",
      "{-2.84286402e-02, 0.00000000e+00, 0.00000000e+00, -5.43798804e-02,\n",
      " 5.03760576e-03, -1.67692378e-02, -1.24402337e-01, -7.76379928e-02,\n",
      " 6.16951802e-05, -2.00183708e-02, -2.90465951e-02, -3.30939814e-02,\n",
      " -6.21011527e-03, 2.36826716e-03, 6.91950554e-04, 0.00000000e+00,\n",
      " 0.00000000e+00, 5.29631041e-04, -1.06101185e-01, -9.07397717e-02,\n",
      " -1.61490619e-01, -1.27444408e-04, 0.00000000e+00, -8.31446722e-02,\n",
      " 4.03989304e-07, -1.67059094e-01, 0.00000000e+00, 6.58664107e-03,\n",
      " -2.86465860e-12, -5.88261702e-24, -8.74342993e-02, -1.91103756e-01,\n",
      " 1.61949322e-02, -3.11079640e-02, -7.12495446e-02, -1.35314703e-01,\n",
      " 3.12178832e-04, -3.97005755e-20, -3.01309247e-02, -1.68031845e-02,\n",
      " 0.00000000e+00, -5.70099856e-10, -1.00006677e-01, 3.13737866e-04,\n",
      " -2.43907124e-02, -2.09325310e-02, -3.56818736e-03, 1.02671178e-08,\n",
      " -1.12234559e-02, -7.61518654e-19}\n",
      "{-6.1714118e-03, 0.0000000e+00, 1.5107011e-04, -1.4094867e-02,\n",
      " 1.2674304e-03, -1.0695183e-02, -4.9586456e-02, -1.8610576e-02,\n",
      " 6.1626015e-03, -1.0633260e-02, -1.2352335e-02, -1.1166649e-02,\n",
      " -1.7197712e-03, 3.4373984e-06, 3.5609590e-04, -4.9655287e-11,\n",
      " 0.0000000e+00, -4.0876465e-03, -3.5371739e-02, -2.7922487e-02,\n",
      " -3.9390270e-02, -5.6523897e-10, 0.0000000e+00, -2.6077477e-02,\n",
      " 5.0313901e-03, -5.8546413e-02, 1.4493412e-11, -1.4680020e-03,\n",
      " 0.0000000e+00, 2.4768317e-14, -2.3851668e-02, -6.7098565e-02,\n",
      " 2.2984055e-04, -5.5155503e-03, -2.6779095e-02, -4.3729171e-02,\n",
      " 8.7000132e-08, 3.6916379e-03, -1.1241768e-02, -5.0989930e-03,\n",
      " 6.6657972e-06, 0.0000000e+00, -3.1736318e-02, 9.6661341e-04,\n",
      " -1.0053198e-02, -1.1214684e-02, -3.1940911e-03, 3.9673136e-09,\n",
      " -5.5914694e-03, 0.0000000e+00}\n",
      "{ 1.11396365e-01, 0.00000000e+00, 3.02496627e-02, 4.32894588e-01,\n",
      " 4.56336699e-03, 5.39538392e-04, -7.26336657e-05, 4.75129634e-01,\n",
      " 5.89078963e-01, -4.43470664e-03, 1.54522527e-03, 3.02443765e-02,\n",
      " -1.04515173e-03, -3.49466391e-05, 2.77264547e-02, 5.41753252e-15,\n",
      " 0.00000000e+00, 1.44846621e-04, 7.65078440e-02, 1.41601309e-01,\n",
      " 9.61984038e-01, -1.87647387e-09, 0.00000000e+00, 1.46103323e-01,\n",
      " 5.88782609e-01, 1.04763824e-03, -7.61438777e-14, 2.80749798e-01,\n",
      " 0.00000000e+00, -4.46005917e-13, 3.02788794e-01, -1.59847992e-03,\n",
      " -4.15154779e-03, 5.08809447e-01, 1.04246056e-03, -7.82128423e-04,\n",
      " -2.38818188e-06, 3.29217970e-01, -6.45933533e-03, -3.51649040e-04,\n",
      " -1.75696579e-04, 0.00000000e+00, 3.21257770e-01, 1.75769433e-01,\n",
      " -1.56335431e-04, -1.66033700e-04, -2.38889013e-04, -1.06579732e-08,\n",
      " 4.02854639e-05, 0.00000000e+00}\n",
      "{-2.51047648e-02, -1.22698956e-20, -9.56560951e-03, -1.16784744e-01,\n",
      " -4.89841821e-03, 6.24366803e-03, 4.96176742e-02, -1.23684049e-01,\n",
      " -1.89906433e-01, 9.05031431e-03, 1.10948030e-02, 1.19390897e-03,\n",
      " 2.76634702e-03, -9.46742308e-04, -9.30217654e-03, -2.72135634e-12,\n",
      " 0.00000000e+00, -4.07364219e-04, 1.36150382e-02, -1.27432942e-02,\n",
      " -2.52275378e-01, 5.10642349e-05, 0.00000000e+00, -1.43392459e-02,\n",
      " -1.88580990e-01, 6.63868636e-02, -9.63134572e-11, -9.40226391e-02,\n",
      " 0.00000000e+00, 1.10094889e-13, -6.57508820e-02, 7.66951218e-02,\n",
      " -5.56484750e-03, -1.51343256e-01, 2.81811580e-02, 5.07128984e-02,\n",
      " -1.26360828e-04, -1.06405161e-01, 1.34607702e-02, 6.86675543e-03,\n",
      " 4.76426103e-05, 2.28135733e-10, -6.45191967e-02, -5.58182038e-02,\n",
      " 9.83712450e-03, 8.32360797e-03, 1.52425724e-03, -3.27436468e-16,\n",
      " 4.41980269e-03, 0.00000000e+00}\n",
      "{-7.76794693e-03, 0.00000000e+00, 0.00000000e+00, -1.48638804e-02,\n",
      " 1.38738926e-03, -4.57358640e-03, -3.40249129e-02, -2.12288089e-02,\n",
      " 1.88984995e-05, -5.47286263e-03, -7.94197246e-03, -9.04098339e-03,\n",
      " -1.69887219e-03, 6.49273279e-04, 2.15149048e-04, 1.71079915e-12,\n",
      " 0.00000000e+00, 1.50224892e-04, -2.90065575e-02, -2.48151086e-02,\n",
      " -4.31130007e-02, -3.48721951e-05, 0.00000000e+00, -2.27379799e-02,\n",
      " 1.10205264e-07, -4.56860363e-02, 0.00000000e+00, 1.81801314e-03,\n",
      " -8.17714874e-13, -1.72452942e-24, -2.39095781e-02, -5.22709787e-02,\n",
      " 4.44692047e-03, -8.49418249e-03, -1.94886569e-02, -3.70077789e-02,\n",
      " 8.62120069e-05, -1.08741276e-20, -8.22370686e-03, -4.58518881e-03,\n",
      " 0.00000000e+00, -1.55974969e-10, -2.73442380e-02, 9.22532563e-05,\n",
      " -6.66676369e-03, -5.72205102e-03, -9.67519358e-04, 3.22490701e-09,\n",
      " -3.04740155e-03, -2.08950153e-19}\n",
      "{-9.3817040e-02, -3.8362462e-20, -2.6792895e-02, -3.7399301e-01,\n",
      " -4.9413461e-03, 2.4424167e-03, 2.2433687e-02, -4.0683773e-01,\n",
      " -5.2164662e-01, 7.5854822e-03, 3.8407117e-03, -2.0748932e-02,\n",
      " 2.0256999e-03, -3.8843427e-04, -2.5157699e-02, 2.9590708e-11,\n",
      " 0.0000000e+00, -2.0437676e-04, -4.8570342e-02, -1.0887991e-01,\n",
      " -8.2333338e-01, 2.2711492e-05, 0.0000000e+00, -1.1461533e-01,\n",
      " -5.2152741e-01, 2.8832771e-02, -2.1729958e-10, -2.4980238e-01,\n",
      " -2.9930263e-13, 3.9448897e-13, -2.5240433e-01, 3.5605010e-02,\n",
      " 7.2111562e-04, -4.4520825e-01, 1.1903211e-02, 2.5021195e-02,\n",
      " -5.5337125e-05, -2.9153186e-01, 1.0760408e-02, 3.1294636e-03,\n",
      " 1.5446209e-04, 9.9182822e-11, -2.6664674e-01, -1.5588583e-01,\n",
      " 4.4513559e-03, 3.9128270e-03, 7.4509846e-04, 6.5214245e-12,\n",
      " 1.6288490e-03, 1.2323297e-19}\n",
      "{ 8.9766867e-02, 3.4235047e-20, 2.4315964e-02, 3.4829983e-01,\n",
      " 3.8104914e-03, 4.8582850e-04, -5.7918394e-05, 3.8245830e-01,\n",
      " 4.7396237e-01, -3.5281796e-03, 1.2334840e-03, 2.4531245e-02,\n",
      " -8.3850720e-04, -2.7938973e-05, 2.2418678e-02, 4.3480060e-15,\n",
      " 0.0000000e+00, 1.3881759e-04, 6.1949715e-02, 1.1425013e-01,\n",
      " 7.7452254e-01, -1.6557763e-09, 0.0000000e+00, 1.1765341e-01,\n",
      " 4.7361594e-01, 8.7797991e-04, 1.9547262e-10, 2.2600415e-01,\n",
      " 0.0000000e+00, -3.5323919e-13, 2.4395032e-01, -1.2566047e-03,\n",
      " -3.3043583e-03, 4.0938348e-01, 8.3020912e-04, -2.3382157e-04,\n",
      " -1.8184260e-06, 2.6491019e-01, -5.0347876e-03, -2.5337734e-04,\n",
      " -1.4064289e-04, 0.0000000e+00, 2.5861794e-01, 1.4133231e-01,\n",
      " -1.1131835e-04, -1.3265804e-04, -1.7105279e-04, -7.7709483e-09,\n",
      " 1.0322826e-04, 0.0000000e+00}\n",
      "{4.81243114e-05, 0.00000000e+00, 0.00000000e+00, 8.04669617e-05,\n",
      " 2.90904227e-05, 3.56152195e-05, 2.18854587e-11, 5.65272385e-05,\n",
      " 2.86499235e-05, 4.16796956e-06, 2.48219908e-06, 3.82194121e-05,\n",
      " 0.00000000e+00, 0.00000000e+00, 1.09031775e-04, 0.00000000e+00,\n",
      " 0.00000000e+00, 7.71056066e-11, 3.89885245e-05, 5.56036093e-06,\n",
      " 3.57840509e-05, 0.00000000e+00, 0.00000000e+00, 4.46080012e-05,\n",
      " 1.82153217e-05, 5.18352826e-05, 0.00000000e+00, 3.17087433e-05,\n",
      " 0.00000000e+00, 0.00000000e+00, 1.87907826e-05, 1.79759900e-05,\n",
      " 2.98617380e-08, 8.83105895e-05, 2.24076270e-12, 1.97872282e-07,\n",
      " 2.65315983e-08, 1.23246609e-05, 9.60023317e-05, 4.71411877e-05,\n",
      " 0.00000000e+00, 0.00000000e+00, 2.42024944e-05, 3.35989134e-05,\n",
      " 2.45759002e-05, 0.00000000e+00, 3.47930545e-05, 1.61274716e-09,\n",
      " 9.07462236e-05, 0.00000000e+00}\n",
      "{ 3.05177865e-17, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      " 1.26702400e-13, 5.43293607e-14, 0.00000000e+00, 8.04620517e-14,\n",
      " 2.76120628e-15, 3.28042545e-14, -1.44734718e-18, 1.67807283e-13,\n",
      " 7.27620817e-14, 0.00000000e+00, 1.46992245e-16, 0.00000000e+00,\n",
      " 0.00000000e+00, 3.41166321e-15, 3.02743941e-13, 1.95059152e-13,\n",
      " 3.73825401e-13, 0.00000000e+00, 0.00000000e+00, -4.17034566e-36,\n",
      " 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.53102846e-16,\n",
      " 0.00000000e+00, 0.00000000e+00, 1.79407609e-13, 0.00000000e+00,\n",
      " 3.00663764e-13, 7.84566504e-14, 0.00000000e+00, 2.13816242e-13,\n",
      " 3.21726068e-15, 0.00000000e+00, 1.00563742e-16, 0.00000000e+00,\n",
      " 0.00000000e+00, 0.00000000e+00, 2.43601221e-14, 8.73418701e-15,\n",
      " 0.00000000e+00, 1.32855305e-14, 0.00000000e+00, 0.00000000e+00,\n",
      " 0.00000000e+00, 0.00000000e+00}\n",
      "{-1.9963946e-04, 0.0000000e+00, 0.0000000e+00, 8.0602092e-04,\n",
      " 5.8202585e-04, 2.5335543e-03, 5.9376261e-03, 1.0899887e-03,\n",
      " -4.0274749e-06, 2.1895492e-03, 1.7061494e-03, 1.6534589e-03,\n",
      " -1.0817667e-05, 2.9102495e-04, -2.6164598e-05, 8.8319234e-13,\n",
      " 0.0000000e+00, 1.8439104e-03, 3.9034397e-03, 2.6696543e-03,\n",
      " 3.6960295e-03, -1.5611011e-05, 0.0000000e+00, 1.7095972e-03,\n",
      " 2.6032847e-08, 4.4540782e-03, 0.0000000e+00, 2.9389362e-03,\n",
      " -2.3411774e-13, -6.1721591e-25, 1.9234398e-03, 5.2390490e-03,\n",
      " 1.9824973e-03, 6.4318196e-04, 2.6801473e-03, 3.3692929e-03,\n",
      " 3.8175531e-05, -1.5486620e-15, 1.2108604e-03, 6.3592917e-05,\n",
      " -6.9085452e-08, -6.9902445e-11, 2.9948179e-03, 6.9223079e-06,\n",
      " 1.2709859e-03, 2.2431375e-03, 8.8879862e-04, -2.5589789e-10,\n",
      " 9.2957087e-04, -9.1509873e-20}\n",
      "{ 7.9243921e-02, 3.1744452e-20, 2.1834437e-02, 3.1088814e-01,\n",
      " 1.0872353e-03, -2.3602790e-03, -3.4677368e-03, 3.3855778e-01,\n",
      " 4.2208779e-01, -5.5208793e-03, -4.2654341e-05, 1.8192466e-02,\n",
      " -8.0701488e-04, -4.9490039e-04, 1.9328358e-02, 3.4158031e-11,\n",
      " 0.0000000e+00, -1.5242995e-03, 4.7603976e-02, 9.6106499e-02,\n",
      " 6.8211174e-01, -1.7284603e-09, 0.0000000e+00, 1.0339074e-01,\n",
      " 4.2299077e-01, -2.8065867e-03, 1.5737121e-10, 1.9732098e-01,\n",
      " 7.9049251e-37, -3.4642002e-13, 2.1231744e-01, -4.9584573e-03,\n",
      " -6.2141619e-03, 3.6445278e-01, -7.0171687e-04, -7.1615912e-03,\n",
      " -6.4206113e-05, 2.3565914e-01, -6.1238967e-03, -4.4935185e-04,\n",
      " -1.3407157e-04, 1.1286454e-10, 2.2726460e-01, 1.2670651e-01,\n",
      " -1.1200311e-03, -1.6946977e-03, -7.1025011e-04, -1.1754341e-08,\n",
      " -1.2009247e-03, 0.0000000e+00}\n",
      "{-8.6054489e-02, -3.6837287e-20, -2.6954155e-02, -3.5957560e-01,\n",
      " -8.2864836e-03, 7.9057105e-03, 6.4531773e-02, -3.8812044e-01,\n",
      " -5.2892768e-01, 1.4006861e-02, 1.3697219e-02, -1.2483582e-02,\n",
      " 4.0976913e-03, -1.1965071e-03, -2.5621194e-02, 8.6305713e-11,\n",
      " 0.0000000e+00, -4.9618585e-04, -1.8144071e-02, -8.3675057e-02,\n",
      " -7.8756851e-01, 6.6009270e-05, 0.0000000e+00, -8.8677011e-02,\n",
      " -5.2733052e-01, 8.5518420e-02, -2.3347699e-10, -2.5637439e-01,\n",
      " 6.3264883e-19, 3.6282547e-13, -2.2990726e-01, 1.0025761e-01,\n",
      " -5.0712582e-03, -4.4059265e-01, 3.6093201e-02, 6.6939086e-02,\n",
      " -1.6240374e-04, -2.9589194e-01, 2.0421820e-02, 8.9490358e-03,\n",
      " 1.4723286e-04, 2.9247380e-10, -2.3761252e-01, -1.5700439e-01,\n",
      " 1.2802508e-02, 1.0947932e-02, 2.0412321e-03, 2.2486875e-11,\n",
      " 5.5407220e-03, 3.7981301e-19}\n",
      "state \t torch.Size([12])\n",
      "-0.084949665\n",
      "-0.027354088\n",
      "0.23846872\n",
      "-0.04359024\n",
      "-0.023220167\n",
      "-0.19623213\n",
      "0.19199863\n",
      "8.0852566e-05\n",
      "7.2566315e-14\n",
      "0.0024573687\n",
      "0.16828194\n",
      "-0.1709648\n",
      "state \t torch.Size([3, 12])\n",
      "{ 2.78294496e-02, -3.53251189e-01, -1.17984414e-01, 2.11377367e-02,\n",
      " 1.15701221e-02, -2.51808465e-02, -1.69832200e-01, 5.04801051e-07,\n",
      " 2.55253260e-14, -5.88353141e-04, -2.20742315e-01, 1.11775264e-01}\n",
      "{-2.6500259e-02, 3.0071348e-02, 1.2619656e-02, -3.5321016e-02,\n",
      " -1.1451540e-02, -1.0867599e-01, 1.6961865e-02, 3.5954763e-06,\n",
      " 9.5214960e-17, 7.3743972e-04, 2.7538590e-02, -8.9996688e-02}\n",
      "{-4.53098473e-04, 3.23179811e-01, 1.05364785e-01, 1.41832903e-02,\n",
      " -1.18588832e-04, 1.33856803e-01, 1.52870342e-01, -4.10021539e-06,\n",
      " -2.75925252e-14, -1.49089858e-04, 1.93203747e-01, -2.17785574e-02}\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(D_in, 50, D_out)\n",
    "mlp_model.load('MLP_Model')\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in mlp_model.state_dict():\n",
    "    print(param_tensor, \"\\t\\t\", mlp_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "\n",
    "with open('weights.txt', 'w') as outfile:\n",
    "    for var_name in optimizer.state_dict():\n",
    "        if var_name == 'state':\n",
    "            for i in range(len(mlp_model.state_dict())):\n",
    "                print(var_name, \"\\t\", optimizer.state_dict()[var_name][i]['momentum_buffer'].shape)\n",
    "                # print(var_name, \"\\t\", np.array(optimizer.state_dict()[var_name][i]['momentum_buffer']))\n",
    "                y = np.array(optimizer.state_dict()[var_name][i]['momentum_buffer'])\n",
    "                for x in y:\n",
    "                    x = str(x)\n",
    "                    x = x.replace('[','{').replace(']','}').replace(' ', ', ').replace('{,', '{').replace(', ,', ',').replace('\\n,', ',\\n')\n",
    "                    print(x)\n",
    "                    outfile.write(x)\n",
    "                # np.savetxt(outfile, np.array(optimizer.state_dict()[var_name][i]['momentum_buffer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9333333333333333, 1: 0.03333333333333333, 2: 0.03333333333333333}\n",
      "{1: 0.9807692307692307, 0: 0.019230769230769232}\n",
      "{2: 0.9629629629629629, 0: 0.037037037037037035}\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(D_in, 50, D_out)\n",
    "mlp_model.load('MLP_Model')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(D_out):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical = df.copy()\n",
    "df_hierarchical['tag'] = df_hierarchical['tag'].apply(lambda x: 0 if x <= 7 else 1)\n",
    "\n",
    "msk = np.random.rand(len(df_hierarchical)) < 0.8\n",
    "df_train = df_hierarchical[msk]\n",
    "df_test = df_hierarchical[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical = MLP(D_in, 50, 2)\n",
    "optimizer = torch.optim.SGD(model_hierarchical.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical, criterion, optimizer, dataset.X, dataset.y, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 2)\n",
    "mlp_model.load('MLP_Model_Hierarchical_1')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(2):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical_2 = df.copy()\n",
    "df_hierarchical_2 = df_hierarchical_2[df_hierarchical_2['tag'] >= 8]\n",
    "df_hierarchical_2 = df_hierarchical_2.apply(lambda x: x-8)\n",
    "msk = np.random.rand(len(df_hierarchical_2)) < 0.8\n",
    "df_train = df_hierarchical_2[msk]\n",
    "df_test = df_hierarchical_2[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)\n",
    "\n",
    "model_hierarchical_2 = MLP(D_in, 50, 2)\n",
    "# optimizer = torch.optim.Adam(model_hierarchical_2.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(model_hierarchical_2.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical_2, criterion, optimizer, dataset.X, dataset.y, num_epochs=20)\n",
    "\n",
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 2)\n",
    "mlp_model.load('MLP_Model_Hierarchical_2')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(2):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x) + 8\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hierarchical_3 = df.copy()\n",
    "df_hierarchical_3 = df_hierarchical_3[df_hierarchical_3['tag'] < 8]\n",
    "msk = np.random.rand(len(df_hierarchical_3)) < 0.8\n",
    "df_train = df_hierarchical_3[msk]\n",
    "df_test = df_hierarchical_3[~msk]\n",
    "\n",
    "dataset = FeatureDataset(df_train)\n",
    "\n",
    "model_hierarchical_3 = MLP(D_in, 50, 8)\n",
    "# optimizer = torch.optim.Adam(model_hierarchical_2.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(model_hierarchical_3.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "model, losses, accuracies = train_val_model(model_hierarchical_3, criterion, optimizer, dataset.X, dataset.y, num_epochs=30)\n",
    "\n",
    "torch.save(model.state_dict(), 'MLP_Model_Hierarchical_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(D_in, 50, 8)\n",
    "mlp_model.load('MLP_Model_Hierarchical_3')\n",
    "mlp_model.eval()\n",
    "\n",
    "for to_predict in range(8):\n",
    "    df_target = df_test[df_test['tag'] == to_predict]\n",
    "\n",
    "    df_random = df_test\n",
    "\n",
    "    df_filtered = torch.from_numpy(np.array(pd.merge(df_target, df_random))[:,:-1])\n",
    "    output = mlp_model.predict(df_filtered)\n",
    "    # print(output)\n",
    "    proba_dict = {}\n",
    "\n",
    "    for x in output:\n",
    "        x = int(x)\n",
    "        if x not in proba_dict:\n",
    "            proba_dict[x] = 1\n",
    "        else:\n",
    "            proba_dict[x] += 1\n",
    "    for k in proba_dict.keys():\n",
    "        proba_dict[k] /= len(output)\n",
    "\n",
    "    print(dict(sorted(proba_dict.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
